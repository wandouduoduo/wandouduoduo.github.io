<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>运维随笔</title>
  
  <subtitle>SRE &amp; Devops &amp; Architect</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wandouduoduo.github.io/"/>
  <updated>2021-05-26T04:12:52.907Z</updated>
  <id>https://wandouduoduo.github.io/</id>
  
  <author>
    <name>豌豆多多</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>glusterfs集群横向扩容缩容</title>
    <link href="https://wandouduoduo.github.io/articles/9e838829.html"/>
    <id>https://wandouduoduo.github.io/articles/9e838829.html</id>
    <published>2021-05-26T03:14:28.000Z</published>
    <updated>2021-05-26T04:12:52.907Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>glusterfs集群的搭建和使用这里就不再赘述了，可以看以前的教程文档。本文主要聊的是随着服务使用量的增加，那么存储集群势必要扩充空间。服务器迁移，需要先扩容后缩容等等。所以本文的主旨是聊glusterfs集群的横向优化：扩容和缩容。</p><a id="more"></a><h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><p><strong>集群搭建这里忽略</strong><br>查看glusterfs的节点和客户端挂载情况得知，目前是三个节点的<strong>分布式卷</strong>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看节点数量</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster peer status</span></span><br><span class="line">Number of Peers: 2</span><br><span class="line">Hostname: 192.168.52.123</span><br><span class="line">Uuid: 0f07e396-fc0d-476c-884a-2cfb154f48d4</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line">Hostname: 192.168.52.124</span><br><span class="line">Uuid: 173df46f-a90a-4b0a-a5d0-834a17df17f6</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line"><span class="comment">#挂载 </span></span><br><span class="line">root@wyl01:/<span class="comment"># mount -t glusterfs 192.168.52.122:gv1 /gsclient/</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># df -h</span></span><br><span class="line">Filesystem Size Used Avail Use% Mounted on</span><br><span class="line">udev 1.9G 0 1.9G 0% /dev</span><br><span class="line">tmpfs 395M 972K 394M 1% /run</span><br><span class="line">/dev/vda3 49G 3.4G 44G 8% /</span><br><span class="line">tmpfs 2.0G 0 2.0G 0% /dev/shm</span><br><span class="line">tmpfs 5.0M 0 5.0M 0% /run/lock</span><br><span class="line">tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup</span><br><span class="line">/dev/loop0 90M 90M 0 100% /snap/core/8039</span><br><span class="line">/dev/loop1 89M 89M 0 100% /snap/core/6964</span><br><span class="line">/dev/vda2 190M 80M 97M 46% /boot</span><br><span class="line">tmpfs 395M 0 395M 0% /run/user/0</span><br><span class="line">/dev/vdb 196G 62M 186G 1% /data</span><br><span class="line">192.168.52.122:gv1 588G 8.1G 580G 2% /gsclient</span><br></pre></td></tr></table></figure><p>创建20个文件</p><p>查看文件的分布情况如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第1台</span></span><br><span class="line">root@wyl01:/data<span class="comment"># ls</span></span><br><span class="line">10.txt 11.txt 12.txt 14.txt 15.txt 16.txt 18.txt 1.txt 20.txt 2.txt 3.txt 6.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第2台</span></span><br><span class="line">root@gluster002-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">13.txt 17.txt 19.txt 4.txt 8.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第3台</span></span><br><span class="line">root@gluster003-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">5.txt 7.txt 9.txt lost+found</span><br></pre></td></tr></table></figure><h2 id="分布式卷优化"><a href="#分布式卷优化" class="headerlink" title="分布式卷优化"></a>分布式卷优化</h2><h3 id="添加节点扩容"><a href="#添加节点扩容" class="headerlink" title="添加节点扩容"></a>添加节点扩容</h3><p>现要对集群进行扩容，增加一个节点 gluster004-hf-aiui.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加一个节点</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster peer probe 192.168.52.125</span></span><br><span class="line">peer probe: success.</span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster peer status</span></span><br><span class="line">Number of Peers: 3</span><br><span class="line">Hostname: 192.168.52.123</span><br><span class="line">Uuid: 0f07e396-fc0d-476c-884a-2cfb154f48d4</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line">Hostname: 192.168.52.124</span><br><span class="line">Uuid: 173df46f-a90a-4b0a-a5d0-834a17df17f6</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line">Hostname: 192.168.52.125</span><br><span class="line">Uuid: f6578f82-adb4-4529-b803-eedde37cb550</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一个brick</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume add-brick gv1 192.168.52.125:/data force</span></span><br><span class="line">volume add-brick: success</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看卷的信息</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume info</span></span><br><span class="line">Volume Name: gv1</span><br><span class="line">Type: Distribute</span><br><span class="line">Volume ID: 110caace-b49f-4493-8792-bc2982319c23</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 4</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: 192.168.52.122:/data</span><br><span class="line">Brick2: 192.168.52.123:/data</span><br><span class="line">Brick3: 192.168.52.124:/data</span><br><span class="line">Brick4: 192.168.52.125:/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">performance.client-io-threads: on</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br></pre></td></tr></table></figure><p>再创建30个文件，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient<span class="comment"># touch &#123;101..130&#125;.txt</span></span><br><span class="line"><span class="comment"># 第 1 台</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># ls /data/</span></span><br><span class="line">101.txt 107.txt 10.txt 112.txt 114.txt 117.txt 11.txt 122.txt 12.txt 14.txt 15.txt 16.txt 18.txt 1.txt 20.txt 2.txt 3.txt 6.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第 2 台</span></span><br><span class="line">root@wyl02:/data<span class="comment"># ls</span></span><br><span class="line">105.txt 115.txt 116.txt 124.txt 125.txt 127.txt 128.txt 129.txt 13.txt 17.txt 19.txt 4.txt 8.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第 3 台</span></span><br><span class="line">root@wyl03-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">102.txt 103.txt 104.txt 106.txt 108.txt 109.txt 110.txt 111.txt 121.txt 130.txt 5.txt 7.txt 9.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第 4 台</span></span><br><span class="line">root@wyl04-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">113.txt 118.txt 119.txt 120.txt 123.txt 126.txt lost+found</span><br></pre></td></tr></table></figure><p>结论：可以看出当扩容后，原先的数据不会均衡到第四台glusterfs上，但是新增加的文件是可以的。</p><h3 id="分布式卷数据rebalance"><a href="#分布式卷数据rebalance" class="headerlink" title="分布式卷数据rebalance"></a>分布式卷数据rebalance</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume rebalance gv1 start</span></span><br><span class="line">volume rebalance: gv1: success: Rebalance on gv1 has been started successfully. Use rebalance status <span class="built_in">command</span> to check status of the rebalance process.</span><br><span class="line">ID: 76b07497-b26d-438f-bd6f-7659a9aba251</span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume rebalance gv1 status</span></span><br><span class="line">Node Rebalanced-files size scanned failures skipped status run time <span class="keyword">in</span> h:m:s</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">192.168.52.123 4 0Bytes 13 0 0 completed 0:00:00</span><br><span class="line">192.168.52.124 1 0Bytes 14 0 0 completed 0:00:00</span><br><span class="line">192.168.52.125 0 0Bytes 6 0 0 completed 0:00:00</span><br><span class="line">localhost 12 0Bytes 18 0 0 completed 0:00:01</span><br><span class="line">volume rebalance: gv1: success</span><br><span class="line"><span class="comment">#第 1 台</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># ls /data/</span></span><br><span class="line">101.txt 107.txt 112.txt 114.txt 117.txt 122.txt 13.txt 17.txt 4.txt 8.txt lost+found</span><br><span class="line"><span class="comment">#第 2 台</span></span><br><span class="line">root@wyl02-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">105.txt 115.txt 116.txt 124.txt 125.txt 127.txt 128.txt 129.txt 19.txt 9.txt lost+found</span><br><span class="line"><span class="comment">#第 3 台</span></span><br><span class="line">root@wyl03-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">100.txt 102.txt 103.txt 104.txt 106.txt 108.txt 109.txt 110.txt 111.txt 121.txt 130.txt 2.txt 5.txt 7.txt lost+found</span><br><span class="line"><span class="comment">#第 4 台</span></span><br><span class="line">root@wyl04-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">10.txt 113.txt 118.txt 119.txt 11.txt 120.txt 123.txt 126.txt 12.txt 14.txt 15.txt 16.txt 18.txt 1.txt 20.txt 3.txt 6.txt lost+found</span><br></pre></td></tr></table></figure><p>可以看到，数据rebalance，第 4 台上的数据明显增加了。</p><p>这里有一个需要注意的地方，当数据量太大的时候，对数据进行rebalance必须要考虑的一个问题就是性能，不能因为数据rebalance而影响我们的存储的正常使用。Glusterfs也考虑到了这个问题，在进行数据rebalance时，根据实际场景不同设计了三种不同的“级别”：</p><p><strong>lazy</strong>：每次仅可以迁移一个文件<br><strong>normal</strong>：默认设置，每次迁移2个文件或者是(CPU逻辑个数-4)/2,哪个大，选哪个<br><strong>aggressive</strong>：每次迁移4个文件或者是(CPU逻辑个数-4)/2<br>通过以下命令进行配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster volume set VOLUME-NAME cluster.rebal-throttle [lazy|normal|aggressive]</span><br></pre></td></tr></table></figure><p>如将volume repvol设置为lazy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nwyl01 ~]<span class="comment"># gluster volume set gv1 cluster.rebal-throttle lazy</span></span><br><span class="line">volume <span class="built_in">set</span>: success</span><br></pre></td></tr></table></figure><h3 id="分布式卷缩容"><a href="#分布式卷缩容" class="headerlink" title="分布式卷缩容"></a>分布式卷缩容</h3><p>缩容之前我们先需要将数据迁移到其他的brick上，假设我们移除gluster004-hf-aiui节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume remove-brick gv1 gluster004-hf-aiui:/data help</span></span><br><span class="line">Usage:</span><br><span class="line">volume remove-brick &lt;VOLNAME&gt; [replica &lt;COUNT&gt;] &lt;BRICK&gt; ... &lt;start|stop|status|commit|force&gt;</span><br><span class="line"></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume remove-brick gv1 gluster004-hf-aiui:/data start</span></span><br><span class="line">Running remove-brick with cluster.force-migration enabled can result <span class="keyword">in</span> data corruption. It is safer to <span class="built_in">disable</span> this option so that files that receive writes during migration are not migrated.</span><br><span class="line">Files that are not migrated can <span class="keyword">then</span> be manually copied after the remove-brick commit operation.</span><br><span class="line">Do you want to <span class="built_in">continue</span> with your current cluster.force-migration settings? (y/n) y</span><br><span class="line">volume remove-brick start: success</span><br><span class="line">ID: e30a9e72-53ef-4e79-a394-38dcac9061ba</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看移除节点的状态</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume remove-brick gv1 gluster004-hf-aiui:/data status</span></span><br><span class="line">Node Rebalanced-files size scanned failures skipped status run time <span class="keyword">in</span> h:m:s</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">192.168.52.125 17 0Bytes 17 0 0 completed 0:00:00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除前先将数据同步到其他brick上</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume remove-brick gv1 gluster004-hf-aiui:/data commit</span></span><br><span class="line">volume remove-brick commit: success</span><br><span class="line">Check the removed bricks to ensure all files are migrated.</span><br><span class="line">If files with data are found on the brick path, copy them via a gluster mount point before re-purposing the removed brick.</span><br></pre></td></tr></table></figure><p>移除后，我们看数据的分布情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第 1 台</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># ls /data/</span></span><br><span class="line">101.txt 107.txt 112.txt 114.txt 117.txt 122.txt 13.txt 17.txt 4.txt 8.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第 2 台</span></span><br><span class="line">root@wyl02-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">105.txt 115.txt 116.txt 124.txt 125.txt 127.txt 128.txt 129.txt 19.txt 9.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第 3 台</span></span><br><span class="line">root@wyl03-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">100.txt 103.txt 106.txt 109.txt 110.txt 113.txt 119.txt 120.txt 123.txt 12.txt 14.txt 16.txt 1.txt 2.txt 5.txt 7.txt</span><br><span class="line">102.txt 104.txt 108.txt 10.txt 111.txt 118.txt 11.txt 121.txt 126.txt 130.txt 15.txt 18.txt 20.txt 3.txt 6.txt lost+found</span><br></pre></td></tr></table></figure><p>可以看到文件被迁移到其他的brick上了。</p><h2 id="复制卷的扩容rebalance缩容"><a href="#复制卷的扩容rebalance缩容" class="headerlink" title="复制卷的扩容rebalance缩容"></a>复制卷的扩容rebalance缩容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume info # 卷的基本信息</span></span><br><span class="line">Volume Name: gv1</span><br><span class="line">Type: Replicate</span><br><span class="line">Volume ID: ff65e899-4f30-4249-9cf4-532a7d4eab74</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 1 x 2 = 2</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: 192.168.52.122:/data</span><br><span class="line">Brick2: 192.168.52.123:/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line">performance.client-io-threads: off</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建20个文件</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># touch &#123;1..20&#125;.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看分布情况</span></span><br><span class="line"><span class="comment"># 第 1 台</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># ls /data/</span></span><br><span class="line">10.txt 11.txt 12.txt 13.txt 14.txt 15.txt 16.txt 17.txt 18.txt 19.txt 1.txt 20.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt lost+found</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第 2 台</span></span><br><span class="line">root@wyl02-hf-aiui:/data<span class="comment"># ls</span></span><br><span class="line">10.txt 11.txt 12.txt 13.txt 14.txt 15.txt 16.txt 17.txt 18.txt 19.txt 1.txt 20.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt 8.txt 9.txt lost+found</span><br><span class="line">添加gluster003和gluster004两个节点</span><br></pre></td></tr></table></figure><h3 id="添加gluster003-节点"><a href="#添加gluster003-节点" class="headerlink" title="添加gluster003 节点"></a>添加gluster003 节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient# gluster peer probe 192.168.52.124</span><br><span class="line">peer probe: success.</span><br></pre></td></tr></table></figure><h3 id="添加gluster04-节点"><a href="#添加gluster04-节点" class="headerlink" title="添加gluster04 节点"></a>添加gluster04 节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient# gluster peer probe 192.168.52.125</span><br><span class="line">peer probe: success.</span><br></pre></td></tr></table></figure><h3 id="查看peer信息"><a href="#查看peer信息" class="headerlink" title="查看peer信息"></a>查看peer信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient<span class="comment"># gluster peer status</span></span><br><span class="line">Number of Peers: 3</span><br><span class="line">Hostname: 192.168.52.123</span><br><span class="line">Uuid: 0f07e396-fc0d-476c-884a-2cfb154f48d4</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line">Hostname: 192.168.52.124</span><br><span class="line">Uuid: 173df46f-a90a-4b0a-a5d0-834a17df17f6</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line">Hostname: 192.168.52.125</span><br><span class="line">Uuid: f6578f82-adb4-4529-b803-eedde37cb550</span><br><span class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩容brick</span></span><br><span class="line"></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume add-brick gv1 replica 2 192.168.52.124:/data 192.168.52.125:/data force</span></span><br><span class="line">volume add-brick: success</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看卷的信息</span></span><br><span class="line"></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume info</span></span><br><span class="line">Volume Name: gv1</span><br><span class="line">Type: Distributed-Replicate</span><br><span class="line">Volume ID: ff65e899-4f30-4249-9cf4-532a7d4eab74</span><br><span class="line">Status: Started</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 2 x 2 = 4</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: 192.168.52.122:/data</span><br><span class="line">Brick2: 192.168.52.123:/data</span><br><span class="line">Brick3: 192.168.52.124:/data</span><br><span class="line">Brick4: 192.168.52.125:/data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line">performance.client-io-threads: off</span><br></pre></td></tr></table></figure><p>发现现在变成2*2了模式了。重新写入20个txt文件，扩容后这里需要注意的是必须先rebalance。然后重新写入文件才会hash到新的节点上。之前的旧数据也会被rebalance。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume rebalance gv1 start</span></span><br><span class="line">volume rebalance: gv1: success: Rebalance on gv1 has been started successfully. Use rebalance status <span class="built_in">command</span> to check status of the rebalance process.</span><br><span class="line">ID: 90df529c-d950-4010-9248-19ffa7c83853</span><br></pre></td></tr></table></figure><p>节点的缩容，这里是分布式复制，所以缩容也是成对节点的一起缩容，操作如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始移除节点</span></span><br><span class="line"></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume remove-brick gv1 replica 2 wyl03-hf-aiui:/data wyl04-hf-aiui:/data start</span></span><br><span class="line">Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avaoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.</span><br><span class="line">Do you still want to <span class="built_in">continue</span>?</span><br><span class="line">(y/n) y</span><br><span class="line">Running remove-brick with cluster.force-migration enabled can result <span class="keyword">in</span> data corruption. It is safer to <span class="built_in">disable</span> this option so that files that receive writes during migration are not migrated.</span><br><span class="line">Files that are not migrated can <span class="keyword">then</span> be manually copied after the remove-brick commit operation.</span><br><span class="line">Do you want to <span class="built_in">continue</span> with your current cluster.force-migration settings? (y/n) y</span><br><span class="line">volume remove-brick start: success</span><br><span class="line">ID: d4ce7df1-30c9-4124-9986-c9634986609f</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除前先将数据同步到其他brick上</span></span><br><span class="line">root@wyl01:/gsclient<span class="comment"># gluster volume remove-brick gv1 replica 2 wyl03-hf-aiui:/data wyl04-hf-aiui:/data commit</span></span><br><span class="line">Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avaoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.</span><br><span class="line">Do you still want to <span class="built_in">continue</span>?</span><br><span class="line">(y/n) y</span><br><span class="line">volume remove-brick commit: success</span><br><span class="line">Check the removed bricks to ensure all files are migrated.</span><br><span class="line">If files with data are found on the brick path, copy them via a gluster mount point before re-purposing the removed</span><br></pre></td></tr></table></figure></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;glusterfs集群的搭建和使用这里就不再赘述了，可以看以前的教程文档。本文主要聊的是随着服务使用量的增加，那么存储集群势必要扩充空间。服务器迁移，需要先扩容后缩容等等。所以本文的主旨是聊glusterfs集群的横向优化：扩容和缩容。&lt;/p&gt;
    
    </summary>
    
      <category term="数据库" scheme="https://wandouduoduo.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="GlusterFS" scheme="https://wandouduoduo.github.io/tags/GlusterFS/"/>
    
  </entry>
  
  <entry>
    <title>kafka之扩容和缩容</title>
    <link href="https://wandouduoduo.github.io/articles/c34eeebc.html"/>
    <id>https://wandouduoduo.github.io/articles/c34eeebc.html</id>
    <published>2021-05-19T06:17:37.000Z</published>
    <updated>2021-05-19T06:43:23.785Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>本文讨论Kafka的扩缩容以及故障后如何“补齐”分区。实质上先扩容再缩容也是迁移的操作。</p><a id="more"></a><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>Kafka 版本2.6。</p><h2 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h2><p>扩容也就是新增节点，扩容后老的数据不会自动迁移，只有新创建的topic才可能会分配到新增的节点上面。如果我们不需要迁移旧数据，那直接把新的节点启动起来就行了，不需要做额外的操作。但有的时候，新增节点后，我们会将一些老数据迁移到新的节点上，以达到负载均衡的目的，这个时候就需要手动操作了。Kafka提供了一个脚本（在bin目录下）：<strong>kafka-reassign-partitions.sh</strong>，通过这个脚本可以重新分配分区的分布。脚本的使用比较简单，提供一个JSON格式的分配方案，然后传给脚本，脚本根据我们的分配方案重新进行平衡。</p><p>举个例子，假如现在集群有181、182两个broker，上面有4个topic：test-1，test-2，test-3，test-4，这些topic都有4个分区，2个副本，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 两个broker</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /kafka_26/brokers/ids</span><br><span class="line">[181, 182]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4个topic</span></span><br><span class="line">➜ bin/kafka-topics.sh --list --zookeeper localhost:2181/kafka_26</span><br><span class="line">__consumer_offsets</span><br><span class="line">test-1</span><br><span class="line">test-1</span><br><span class="line">test-3</span><br><span class="line">test-4</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">test</span>-1</span></span><br><span class="line">➜  bin/kafka-topics.sh --describe --topic test-1  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-1   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-1   Partition: 0    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-1   Partition: 1    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line">        Topic: test-1   Partition: 2    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-1   Partition: 3    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">test</span>-2</span></span><br><span class="line">➜  bin/kafka-topics.sh --describe --topic test-2  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-2   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-2   Partition: 0    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-2   Partition: 1    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line">        Topic: test-2   Partition: 2    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-2   Partition: 3    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">test</span>-3</span></span><br><span class="line">➜  bin/kafka-topics.sh --describe --topic test-3  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-3   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-3   Partition: 0    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-3   Partition: 1    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line">        Topic: test-3   Partition: 2    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-3   Partition: 3    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">test</span>-4</span></span><br><span class="line">➜ bin/kafka-topics.sh --describe --topic test-4  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-4   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-4   Partition: 0    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line">        Topic: test-4   Partition: 1    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br><span class="line">        Topic: test-4   Partition: 2    Leader: 182     Replicas: 182,181       Isr: 182,181</span><br><span class="line">        Topic: test-4   Partition: 3    Leader: 181     Replicas: 181,182       Isr: 181,182</span><br></pre></td></tr></table></figure><p>现在扩容了，新增了两个节点：183和184。扩容后，我们想要把test-3，test-4迁移到183，184上面去。</p><p>首先我们可以准备如下JSON格式的文件（假设文件名为<code>topics-to-move.json</code>）：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"topics"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"topic"</span>: <span class="string">"test-3"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"topic"</span>: <span class="string">"test-4"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"version"</span>: <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>里面写明想要重新分配的topic。然后执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --topics-to-move-json-file topics-to-move.json --broker-list "183,184" --generate</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前分区的分布情况</span></span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"test-3","partition":0,"replicas":[181,182],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":1,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":2,"replicas":[181,182],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":3,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":0,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":1,"replicas":[181,182],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":2,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":3,"replicas":[181,182],"log_dirs":["any","any"]&#125;]&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 建议的分区分布情况</span></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"test-3","partition":0,"replicas":[183,184],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":1,"replicas":[184,183],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":2,"replicas":[183,184],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":3,"replicas":[184,183],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":0,"replicas":[184,183],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":1,"replicas":[183,184],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":2,"replicas":[184,183],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":3,"replicas":[183,184],"log_dirs":["any","any"]&#125;]&#125;</span><br></pre></td></tr></table></figure><p>可以看到上面的命令会列出当前分区的分布情况，并且会给出一个建议的新分区分配方案，都是JSON格式的，内容也很简单。然后我们将建议的分配方案保存为一个文件（假设文件名为<code>expand-cluster-reassignment.json</code>），当然我们也可以手动修改这个方案，只要格式正确即可。然后执行下面命令使用新的方案进行分区重分配：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;"version":1,"partitions":[&#123;"topic":"test-3","partition":0,"replicas":[181,182],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":1,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":2,"replicas":[181,182],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-3","partition":3,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":0,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":1,"replicas":[181,182],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":2,"replicas":[182,181],"log_dirs":["any","any"]&#125;,&#123;"topic":"test-4","partition":3,"replicas":[181,182],"log_dirs":["any","any"]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for test-3-0,test-3-1,test-3-2,test-3-3,test-4-0,test-4-1,test-4-2,test-4-3</span><br></pre></td></tr></table></figure><p>这样就<strong>提交</strong>了重分配的任务，可以使用下面的命令查看任务的执行状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file expand-cluster-reassignment.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition test-3-0 is complete.</span><br><span class="line">Reassignment of partition test-3-1 is complete.</span><br><span class="line">Reassignment of partition test-3-2 is complete.</span><br><span class="line">Reassignment of partition test-3-3 is complete.</span><br><span class="line">Reassignment of partition test-4-0 is complete.</span><br><span class="line">Reassignment of partition test-4-1 is complete.</span><br><span class="line">Reassignment of partition test-4-2 is complete.</span><br><span class="line">Reassignment of partition test-4-3 is complete.</span><br><span class="line"></span><br><span class="line">Clearing broker-level throttles on brokers 181,182,183,184</span><br><span class="line">Clearing topic-level throttles on topics test-3,test-4</span><br></pre></td></tr></table></figure><p>完成后，我们检查一下新的test-3和test-4的分区分配情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ bin/kafka-topics.sh --describe --topic test-3  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-3   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-3   Partition: 0    Leader: 183     Replicas: 183,184       Isr: 183,184</span><br><span class="line">        Topic: test-3   Partition: 1    Leader: 184     Replicas: 184,183       Isr: 183,184</span><br><span class="line">        Topic: test-3   Partition: 2    Leader: 183     Replicas: 183,184       Isr: 183,184</span><br><span class="line">        Topic: test-3   Partition: 3    Leader: 184     Replicas: 184,183       Isr: 183,184</span><br><span class="line">        </span><br><span class="line">➜ bin/kafka-topics.sh --describe --topic test-4  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-4   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-4   Partition: 0    Leader: 184     Replicas: 184,183       Isr: 183,184</span><br><span class="line">        Topic: test-4   Partition: 1    Leader: 183     Replicas: 183,184       Isr: 183,184</span><br><span class="line">        Topic: test-4   Partition: 2    Leader: 184     Replicas: 184,183       Isr: 183,184</span><br><span class="line">        Topic: test-4   Partition: 3    Leader: 183     Replicas: 183,184       Isr: 184,183</span><br></pre></td></tr></table></figure><p>可以看到，这两个topic的数据已经全部分配到183和184节点上了。</p><h2 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h2><p>从上面可以看到，其实数据分配完全是由我们自己把控的，缩容也只是数据迁移而已，只需要提供正确的迁移方案即可。一般生产环境很少有缩容的，但有一个场景比较常见，就是某个节点故障了，且无法恢复。以前的文章提到过，节点故障后，这个节点上的分区就丢了，Kafka不会自动在其它可用节点上重新创建一个副本，这个时候就需要我们自己手动在其他可用节点创建副本，原理和扩容是一样的。接着上面的例子，比如现在184节点故障了，且无法恢复了，而test-3和test-4有部分分区是在该节点上面的，自然也就丢了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 节点挂了，zk中的节点已经没了</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 15] ls /kafka_26/brokers/ids</span><br><span class="line">[181, 182, 183]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到ISR中已经没有184了</span></span><br><span class="line">➜ bin/kafka-topics.sh --describe --topic test-3  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-3   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-3   Partition: 0    Leader: 183     Replicas: 183,184       Isr: 183</span><br><span class="line">        Topic: test-3   Partition: 1    Leader: 183     Replicas: 184,183       Isr: 183</span><br><span class="line">        Topic: test-3   Partition: 2    Leader: 183     Replicas: 183,184       Isr: 183</span><br><span class="line">        Topic: test-3   Partition: 3    Leader: 183     Replicas: 184,183       Isr: 183</span><br><span class="line">➜ bin/kafka-topics.sh --describe --topic test-4  --zookeeper localhost:2181/kafka_26</span><br><span class="line">Topic: test-4   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-4   Partition: 0    Leader: 183     Replicas: 184,183       Isr: 183</span><br><span class="line">        Topic: test-4   Partition: 1    Leader: 183     Replicas: 183,184       Isr: 183</span><br><span class="line">        Topic: test-4   Partition: 2    Leader: 183     Replicas: 184,183       Isr: 183</span><br><span class="line">        Topic: test-4   Partition: 3    Leader: 183     Replicas: 183,184       Isr: 183</span><br></pre></td></tr></table></figure><p>这个时候，我们准备把test-3原来在184上的分区分配到181上面去，把test-4在184上的分区分配到182上去，那分配方案就是下面这样的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">➜ cat expand-cluster-reassignment.json</span><br><span class="line">&#123;</span><br><span class="line">  "version": 1,</span><br><span class="line">  "partitions": [</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-3",</span><br><span class="line">      "partition": 0,</span><br><span class="line">      "replicas": [183, 181],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-3",</span><br><span class="line">      "partition": 1,</span><br><span class="line">      "replicas": [181, 183],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-3",</span><br><span class="line">      "partition": 2,</span><br><span class="line">      "replicas": [183, 181],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-3",</span><br><span class="line">      "partition": 3,</span><br><span class="line">      "replicas": [181, 183],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-4",</span><br><span class="line">      "partition": 0,</span><br><span class="line">      "replicas": [182, 183],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-4",</span><br><span class="line">      "partition": 1,</span><br><span class="line">      "replicas": [183, 182],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-4",</span><br><span class="line">      "partition": 2,</span><br><span class="line">      "replicas": [182, 183],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      "topic": "test-4",</span><br><span class="line">      "partition": 3,</span><br><span class="line">      "replicas": [183, 182],</span><br><span class="line">      "log_dirs": ["any", "any"]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后执行分配方案即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 执行分配方案</span></span><br><span class="line">➜ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br><span class="line"><span class="meta">#</span><span class="bash"> 输出略</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看进度</span></span><br><span class="line">➜ bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file expand-cluster-reassignment.json --verify </span><br><span class="line"><span class="meta">#</span><span class="bash"> 输出略</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 完成后查看<span class="built_in">test</span>-3和<span class="built_in">test</span>-4</span></span><br><span class="line">➜ bin/kafka-topics.sh --describe --topic test-3  --zookeeper localhost:2181/kafka_26Topic: test-3   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-3   Partition: 0    Leader: 183     Replicas: 183,181       Isr: 183,181</span><br><span class="line">        Topic: test-3   Partition: 1    Leader: 183     Replicas: 181,183       Isr: 183,181</span><br><span class="line">        Topic: test-3   Partition: 2    Leader: 183     Replicas: 183,181       Isr: 183,181</span><br><span class="line">        Topic: test-3   Partition: 3    Leader: 183     Replicas: 181,183       Isr: 183,181</span><br><span class="line">➜ bin/kafka-topics.sh --describe --topic test-4  --zookeeper localhost:2181/kafka_26Topic: test-4   PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: test-4   Partition: 0    Leader: 183     Replicas: 182,183       Isr: 183,182</span><br><span class="line">        Topic: test-4   Partition: 1    Leader: 183     Replicas: 183,182       Isr: 183,182</span><br><span class="line">        Topic: test-4   Partition: 2    Leader: 183     Replicas: 182,183       Isr: 183,182</span><br><span class="line">        Topic: test-4   Partition: 3    Leader: 183     Replicas: 183,182       Isr: 183,182</span><br></pre></td></tr></table></figure><h2 id="kafka-manager页面操作"><a href="#kafka-manager页面操作" class="headerlink" title="kafka manager页面操作"></a>kafka manager页面操作</h2><p>页面操作不支持批量操作topic，需要逐个topic进行操作。</p><p>1，进入topic视图，点击 Generate Partition Assignments 生成分区分配。进入分区分配界面，</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/c34eeebc/1.png" alt></p><p>2，对该topic需要占用的节点进行勾选，再次点击 Generate Partition Assignments</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/c34eeebc/2.png" alt></p><p>3，分区完成 ， go to topic view</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/c34eeebc/3.png" alt></p><p>4,  重新分配。 <strong>Reassign Partitions</strong></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/c34eeebc/4.png" alt></p><p>5，go to reassign partitions 转到重新分配分区</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/c34eeebc/5.png" alt></p><p>6，验证查看</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/c34eeebc/6.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不管扩容还是缩容，或者是故障后手动补齐分区，实质都是分区重分配，使用<code>kafka-reassign-partitions.sh</code>脚本即可。该脚本使用也非常简单：</p><ol><li>先提供一个JSON格式的需要重分配的topic列表，然后执行<code>--generate</code>生成迁移方案文件；</li><li>然后使用<code>--execute</code>执行新的分配方案；</li><li>最后使用<code>--verify</code>查看分配方案执行进度。</li></ol><p>如果对于分配方案文件格式很熟悉，可以跳过1.</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文讨论Kafka的扩缩容以及故障后如何“补齐”分区。实质上先扩容再缩容也是迁移的操作。&lt;/p&gt;
    
    </summary>
    
      <category term="运维技术" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Kafka" scheme="https://wandouduoduo.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>lvs负载均衡实战</title>
    <link href="https://wandouduoduo.github.io/articles/17e1663a.html"/>
    <id>https://wandouduoduo.github.io/articles/17e1663a.html</id>
    <published>2021-04-19T06:23:22.000Z</published>
    <updated>2021-04-20T07:35:26.203Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>通过本文掌握什么是负载均衡及负载均衡的作用和意义；了解lvs负载均衡的三种模式；了解lvs-DR负载均衡部署方法；掌握nginx实现负载均衡的方法；掌握lvs+nginx负载均衡拓扑结构。</p> <a id="more"></a><h2 id="负载均衡方案"><a href="#负载均衡方案" class="headerlink" title="负载均衡方案"></a>负载均衡方案</h2><h3 id="什么是负载均衡"><a href="#什么是负载均衡" class="headerlink" title="什么是负载均衡"></a>什么是负载均衡</h3><p> 一台普通服务器的处理能力是有限的，假如能达到每秒几万个到几十万个请求，但却无法在一秒钟内处理上百万个甚至更多的请求。但若能将多台这样的服务器组成一个系统，并通过软件技术将所有请求平均分配给所有服务器，那么这个系统就完全拥有每秒钟处理几百万个甚至更多请求的能力。这就是负载均衡最初的基本设计思想。</p><p>负载均衡是由多台服务器以对称的方式组成一个服务器集合，每台服务器都具有等价的地位，都可以单独对外提供服务而无须其他服务器的辅助。通过某种负载分担技术，将外部发送来的请求按照某种策略分配到服务器集合的某一台服务器上，而接收到请求的服务器独立地回应客户的请求。负载均衡解决了大量并发访问服务问题，其目的就是用最少的投资获得接近于大型主机的性能。 </p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/1.png" alt="img"></p><h3 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h3><h4 id="基于DNS的负载均衡"><a href="#基于DNS的负载均衡" class="headerlink" title="基于DNS的负载均衡"></a>基于DNS的负载均衡</h4><p>DNS（Domain Name System，域名系统），因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。DNS协议运行在UDP协议之上，使用端口号53。</p><p>DNS负载均衡技术是最早的负载均衡解决方案，它是通过DNS服务中的随机名字解析来实现的，在DNS服务器中，可以为多个不同的地址配置同一个名字，而最终查询这个名字的客户机将在解析这个名字时得到其中的一个地址。因此，对于同一个名字，不同的客户机会得到不同的地址，它们也就访问不同地址上的Web服务器，从而达到负载均衡的目的。</p><p>如下图：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/2.png" alt="img"></p><p><strong>优点</strong></p><p>实现简单、实施容易、成本低、适用于大多数TCP/IP应用；</p><p><strong>缺点</strong></p><p>1、 负载分配不均匀，DNS服务器将Http请求平均地分配到后台的Web服务器上，而不考虑每个Web服务器当前的负载情况；如果后台的Web服务器的配置和处理能力不同，最慢的Web服务器将成为系统的瓶颈，处理能力强的服务器不能充分发挥作用；</p><p>2、可靠性低，如果后台的某台Web服务器出现故障，DNS服务器仍然会把DNS请求分配到这台故障服务器上，导致不能响应客户端。</p><p>3、变更生效时间长，如果更改NDS有可能造成相当一部分客户不能享受Web服务，并且由于DNS缓存的原因，所造成的后果要持续相当长一段时间(一般DNS的刷新周期约为24小时)。</p><h4 id="基于四层交换技术的负载均衡"><a href="#基于四层交换技术的负载均衡" class="headerlink" title="基于四层交换技术的负载均衡"></a>基于四层交换技术的负载均衡</h4><p>基于四层交换技术的负载均衡是通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器与请求客户端建立TCP连接，然后发送Client请求的数据。</p><p>如下图：</p><p>​        <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/3.png" alt="img"></p><p>client发送请求至4层负载均衡器，4层负载均衡器根据负载策略把client发送的报文目标地址(原来是负载均衡设备的IP地址)修改为后端服务器（可以是web服务器、邮件服务等）IP地址，这样client就可以直接跟后端服务器建立TCP连接并发送数据。</p><p>具有代表意义的产品：LVS（开源软件），F5（硬件）</p><p><strong>优点</strong></p><p>性能高、支持各种网络协议</p><p><strong>缺点</strong></p><p>对网络依赖较大，负载智能化方面没有7层负载好（比如不支持对url个性化负载），F5硬件性能很高但成本也高需要人民币几十万，对于小公司就望而却步了。</p><h4 id="基于七层交换技术的负载均衡"><a href="#基于七层交换技术的负载均衡" class="headerlink" title="基于七层交换技术的负载均衡"></a>基于七层交换技术的负载均衡</h4><p>基于七层交换技术的负载均衡也称内容交换，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的服务器。</p><p>如下图：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/4.png" alt="img"></p><p>七层负载均衡服务器起了一个代理服务器的作用，client要访问webserver要先与七层负载设备进行三次握手后建立TCP连接，把要访问的报文信息发送给七层负载均衡；然后七层负载均衡再根据设置的均衡规则选择特定的webserver，然后通过三次握手与此台webserver建立TCP连接，然后webserver把需要的数据发送给七层负载均衡设备，负载均衡设备再把数据发送给client。</p><p>具有代表意义的产品：nginx（软件）、apache（软件）</p><p><strong>优点</strong></p><p>对网络依赖少，负载智能方案多（比如可根据不同的url进行负载）</p><p><strong>缺点</strong></p><p>网络协议有限，nginx和apache支持http负载，性能没有4层负载高</p><h3 id="确定使用四层-七层负载结合方案"><a href="#确定使用四层-七层负载结合方案" class="headerlink" title="确定使用四层+七层负载结合方案"></a>确定使用四层+七层负载结合方案</h3><p>四层负载使用lvs软件或F5硬件实现。</p><p>七层负载使用nginx实现。</p><p>如下图是lvs+nginx的拓扑结构：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/5.png" alt="img"></p><h3 id="nginx集群背景"><a href="#nginx集群背景" class="headerlink" title="nginx集群背景"></a>nginx集群背景</h3><p>在keepalived+nginx的主备容灾高可用的架构中，nginx是作为外部访问系统的唯一入口，理论上一台nginx的最大并发量可以高达50000，但是当并发量更大的时候，keepalived+nginx的高可用机制是没办法满足需求的，因为keepalived+nginx的架构中确确实实是一台nginx在工作，只有当master宕机或异常时候，备份机才会上位。那么如何解决更大的高并发问题呢，也许会问能不能搭建nginx集群，直接对外提供访问？</p><p>很显然这是欠妥当的，因为当nginx作为外部的唯一访问入口，没办法直接以集群的形式对外提供服务，没有那么多的公网ip资源可用，既太浪费也不友好。但是在内网环境下，是可以用nginx集群（nginx横向扩展服务集合）的，当然总得有一个对外入口，所以需要在nginx集群之上，在加一层负载均衡器，作为系统的唯一入口。</p><h2 id="lvs实现四层负载DR模式"><a href="#lvs实现四层负载DR模式" class="headerlink" title="lvs实现四层负载DR模式"></a>lvs实现四层负载DR模式</h2><h3 id="什么是lvs"><a href="#什么是lvs" class="headerlink" title="什么是lvs"></a>什么是lvs</h3><p>LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩博士成立，是中国国内最早出现的自由软件项目之一。</p><h3 id="lvs实现负载的三种方式"><a href="#lvs实现负载的三种方式" class="headerlink" title="lvs实现负载的三种方式"></a>lvs实现负载的三种方式</h3><p>运行 lPVS软件的服务器，在整个负载均衡集群中承担一调度角色 软件的服务器，（即 向真实服务器分配从客户端过来的请求。LVS中的调度方法有三种 ：NAT（Network Address Translation网络地址转换）、TUN（tunnel 隧道）、DR（direct route 直接路由）</p><h3 id="LVS-DR-模式"><a href="#LVS-DR-模式" class="headerlink" title="LVS-DR 模式"></a>LVS-DR 模式</h3><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/6.png" alt="img"></p><p>请求由LVS接受，由真实提供服务的服务器(RealServer, RS)直接返回给用户，返回的时候不经过LVS。</p><p>DR模式下需要LVS服务器和RS绑定同一个VIP， 一个请求过来时，LVS只需要将网络帧的MAC地址修改为某一台RS的MAC，该包就会被转发到相应的RS处理，注意此时的源IP和目标IP都没变，RS收到LVS转发来的包，发现MAC是自己的，发现IP也是自己的，于是这个包被合法地接受，而当RS返回响应时，只要直接向源IP(即用户的IP)返回即可，不再经过LVS。</p><p>DR模式下，lvs接收请求输入，将请求转发给RS，由RS输出响应给用户，性能非常高。</p><p>它的不足之处是要求负载均衡器与RS在一个物理段上。</p><h3 id="LVS-NAT模式"><a href="#LVS-NAT模式" class="headerlink" title="LVS-NAT模式"></a>LVS-NAT模式</h3><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/7.png" alt="img"></p><p>NAT(Network Address Translation)是一种外网和内网地址映射的技术。NAT模式下，LVS需要作为RS的网关，当网络包到达LVS时，LVS做目标地址转换(DNAT)，将目标IP改为RS的IP。RS接收到包以后，处理完，返回响应时，源IP是RS IP，目标IP是客户端的IP，这时RS的包通过网关(LVS)中转，LVS会做源地址转换(SNAT)，将包的源地址改为VIP，对于客户端只知道是LVS直接返回给它的。</p><p>NAT模式请求和响应都需要经过lvs，性能没有DR模式好。</p><h3 id="LVS-TUN模式"><a href="#LVS-TUN模式" class="headerlink" title="LVS-TUN模式"></a>LVS-TUN模式</h3><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/31.png" alt="img"></p><p>TUN模式是通过ip隧道技术减轻lvs调度服务器的压力，许多Internet服务（例如WEB服务器）的请求包很短小，而应答包通常很大，负载均衡器只负责将请求包分发给物理服务器，而物理服务器将应答包直接发给用户。所以，负载均衡器能处理很巨大的请求量。相比NAT性能要高的多，比DR模式的优点是不限制负载均衡器与RS在一个物理段上。但是它的不足需要所有的服务器（lvs、RS）支持”IP Tunneling”(IP Encapsulation)协议。</p><h2 id="lvs-DR实战"><a href="#lvs-DR实战" class="headerlink" title="lvs-DR实战"></a>lvs-DR实战</h2><p>vip：192.168.101.100</p><p>lvs-director：192.168.101.8</p><p>nginx1：192.168.101.3</p><p>nginx2：192.168.101.4</p><h3 id="lvs调度服务器Director安装"><a href="#lvs调度服务器Director安装" class="headerlink" title="lvs调度服务器Director安装"></a>lvs调度服务器Director安装</h3><h4 id="安装lvs"><a href="#安装lvs" class="headerlink" title="安装lvs"></a>安装lvs</h4><p>在192.168.101.8上安装lvs</p><p>centos6.5自带lvs，检查linux内核是否集成lvs模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe -l | grep ipvs</span><br></pre></td></tr></table></figure><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/8.png" alt="img"> </p><h4 id="安装lvs的管理工具ipvsadm"><a href="#安装lvs的管理工具ipvsadm" class="headerlink" title="安装lvs的管理工具ipvsadm"></a>安装lvs的管理工具ipvsadm</h4><ul><li>安装依赖</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y gcc gcc-c++ makepcre pcre-devel kernel-devel openssl-devel libnl-devel popt*</span><br></pre></td></tr></table></figure><ul><li>安装ipvsadm</li></ul><p>将ipvsadm-1.26.tar.gz拷贝至/usr/local/下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span></span><br><span class="line">tar -zxvf ipvsadm-1.26.tar.gz</span><br><span class="line"><span class="built_in">cd</span> ipvsadm-1.26</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">或者</span><br><span class="line">yum install ipvsadm -y</span><br></pre></td></tr></table></figure><p>校验是否安装成功：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/9.png" alt="img"></p><h4 id="真实服务器Real-Server安装"><a href="#真实服务器Real-Server安装" class="headerlink" title="真实服务器Real Server安装"></a>真实服务器Real Server安装</h4><p>在192.168.101.3和192.168.101.4上安装nginx。</p><p><strong>nginx配置文件</strong></p><p>创建nginx-lvs.conf，http内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line"></span><br><span class="line">    include       mime.types;</span><br><span class="line"></span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line"></span><br><span class="line">        listen       80;</span><br><span class="line"></span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line"></span><br><span class="line">            root   html;</span><br><span class="line"></span><br><span class="line">            index  index.html index.htm;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="Director-Server配置"><a href="#Director-Server配置" class="headerlink" title="Director Server配置"></a>Director Server配置</h3><h4 id="在eth0上绑定虚拟ip"><a href="#在eth0上绑定虚拟ip" class="headerlink" title="在eth0上绑定虚拟ip"></a>在eth0上绑定虚拟ip</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0:0 192.168.101.100 broadcast 192.168.101.100 netmask 255.255.255.255 up</span><br></pre></td></tr></table></figure><p>此处在eth0设备上绑定了一个虚拟设备eth0:0，同时设置了一个虚拟IP是<em>192.168.101.100</em>，然后指定广播地址也为<em>192.168.101.100</em>，需要特别注意的是，虚拟ip地址的广播地址是它本身，子网掩码是255.255.255.255。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/10.png" alt="img"></p><h4 id="添加路由规则"><a href="#添加路由规则" class="headerlink" title="添加路由规则"></a>添加路由规则</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">route add -host 192.168.101.100 dev eth0:0</span><br></pre></td></tr></table></figure><h4 id="启用系统的包转发功能"><a href="#启用系统的包转发功能" class="headerlink" title="启用系统的包转发功能"></a>启用系统的包转发功能</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"1"</span> &gt;/proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure><p>参数值为1时启用ip转发，为0时禁止ip转发。</p><h4 id="清除原有转发规则"><a href="#清除原有转发规则" class="headerlink" title="清除原有转发规则"></a>清除原有转发规则</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm --clear</span><br></pre></td></tr></table></figure><h4 id="添加虚拟IP规则"><a href="#添加虚拟IP规则" class="headerlink" title="添加虚拟IP规则"></a>添加虚拟IP规则</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.101.100:80 -s rr</span><br></pre></td></tr></table></figure><p> -s rr表示采用轮询策略。</p><p>:80表示负载转发的端口是80</p><h4 id="在虚拟IP中添加服务规则"><a href="#在虚拟IP中添加服务规则" class="headerlink" title="在虚拟IP中添加服务规则"></a>在虚拟IP中添加服务规则</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -a -t 192.168.101.100:80 -r 192.168.101.3:80 -g</span><br><span class="line">ipvsadm -a -t 192.168.101.100:80 -r 192.168.101.4:80 -g</span><br></pre></td></tr></table></figure><p>在新加虚拟IP记录中添加两条新的Real Server记录，-g表示指定LVS 的工作模式为直接路由模式。</p><p>lvs进行负载转发需要保证lvs负载的端口要和nginx服务的端口的一致，这里都为80。</p><h4 id="重启lvs"><a href="#重启lvs" class="headerlink" title="重启lvs"></a>重启lvs</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm</span><br></pre></td></tr></table></figure><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/11.png" alt="img"></p><h3 id="Real-Server配置"><a href="#Real-Server配置" class="headerlink" title="Real Server配置"></a>Real Server配置</h3><p>在lvs的DR和TUn模式下，用户的访问请求到达真实服务器后，是直接返回给用户的，而不再经过前端的Director Server，因此，就需要在每个Real server节点上增加虚拟的VIP地址，这样数据才能直接返回给用户。</p><h4 id="在回环设备上绑定了一个虚拟IP地址"><a href="#在回环设备上绑定了一个虚拟IP地址" class="headerlink" title="在回环设备上绑定了一个虚拟IP地址"></a>在回环设备上绑定了一个虚拟IP地址</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig lo:0 192.168.101.100 broadcast 192.168.101.100 netmask 255.255.255.255 up</span><br><span class="line">/sbin/route add -host 192.168.101.100 dev lo:0</span><br></pre></td></tr></table></figure><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/12.png" alt="img"></p><h4 id="关闭arp解析"><a href="#关闭arp解析" class="headerlink" title="关闭arp解析"></a>关闭arp解析</h4><p>arp_announce ：定义不同级别：当ARP请求通过某个端口进来是否利用这个接口来回应。</p><p>​         0 -利用本地的任何地址，不管配置在哪个接口上去响应ARP请求；</p><p>​         1 - 避免使用另外一个接口上的mac地址去响应ARP请求；</p><p>​         2 - 尽可能使用能够匹配到ARP请求的最佳地址。</p><p>arp_ignore：当ARP请求发过来后发现自己正是请求的地址是否响应；</p><p>​            0 - 利用本地的任何地址，不管配置在哪个接口上去响应ARP请求；</p><p>​            1 - 哪个接口上接受ARP请求，就从哪个端口上回应。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"1"</span> &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"2"</span> &gt;/proc/sys/net/ipv4/conf/lo/arp_announce </span><br><span class="line"><span class="built_in">echo</span> <span class="string">"1"</span> &gt;/proc/sys/net/ipv4/conf/all/arp_ignore</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"2"</span> &gt;/proc/sys/net/ipv4/conf/all/arp_announce </span><br><span class="line"></span><br><span class="line">sysctl -p <span class="comment">#使用修改生效</span></span><br></pre></td></tr></table></figure><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/13.png" alt="img"></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="预期目标"><a href="#预期目标" class="headerlink" title="预期目标"></a>预期目标</h4><p>由于lvs设置为rr轮询策略，当访问虚IP <a href="http://192.168.101.100，每次刷新请求通过lvs负载到不同的服务器。" rel="noopener" target="_blank">http://192.168.101.100，每次刷新请求通过lvs负载到不同的服务器。</a></p><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>1、测试时需要在nginx的http中设置keepalive_timeout  0; 取消使用http持久连接模式，保证每次客户端发起请求都需要向服务端建立连接，这样做是为了每次刷新页面都要经过lvs负载转发。</p><p>2、lvs进行负载转发需要保证lvs负载的端口要和nginx服务的端口的一致，这里都为80。</p><p>keepalive_timeout说明：</p><p>在nginx中keepalive_timeout的默认值是75秒，默认使用http持久连接模式，可使客户端到服务器端的连接持续有效，当出现对服务器的后继请求时，可避免建立或重新建立连接。生产环境建议keepalive_timeout不要设置为0。</p><h4 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h4><p>修改192.168.101.3和192.168.101.4下html目录中index.html的内容使之个性化。</p><p>第一次请求：<a href="http://192.168.101.100" rel="noopener" target="_blank">http://192.168.101.100</a></p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/14.png" alt="img"></p><p>刷新，相当于第二次请求：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/15.png" alt="img"></p><p>依次交替测试，发现每次请求被负载到不同的nginx上。</p><p>任意停止掉一个nginx，请求<a href="http://192.168.101.100继续可以浏览，由于lvs采用轮询策略如果其中一个nginx请求不可到达则去请求另外的nginx。" rel="noopener" target="_blank">http://192.168.101.100继续可以浏览，由于lvs采用轮询策略如果其中一个nginx请求不可到达则去请求另外的nginx。</a></p><h3 id="脚本封装"><a href="#脚本封装" class="headerlink" title="脚本封装"></a>脚本封装</h3><p>为了方便配置启动lvs将上边Director Server和Real Server的配置过程封装在shell脚本中。</p><h4 id="Director-Server配置-1"><a href="#Director-Server配置-1" class="headerlink" title="Director Server配置"></a>Director Server配置</h4><p>在/etc/init.d下创建lvsdr，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="comment"># 定义虚拟ip</span></span><br><span class="line">VIP=192.168.101.100 <span class="comment">#虚拟 ip根据需求修改</span></span><br><span class="line"><span class="comment"># 定义realserver,并已空格分开，根据需求修改</span></span><br><span class="line">RIPS=<span class="string">"192.168.101.3 192.168.101.4"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义提供服务的端口</span></span><br><span class="line">SERVICE=80</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用init.d脚本的标准库</span></span><br><span class="line">. /etc/rc.d/init.d/<span class="built_in">functions</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">        start)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Start LVS of DR Mode"</span></span><br><span class="line">        <span class="comment"># 开启ip转发</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"1"</span> &gt; /proc/sys/net/ipv4/ip_forward</span><br><span class="line">        <span class="comment"># 绑定虚拟ip</span></span><br><span class="line">        ifconfig eth0:0 <span class="variable">$VIP</span> broadcast <span class="variable">$VIP</span> netmask 255.255.255.255 up</span><br><span class="line">        route add -host <span class="variable">$VIP</span> dev eth0:0</span><br><span class="line">        <span class="comment"># 清除lvs规则</span></span><br><span class="line">        ipvsadm -C</span><br><span class="line">        <span class="comment"># 添加一条虚拟服务器记录</span></span><br><span class="line">    <span class="comment"># -p指定一定的时间内将相同的客户端分配到同一台后端服务器</span></span><br><span class="line">    <span class="comment"># 用于解决session的问题,测试时或有别的解决方案时建议去掉</span></span><br><span class="line">        ipvsadm -A -t <span class="variable">$VIP</span>:<span class="variable">$SERVICE</span> -s rr</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加真实服务器记录</span></span><br><span class="line">        <span class="keyword">for</span> RIP <span class="keyword">in</span> <span class="variable">$RIPS</span></span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="variable">$RIP</span>:<span class="variable">$SERVICE</span>;</span><br><span class="line">                ipvsadm -a -t <span class="variable">$VIP</span>:<span class="variable">$SERVICE</span> -r <span class="variable">$RIP</span>:<span class="variable">$SERVICE</span> -g</span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">        <span class="comment"># 设置tcp tcpfin  udp的超时连接值</span></span><br><span class="line">        ipvsadm --<span class="built_in">set</span> 30 120 300</span><br><span class="line">        ipvsadm</span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">        stop)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Stop LVS DR"</span></span><br><span class="line">        ifconfig eth0:0 down</span><br><span class="line">        ipvsadm -C</span><br><span class="line">        ;;</span><br><span class="line">        *)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Usage:<span class="variable">$0</span> &#123;start ¦ stop&#125;"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改脚本权限：</span></span><br><span class="line">chmod +x /etc/init.d/lvsdr</span><br><span class="line"><span class="comment">#启动Director server：</span></span><br><span class="line">service lvsdr start</span><br><span class="line"><span class="comment">#停止Director server：</span></span><br><span class="line">service lvsdr stop</span><br></pre></td></tr></table></figure><h4 id="Real-Server配置-1"><a href="#Real-Server配置-1" class="headerlink" title="Real Server配置"></a>Real Server配置</h4><p>在/etc/init.d下创建lvsdr，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">VIP=192.168.101.100 <span class="comment">#虚拟ip，根据需求修改</span></span><br><span class="line">. /etc/rc.d/init.d/<span class="built_in">functions</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">        start)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"lo:0 port starting"</span></span><br><span class="line">        <span class="comment"># 为了相应lvs调度器转发过来的包,需在本地lo接口上绑定vip</span></span><br><span class="line">        ifconfig lo:0 <span class="variable">$VIP</span> broadcast <span class="variable">$VIP</span> netmask 255.255.255.255 up</span><br><span class="line">        <span class="comment"># 限制arp请求</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"1"</span> &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"2"</span> &gt; /proc/sys/net/ipv4/conf/lo/arp_announce</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"1"</span> &gt; /proc/sys/net/ipv4/conf/all/arp_ignore</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"2"</span> &gt; /proc/sys/net/ipv4/conf/all/arp_announce</span><br><span class="line">        ;;</span><br><span class="line">        stop)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"lo:0 port closing"</span></span><br><span class="line">        ifconfig lo:0 down</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"0"</span> &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"0"</span> &gt; /proc/sys/net/ipv4/conf/lo/arp_announce</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"0"</span> &gt; /proc/sys/net/ipv4/conf/all/arp_ignore</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"0"</span> &gt; /proc/sys/net/ipv4/conf/all/arp_announce</span><br><span class="line">        ;;</span><br><span class="line">        *)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Usage: <span class="variable">$0</span> &#123;start ¦ stop&#125;"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改脚本权限：</span></span><br><span class="line">chmod +x /etc/init.d/lvsdr</span><br><span class="line"><span class="comment">#启动real server：</span></span><br><span class="line">service lvsdr start</span><br><span class="line"><span class="comment">#停止real server：</span></span><br><span class="line">service lvsdr stop</span><br></pre></td></tr></table></figure><h2 id="lvs四层-nginx七层负载均衡"><a href="#lvs四层-nginx七层负载均衡" class="headerlink" title="lvs四层+nginx七层负载均衡"></a>lvs四层+nginx七层负载均衡</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>lvs采用DR模式基本上没有性能瓶颈，用户请求输入至lvs经过负载转发到后台服务上，通过后台服务输出响应给用户。nginx的负载性能远没有lvs好，lvs四层+nginx七层负载的好处是最前端是lvs接收请求进行负载转发，由多个nginx共同完成七层负载，这样nginx的负载性能就可以线性扩展。</p><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><p>vip：192.168.101.100</p><p>lvs-director：192.168.101.8</p><p>nginx1：192.168.101.3                              安装nginx</p><p>nginx2：192.168.101.4                              安装nginx</p><p>tomcat1：192.168.101.5                           安装tomcat</p><p>tomcat2：192.168.101.6                           安装tomcat</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><h4 id="Director-Server配置-2"><a href="#Director-Server配置-2" class="headerlink" title="Director Server配置"></a>Director Server配置</h4><p>vip：192.168.101.100</p><p>lvs-director：192.168.101.8</p><p>参考lvs四层负载DR模式进行配置</p><h4 id="Real-Server配置-2"><a href="#Real-Server配置-2" class="headerlink" title="Real Server配置"></a>Real Server配置</h4><p>nginx1：192.168.101.3                              安装nginx</p><p>nginx2：192.168.101.4                              安装nginx</p><p>参考lvs四层负载DR模式进行配置，需要修改nginx的配置文件使每个nginx对两个tomcat进行负载，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line"></span><br><span class="line">   upstream tomcat_server_pool&#123;</span><br><span class="line">        server 192.168.101.5:8080 weight=10;</span><br><span class="line">        server 192.168.101.6:8080 weight=10;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen 80;</span><br><span class="line">        server_name localhost;</span><br><span class="line">        location / &#123;</span><br><span class="line">                 proxy_pass http://tomcat_server_pool;</span><br><span class="line">                 index index.jsp index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><p>请求<a href="http://192.168.101.100，lvs负载到不同的nginx上，如果停止任意一台nginx或停止任意一台tomcat不影响访问。" rel="noopener" target="_blank">http://192.168.101.100，lvs负载到不同的nginx上，如果停止任意一台nginx或停止任意一台tomcat不影响访问。</a></p><h2 id="lvs高可用-了解"><a href="#lvs高可用-了解" class="headerlink" title="lvs高可用(了解)"></a>lvs高可用(了解)</h2><h3 id="什么是高可用"><a href="#什么是高可用" class="headerlink" title="什么是高可用"></a>什么是高可用</h3><p>lvs作为负载均衡器，所有请求都先到达lvs，可见lvs处于非常重要的位置，如果lvs服务器宕机后端web服务将无法提供服务，影响严重。</p><p>为了屏蔽负载均衡服务器的宕机，需要建立一个备份机。主服务器和备份机上都运行高可用（High Availability）监控程序，通过传送诸如“I am alive”这样的信息来监控对方的运行状况。当备份机不能在一定的时间内收到这样的信息时，它就接管主服务器的服务IP并继续提供负载均衡服务；当备份管理器又从主管理器收到“I am alive”这样的信息时，它就释放服务IP地址，这样的主服务器就开始再次提供负载均衡服务。</p><h3 id="keepalived-lvs实现主备"><a href="#keepalived-lvs实现主备" class="headerlink" title="keepalived+lvs实现主备"></a>keepalived+lvs实现主备</h3><h4 id="什么是keepalived"><a href="#什么是keepalived" class="headerlink" title="什么是keepalived"></a>什么是keepalived</h4><p>keepalived是集群管理中保证集群高可用的一个服务软件，用来防止单点故障。</p><p>Keepalived的作用是检测web服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的web服务器从系统中剔除，当web服务器工作正常后Keepalived自动将web服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的web服务器。</p><h4 id="keepalived工作原理"><a href="#keepalived工作原理" class="headerlink" title="keepalived工作原理"></a>keepalived工作原理</h4><p>keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。</p><p>虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内其他机器的默认路由为该vip），master会发组播，当backup收不到VRRP包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master。这样的话就可以保证路由器的高可用了。</p><p>keepalived主要有三个模块，分别是core、check和VRRP。core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式。VRRP模块是来实现VRRP协议的。</p><h4 id="keepalived-lvs实现主备过程"><a href="#keepalived-lvs实现主备过程" class="headerlink" title="keepalived+lvs实现主备过程"></a>keepalived+lvs实现主备过程</h4><p><strong>初始状态</strong></p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/16.png" alt="img"></p><p><strong>主机宕机</strong></p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/17.png" alt="img"></p><p><strong>主机恢复</strong></p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/18.png" alt="img"> </p><h3 id="准备环境-1"><a href="#准备环境-1" class="headerlink" title="准备环境"></a>准备环境</h3><p>vip：192.168.101.100</p><p>lvs-director：192.168.101.8   主lvs</p><p>lvs-director：192.168.101.9   备lvs</p><p>nginx1：192.168.101.3                              安装nginx</p><p>nginx2：192.168.101.4                              安装nginx</p><p>tomcat1：192.168.101.5                           安装tomcat</p><p>tomcat2：192.168.101.6                           安装tomcat</p><h3 id="安装keepalived"><a href="#安装keepalived" class="headerlink" title="安装keepalived"></a>安装keepalived</h3><p>分别在主备lvs上安装keepalived，参考“<a href="https://www.cnblogs.com/arjenlee/p/9256068.html" rel="noopener" target="_blank">安装手册</a>”进行安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install keepalived -y</span><br></pre></td></tr></table></figure><h3 id="配置keepalived"><a href="#配置keepalived" class="headerlink" title="配置keepalived"></a>配置keepalived</h3><h4 id="主lvs"><a href="#主lvs" class="headerlink" title="主lvs"></a>主lvs</h4><p>修改主lvs下/etc/keepalived/keepalived.conf文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">    #xxxx@itcast.com                                   # 发生故障时发送的邮箱</span><br><span class="line">   &#125;</span><br><span class="line">   #notification_email_from xxxx@itcast.com             # 使用哪个邮箱发送</span><br><span class="line">   #smtp_server xxx.com                                  # 发件服务器</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER             # 标示为主lvs</span><br><span class="line">    interface eth0           # HA检测端口</span><br><span class="line">    virtual_router_id 51     # 主备的virtual_router_id 必须相同</span><br><span class="line">    priority 100             # 优先级,备lvs要比主lvs稍小</span><br><span class="line">    advert_int 1             # VRRP Multicast 广播周期秒数</span><br><span class="line">    authentication &#123;         # 定义认证</span><br><span class="line">        auth_type PASS       # 认证方式为口令认证</span><br><span class="line">        auth_pass 1111       # 定义口令</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;      # 定义vip</span><br><span class="line">        192.168.101.100        # 多个vip可换行添加</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">virtual_server 192.168.101.100 80 &#123;</span><br><span class="line">    delay_loop 6       # 每隔6秒查看realserver状态</span><br><span class="line">    lb_algo wlc        # 调度算法为加权最小连接数</span><br><span class="line">    lb_kind DR         # lvs工作模式为DR(直接路由)模式</span><br><span class="line">    nat_mask 255.255.255.0</span><br><span class="line">    persistence_timeout 50  # 同一IP 的连接50秒内被分配到同一台realserver(测试时建议改为0)</span><br><span class="line">    protocol TCP            # 用TCP监测realserver的状态</span><br><span class="line"></span><br><span class="line">    real_server 192.168.101.3 80 &#123;       # 定义realserver</span><br><span class="line">        weight 3                       # 定义权重</span><br><span class="line">        TCP_CHECK &#123;  # 注意TCP_CHECK和&#123;之间的空格,如果没有的话只会添加第一个realserver</span><br><span class="line">            connect_timeout 3          # 三秒无响应超时</span><br><span class="line">            nb_get_retry 3</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">            connect_port 80</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    real_server 192.168.101.4 80 &#123;</span><br><span class="line">        weight 3</span><br><span class="line">        TCP_CHECK &#123;</span><br><span class="line">            connect_timeout 3</span><br><span class="line">            nb_get_retry 3</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">            connect_port 80</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="备lvs"><a href="#备lvs" class="headerlink" title="备lvs"></a>备lvs</h4><p>修改备lvs下/etc/keepalived/keepalived.conf文件</p><p><strong>配置备</strong>lvs<strong>时需要注意：需要修改state**</strong>为BACKUP , priority<strong><strong>比MASTER</strong></strong>低，virtual_router_id<strong><strong>和master</strong></strong>的值一致**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">    #xxxx@itcast.com                                   # 发生故障时发送的邮箱</span><br><span class="line">   &#125;</span><br><span class="line">   #notification_email_from xxxx@itcast.com             # 使用哪个邮箱发送</span><br><span class="line">   #smtp_server xxx.com                                  # 发件服务器</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP             # 标示为备lvs</span><br><span class="line">    interface eth0           # HA检测端口</span><br><span class="line">    virtual_router_id 51     # 主备的virtual_router_id 必须相同</span><br><span class="line">    priority 99              # 优先级,备lvs要比主lvs稍小</span><br><span class="line">    advert_int 1             # VRRP Multicast 广播周期秒数</span><br><span class="line">    authentication &#123;         # 定义认证</span><br><span class="line">        auth_type PASS       # 认证方式为口令认证</span><br><span class="line">        auth_pass 1111       # 定义口令</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;      # 定义vip</span><br><span class="line">        192.168.101.100        # 多个vip可换行添加</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">virtual_server 192.168.101.100 80 &#123;</span><br><span class="line">    delay_loop 6       # 每隔6秒查看realserver状态</span><br><span class="line">    lb_algo wlc        # 调度算法为加权最小连接数</span><br><span class="line">    lb_kind DR         # lvs工作模式为DR(直接路由)模式</span><br><span class="line">    nat_mask 255.255.255.0</span><br><span class="line">    persistence_timeout 50  # 同一IP 的连接50秒内被分配到同一台realserver(测试时建议改为0)</span><br><span class="line">    protocol TCP            # 用TCP监测realserver的状态</span><br><span class="line"></span><br><span class="line">    real_server 192.168.101.3 80 &#123;       # 定义realserver</span><br><span class="line">        weight 3                       # 定义权重</span><br><span class="line">        TCP_CHECK &#123;  # 注意TCP_CHECK和&#123;之间的空格,如果没有的话只会添加第一个realserver</span><br><span class="line">            connect_timeout 3          # 三秒无响应超时</span><br><span class="line">            nb_get_retry 3</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">            connect_port 80</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    real_server 192.168.101.4 80 &#123;</span><br><span class="line">        weight 3</span><br><span class="line">        TCP_CHECK &#123;</span><br><span class="line">            connect_timeout 3</span><br><span class="line">            nb_get_retry 3</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">            connect_port 80</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h3><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><ul><li>director Server启动：</li></ul><p><strong>注意：使用keepalived就不用手动配置启动lvs，在主、备lvs上启动keepalived即可。</strong></p><p>主备lvs（192.168.101.8、192.168.101.9）都启动keepalived。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service keepalived start</span><br></pre></td></tr></table></figure><ul><li>real server启动：</li></ul><p>192.168.101.3、192.168.101.4启动nginx和lvs的realserver配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/nginx/sbin</span><br><span class="line">./nginx -c /usr/<span class="built_in">local</span>/nginx/conf/nginx-lvs.conf</span><br></pre></td></tr></table></figure><p> 启动lvs的realserver配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service lvsdr start</span><br></pre></td></tr></table></figure><p> <strong>注意：real server的lvs配置需要使用lvsdr脚本。</strong></p><ul><li>tomcat 启动</li></ul><p>略</p><h4 id="初始状态"><a href="#初始状态" class="headerlink" title="初始状态"></a>初始状态</h4><p>查看主lvs的eth0设置：</p><p>vip绑定在主lvs的eth0上。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/19.png" alt="img"></p><p>查询lvs状态：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/20.png" alt="img"></p><p>查看备lvs的eth0设置：</p><p>vip没有绑定在备lvs的eth0上。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/21.png" alt="img"></p><p>访问<a href="http://192.168.101.100，可以正常负载。" rel="noopener" target="_blank">http://192.168.101.100，可以正常负载。</a></p><h4 id="主机宕机"><a href="#主机宕机" class="headerlink" title="主机宕机"></a>主机宕机</h4><p>将主lvs的keepalived停止或将主lvs关机(相当于模拟宕机)，查看主lvs的eth0：</p><p>eth0没有绑定vip</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/22.png" alt="img"> </p><p>查看备lvs的eth0：</p><p>vip已经漂移到备lvs。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/23.png" alt="img"></p><p>访问<a href="http://192.168.101.100，可以正常负载。" rel="noopener" target="_blank">http://192.168.101.100，可以正常负载。</a></p><h4 id="主机恢复"><a href="#主机恢复" class="headerlink" title="主机恢复"></a>主机恢复</h4><p>将主lvs的keepalived启动。</p><p>查看主lvs的eth0：</p><p>查看备lvs的eth0：</p><p>vip漂移到主lvs。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/24.png" alt="img"></p><p>查看备lvs的eth0：</p><p>eth0没有绑定vip</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/25.png" alt="img"></p><p>访问<a href="http://192.168.101.100，可以正常负载。" rel="noopener" target="_blank">http://192.168.101.100，可以正常负载。</a></p><h3 id="keepalived-lvs实现双主"><a href="#keepalived-lvs实现双主" class="headerlink" title="keepalived+lvs实现双主"></a>keepalived+lvs实现双主</h3><p>上边主备方案是当前只有一台lvs工作，这造成资源浪费，可以采用双主结构，让两台lvs当前都进行工作，采用dns轮询方式，当用户访问域名通过dns轮询每台lvs，双主结构需要两个vip，这两个vip要绑定域名。</p><p> 同样，在每台lvs上安装keepalived软件，当keepalived检测到其中一个lvs宕机则将宕机的vip漂移到活动lvs上，当lvs恢复则vip又重新漂移回来。</p><h4 id="初始状态-1"><a href="#初始状态-1" class="headerlink" title="初始状态"></a>初始状态</h4><p>每台lvs绑定一个vip，共两个vip，DNS设置域名对应这两个vip，通过DNS轮询每次解析到不同的vip上即解析到不同的lvs上。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/26.png" alt="img"></p><h4 id="其中一个主机宕机"><a href="#其中一个主机宕机" class="headerlink" title="其中一个主机宕机"></a>其中一个主机宕机</h4><p>其中一个主机宕机，每台lvs上安装的keepalived程序会检测到对方宕机，将宕机一方的vip漂移至活动的lvs服务器上，这样DNS轮询全部到一台lvs继续对外提供服务。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/27.png" alt="img"></p><h4 id="主机恢复-1"><a href="#主机恢复-1" class="headerlink" title="主机恢复"></a>主机恢复</h4><p>当主机恢复又回到初始状态，每个vip绑定在不同的lvs上。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/28.png" alt="img"></p><h2 id="lvs扩展的思考"><a href="#lvs扩展的思考" class="headerlink" title="lvs扩展的思考"></a>lvs扩展的思考</h2><p>前端使用1到2台lvs作为负载基本可以满足中小型网站的并发要求，当lvs的负载成为瓶颈此时就需要对lvs进行优化、扩展。</p><ul><li><strong>方案1：LVS-ospf集群</strong></li></ul><p>​         OSPF(Open Shortest Path First开放式最短路径优先）是一个内部网关协议(Interior Gateway Protocol，简称IGP），用于在单一自治系统（autonomous system,AS）内决策路由。</p><p>LVS（DR）通过ospfd，做lvs集群，实现一个VIP，多台LVS同时工作提供服务，这种方案需要依赖三层交换机设备实现。</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/29.png" alt="img"> </p><p>用户请求（VIP：42.xx.xx.100）到达三层交换机之后，通过对原地址、端口和目的地址、端口的hash，将链接分配到集群中的某一台LVS上，LVS通过内网（10.101.10.x）向后端转发请求，后端再将数据返回给用户。</p><p>LVS-ospf集群模式的最大优势就在于：</p><p>1.LVS调度机自由伸缩，横向线性扩展（最大8台，受限于三层设备允许的等价路由数目maximum load-balancing）；</p><p>2.LVS机器同时工作，不存在备机，提高利用率；</p><p>3.做到了真正的高可用，某台LVS机器宕机后，不会影响服务</p><ul><li><strong>方案2：DNS轮询</strong></li></ul><p>上面讲的是一组双主结构，可以采用多组双主结构达到横向扩展lvs的目的，此方案需要每台lvs都绑定一个vip（公网ip），DNS设置域名轮询多个vip，如下图：</p><p> <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/17e1663a/30.png" alt="img"></p><ul><li><strong>方案3：使用硬件负载均衡设置</strong> </li></ul><p>​         如果资金允许可以购买硬件设置来完成负载均衡，性能不错的有F5、Array等都可以满足超高并发的要求。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过本文掌握什么是负载均衡及负载均衡的作用和意义；了解lvs负载均衡的三种模式；了解lvs-DR负载均衡部署方法；掌握nginx实现负载均衡的方法；掌握lvs+nginx负载均衡拓扑结构。&lt;/p&gt;
    
    </summary>
    
      <category term="运维技术" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Lvs" scheme="https://wandouduoduo.github.io/tags/Lvs/"/>
    
  </entry>
  
  <entry>
    <title>k8s网络flannel和calico网络模式对比</title>
    <link href="https://wandouduoduo.github.io/articles/8b98d1d.html"/>
    <id>https://wandouduoduo.github.io/articles/8b98d1d.html</id>
    <published>2021-04-12T03:40:08.000Z</published>
    <updated>2021-04-20T07:31:14.800Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Kubernetes跨主机容器之间的通信组件，目前主流的是flannel和calico，本文对两个组件进行简单介绍和对比。</p><a id="more"></a><h2 id="Flannel-架构"><a href="#Flannel-架构" class="headerlink" title="Flannel 架构"></a>Flannel 架构</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a><strong>原理</strong></h3><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/2.png" alt="img"></p><p>由CoreOS开发的项目Flannel，可能是最直接和最受欢迎的CNI插件。它是容器编排系统中最成熟的网络结构示例之一，旨在实现更好的容器间和主机间网络。随着CNI概念的兴起，Flannel CNI插件算是早期的入门。</p><p>与其他方案相比，Flannel相对容易安装和配置。它被打包为单个二进制文件FlannelD，许多常见的Kubernetes集群部署工具和许多Kubernetes发行版都可以默认安装Flannel。Flannel可以使用Kubernetes集群的现有etcd集群来使用API存储其状态信息，因此不需要专用的数据存储。</p><p>Flannel配置第3层IPv4 Overlay网络。它会创建一个大型内部网络，跨越集群中每个节点。在此Overlay网络中，每个节点都有一个子网，用于在内部分配IP地址。在配置Pod时，每个节点上的Docker桥接口都会为每个新容器分配一个地址。同一主机中的Pod可以使用Docker桥接进行通信，而不同主机上的pod会使用flanneld将其流量封装在UDP数据包中，以便路由到适当的目标。</p><p>Flannel有几种不同类型的后端可用于封装和路由。默认和推荐的方法是使用VXLAN，因为VXLAN性能更良好并且需要的手动干预更少。</p><h2 id="Calico-架构"><a href="#Calico-架构" class="headerlink" title="Calico 架构"></a><strong>Calico 架构</strong></h2><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/1.jpg" alt></p><p>calico包括如下重要组件：Felix，etcd，BGP Client，BGP Route Reflector。下面分别说明一下这些组件。</p><p>Felix：主要负责路由配置以及ACLS规则的配置以及下发，它存在在每个node节点上。</p><p>etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；</p><p>BGPClient(BIRD), 主要负责把 Felix写入 kernel的路由信息分发到当前 Calico网络，确保 workload间的通信的有效性；</p><p>BGPRoute Reflector(BIRD), 大规模部署时使用，摒弃所有节点互联的mesh模式，通过一个或者多个 BGPRoute Reflector 来完成集中式的路由分发；</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/3.png" alt="img"></p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>如下图所示，描述了从源容器经过源宿主机，经过数据中心的路由，然后到达目的宿主机最后分配到目的容器的过程。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/4.png" alt="img"></p><h3 id="跨主机通信"><a href="#跨主机通信" class="headerlink" title="跨主机通信"></a>跨主机通信</h3><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/6.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/7.jpg" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从上述的原理可以看出，flannel在进行路由转发的基础上进行了封包解包的操作，这样浪费了CPU的计算资源。下图是从网上找到的各个开源网络组件的性能对比。可以看出无论是带宽还是网络延迟，calico和主机的性能是差不多的。<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8b98d1d/5.png" alt="img">  </p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes跨主机容器之间的通信组件，目前主流的是flannel和calico，本文对两个组件进行简单介绍和对比。&lt;/p&gt;
    
    </summary>
    
      <category term="容器编排" scheme="https://wandouduoduo.github.io/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
      <category term="K8s" scheme="https://wandouduoduo.github.io/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/K8s/"/>
    
    
      <category term="K8s" scheme="https://wandouduoduo.github.io/tags/K8s/"/>
    
  </entry>
  
  <entry>
    <title>Docker快速安装jumperserver</title>
    <link href="https://wandouduoduo.github.io/articles/4c63da33.html"/>
    <id>https://wandouduoduo.github.io/articles/4c63da33.html</id>
    <published>2021-04-09T03:57:50.000Z</published>
    <updated>2021-04-09T05:51:39.892Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Jumpserver堡垒机的作用和好处这里就不再赘述，本文教你快速用docker容器安装jumperserver，让你快速体验。本教程是在单机上操作，处于以后扩展的需求，强烈建议在多台服务器上搭建。</p><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">yum -y install wget</span><br><span class="line">wget https://github.com/jumpserver/installer/releases/download/v2.8.2/jumpserver-installer-v2.8.2.tar.gz</span><br><span class="line">tar -xf jumpserver-installer-v2.8.2.tar.gz</span><br><span class="line"><span class="built_in">cd</span> jumpserver-installer-v2.8.2</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim config-example.txt</span><br><span class="line">所有配置都在此文件中，按照实际情况填写信息即可。</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">./jmsctl.sh install</span><br><span class="line"></span><br><span class="line">       ██╗██╗   ██╗███╗   ███╗██████╗ ███████╗███████╗██████╗ ██╗   ██╗███████╗██████╗</span><br><span class="line">       ██║██║   ██║████╗ ████║██╔══██╗██╔════╝██╔════╝██╔══██╗██║   ██║██╔════╝██╔══██╗</span><br><span class="line">       ██║██║   ██║██╔████╔██║██████╔╝███████╗█████╗  ██████╔╝██║   ██║█████╗  ██████╔╝</span><br><span class="line">  ██   ██║██║   ██║██║╚██╔╝██║██╔═══╝ ╚════██║██╔══╝  ██╔══██╗╚██╗ ██╔╝██╔══╝  ██╔══██╗</span><br><span class="line">  ╚█████╔╝╚██████╔╝██║ ╚═╝ ██║██║     ███████║███████╗██║  ██║ ╚████╔╝ ███████╗██║  ██║</span><br><span class="line">   ╚════╝  ╚═════╝ ╚═╝    ╚═╝╚═╝     ╚══════╝╚══════╝╚═╝  ╚═╝  ╚═══╝  ╚══════╝╚═╝  ╚═╝</span><br><span class="line"></span><br><span class="line">                                                             Version:  v2.8.2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; 安装配置 Docker</span><br><span class="line">1. 安装 Docker</span><br><span class="line">开始下载 Docker 程序 ...</span><br><span class="line">完成</span><br><span class="line">开始下载 Docker Compose 程序 ...</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">2. 配置 Docker</span><br><span class="line">是否需要自定义 Docker 数据目录, 默认将使用 /var/lib/docker 目录? (y/n)  (默认为 n): n</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">3. 启动 Docker</span><br><span class="line">Docker 版本发生改变 或 Docker 配置文件发生变化，是否要重启? (y/n)  (默认为 y): y</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; 加载 Docker 镜像</span><br><span class="line">Docker: Pulling from jumpserver/core:v2.8.2         [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/koko:v2.8.2         [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/luna:v2.8.2         [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/nginx:alpine2       [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/redis:6-alpine      [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/lina:v2.8.2         [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/mysql:5             [ OK ]</span><br><span class="line">Docker: Pulling from jumpserver/guacamole:v2.8.2    [ OK ]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; 安装配置 JumpServer</span><br><span class="line">1. 检查配置文件</span><br><span class="line">配置文件位置: /opt/jumpserver/config</span><br><span class="line">/opt/jumpserver/config/config.txt                 [ √ ]</span><br><span class="line">/opt/jumpserver/config/nginx/lb_http_server.conf  [ √ ]</span><br><span class="line">/opt/jumpserver/config/nginx/lb_ssh_server.conf   [ √ ]</span><br><span class="line">/opt/jumpserver/config/core/config.yml   [ √ ]</span><br><span class="line">/opt/jumpserver/config/koko/config.yml   [ √ ]</span><br><span class="line">/opt/jumpserver/config/mysql/my.cnf      [ √ ]</span><br><span class="line">/opt/jumpserver/config/redis/redis.conf  [ √ ]</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">2. 配置 Nginx</span><br><span class="line">配置文件位置:: /opt/jumpserver/config/nginx/cert</span><br><span class="line">/opt/jumpserver/config/nginx/cert/server.crt  [ √ ]</span><br><span class="line">/opt/jumpserver/config/nginx/cert/server.key  [ √ ]</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">3. 备份配置文件</span><br><span class="line">备份至 /opt/jumpserver/config/backup/config.txt.2021-03-19_08-01-51</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">4. 配置网络</span><br><span class="line">是否需要支持 IPv6? (y/n)  (默认为 n): n</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">5. 配置加密密钥</span><br><span class="line">SECRETE_KEY:     ICAgIGluZXQ2IDI0MDk6OGE0ZDpjMjg6ZjkwMTo6ZDRjLzEyO</span><br><span class="line">BOOTSTRAP_TOKEN: ICAgIGluZXQ2IDI0</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">6. 配置持久化目录</span><br><span class="line">是否需要自定义持久化存储, 默认将使用目录 /opt/jumpserver? (y/n)  (默认为 n): n</span><br><span class="line">完成</span><br><span class="line"></span><br><span class="line">7. 配置 MySQL</span><br><span class="line">是否使用外部mysql (y/n)  (默认为n): n</span><br><span class="line"></span><br><span class="line">8. 配置 Redis</span><br><span class="line">是否使用外部redis  (y/n)  (默认为n): n</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; 安装完成了</span><br><span class="line">1. 可以使用如下命令启动, 然后访问</span><br><span class="line">./jmsctl.sh start</span><br><span class="line"></span><br><span class="line">2. 其它一些管理命令</span><br><span class="line">./jmsctl.sh stop</span><br><span class="line">./jmsctl.sh restart</span><br><span class="line">./jmsctl.sh backup</span><br><span class="line">./jmsctl.sh upgrade</span><br><span class="line">更多还有一些命令, 你可以 ./jmsctl.sh --<span class="built_in">help</span> 来了解</span><br><span class="line"></span><br><span class="line">3. Web 访问</span><br><span class="line">http://192.168.100.248:8080</span><br><span class="line">https://192.168.100.248:8443</span><br><span class="line">默认用户: admin  默认密码: admin</span><br><span class="line"></span><br><span class="line">4. SSH/SFTP 访问</span><br><span class="line">ssh admin@192.168.100.248 -p2222</span><br><span class="line">sftp -P2222 admin@192.168.100.248</span><br><span class="line"></span><br><span class="line">5. 更多信息</span><br><span class="line">我们的官网: https://www.jumpserver.org/</span><br><span class="line">我们的文档: https://docs.jumpserver.org/</span><br></pre></td></tr></table></figure></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Jumpserver堡垒机的作用和好处这里就不再赘述，本文教你快速用docker容器安装jumperserver，让你快速体验。本教程是在单机上操作，处于以后扩展的需求，强烈建议在多台服务器上搭建。&lt;/p&gt;
    
    </summary>
    
      <category term="容器技术" scheme="https://wandouduoduo.github.io/categories/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Docker" scheme="https://wandouduoduo.github.io/tags/Docker/"/>
    
      <category term="Jumperserver" scheme="https://wandouduoduo.github.io/tags/Jumperserver/"/>
    
  </entry>
  
  <entry>
    <title>微服务之注册中心选型</title>
    <link href="https://wandouduoduo.github.io/articles/997cf319.html"/>
    <id>https://wandouduoduo.github.io/articles/997cf319.html</id>
    <published>2021-03-29T06:04:50.000Z</published>
    <updated>2021-03-29T10:42:31.059Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>服务注册中心本质上是为了解耦服务提供者和服务消费者。对于任何一个微服务，原则上都应存在或者支持多个提供者，这是由微服务的分布式属性决定的。更进一步，为了支持弹性扩缩容特性，一个微服务的提供者的数量和分布往往是动态变化的，也是无法预先确定的。因此，原本在单体应用阶段常用的静态LB机制就不再适用了，需要引入额外的组件来管理微服务提供者的注册与发现，而这个组件就是服务注册中心。</p><a id="more"></a><h2 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h2><h3 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h3><p>CAP理论是分布式架构中重要理论</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一致性(Consistency) (所有节点在同一时间具有相同的数据)</span><br><span class="line">可用性(Availability) (保证每个请求不管成功或者失败都有响应)</span><br><span class="line">分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作)</span><br></pre></td></tr></table></figure><h3 id="CAP理解"><a href="#CAP理解" class="headerlink" title="CAP理解"></a>CAP理解</h3><p>P的理解是在整个系统中某个部分挂掉或者宕机了，并不影响整个系统的运作或者使用，是网络层面的，通常认为网络是顺畅流通的。</p><p>A可用性是系统的某个节点挂了，但并不影响系统的接受请求或者发出响应。</p><p>C一致性是客户端请求系统中的任意节点，获取的返回结果都是一致的。系统中各个节点会实时同步信息来保证</p><h3 id="CAP侧重"><a href="#CAP侧重" class="headerlink" title="CAP侧重"></a>CAP侧重</h3><p>但CAP 3项不可能都取，只能取其中2两项，造成侧重点不同。</p><p>如果C是第一需求的话，那么会影响A的性能，因为要数据同步，不然请求结果会有差异，但是数据同步会消耗时间，期间可用性就会降低。</p><p>如果A是第一需求，那么只要有一个服务在，就能正常接受请求，但是对与返回结果一致就不能保证，原因是，在分布式部署的时候，数据一致的过程不可能想切线路那么快。</p><p>再如果，同事满足一致性和可用性，那么分区容错就很难保证了，只能是单点，也是分布式的基本核心。好了，明白这些理论，就可以在相应的场景选取服务注册与发现了</p><h2 id="服务注册中心解决方案"><a href="#服务注册中心解决方案" class="headerlink" title="服务注册中心解决方案"></a>服务注册中心解决方案</h2><p>设计或者选型一个服务注册中心，首先要考虑的就是服务注册与发现机制。纵观当下各种主流的服务注册中心解决方案，大致可归为三类：</p><ul><li>应用内：直接集成到应用中，依赖于应用自身完成服务的注册与发现。最典型的是Netflix提供的Eureka</li><li>应用外：把应用当成黑盒，通过应用外的某种机制将服务注册到注册中心，最小化对应用的侵入性，比如Airbnb的SmartStack，HashiCorp的Consul</li><li>DNS：将服务注册为DNS的SRV记录，严格来说，是一种特殊的应用外注册方式。SkyDNS是其中的代表</li></ul><p><em>注1：对于第一类注册方式，除了Eureka这种一站式解决方案，还可以基于ZooKeeper或者Etcd自行实现一套服务注册机制，这在大公司比较常见，但对于小公司而言显然性价比太低。</em></p><p><em>注2：由于DNS固有的缓存缺陷，本文不对第三类注册方式作深入探讨。</em></p><p>除了基本的服务注册与发现机制，从开发和运维角度，至少还要考虑如下五个方面：</p><ul><li>测活：服务注册之后，如何对服务进行测活以保证服务的可用性？</li><li>负载均衡：当存在多个服务提供者时，如何均衡各个提供者的负载？</li><li>集成：在服务提供端或者调用端，如何集成注册中心？</li><li>运行时依赖：引入注册中心之后，对应用的运行时环境有何影响？</li><li>可用性：如何保证注册中心本身的可用性，特别是消除单点故障？</li></ul><h2 id="主流注册中心产品"><a href="#主流注册中心产品" class="headerlink" title="主流注册中心产品"></a>主流注册中心产品</h2><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/997cf319/1.png" alt></p><p><em>Consul是支持自动注销服务实例， 请见文档： <a href="https://www.consul.io/api-docs/agent/service，在check的" rel="noopener" target="_blank">https://www.consul.io/api-docs/agent/service，在check的</a> DeregisterCriticalServiceAfter 这个参数</em><br><em>新版本的Dubbo也扩展了对 Consul 的支持。 参考: <a href="https://github.com/apache/dubbo/tree/master/dubbo-registry" rel="noopener" target="_blank">https://github.com/apache/dubbo/tree/master/dubbo-registry</a></em></p><h3 id="Zookeeper-gt-CP"><a href="#Zookeeper-gt-CP" class="headerlink" title="Zookeeper -&gt; CP"></a>Zookeeper -&gt; CP</h3><p>与 Eureka 有所不同，Zookeeper 在设计时就紧遵CP原则，即任何时候对 Zookeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性，但是 Zookeeper 不能保证每次服务请求都是可达的。</p><p>从 Zookeeper 的实际应用情况来看，在使用 Zookeeper 获取服务列表时，如果此时的 Zookeeper 集群中的 Leader 宕机了，该集群就要进行 Leader 的选举，又或者 Zookeeper 集群中半数以上服务器节点不可用（例如有三个节点，如果节点一检测到节点三挂了 ，节点二也检测到节点三挂了，那这个节点才算是真的挂了），那么将无法处理该请求。所以说，Zookeeper不能保证服务可用性。</p><p>当然，在大多数分布式环境中，尤其是涉及到数据存储的场景，数据一致性应该是首先被保证的，这也是 Zookeeper 设计紧遵CP原则的另一个原因。</p><p>但是对于服务发现来说，情况就不太一样了，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。</p><p>因为对于服务消费者来说，能消费才是最重要的，消费者虽然拿到可能不正确的服务实例信息后尝试消费一下，也要胜过因为无法获取实例信息而不去消费，导致系统异常要好（淘宝的双十一，京东的618就是紧遵AP的最好参照）。</p><p>当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30~120s，而且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。</p><p>在云部署环境下， 因为网络问题使得zk集群失去master节点是大概率事件，虽然服务能最终恢复，但是漫长的选举事件导致注册长期不可用是不能容忍的。</p><h3 id="Eureka-gt-AP"><a href="#Eureka-gt-AP" class="headerlink" title="Eureka  -&gt; AP"></a>Eureka  -&gt; AP</h3><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/997cf319/2.png" alt></p><p>Spring Cloud Netflix 在设计 Eureka 时就紧遵AP原则（尽管现在2.0发布了，但是由于其闭源的原因 ，但是目前 Ereka 1.x 任然是比较活跃的）。</p><p>Eureka Server 也可以运行多个实例来构建集群，解决单点问题，但不同于 ZooKeeper 的选举 leader 的过程，Eureka Server 采用的是Peer to Peer 对等通信。这是一种去中心化的架构，无 master/slave 之分，每一个 Peer 都是对等的。在这种架构风格中，节点通过彼此互相注册来提高可用性，每个节点需要添加一个或多个有效的 serviceUrl 指向其他节点。每个节点都可被视为其他节点的副本。</p><p>在集群环境中如果某台 Eureka Server 宕机，Eureka Client 的请求会自动切换到新的 Eureka Server 节点上，当宕机的服务器重新恢复后，Eureka 会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会在节点间进行复制（replicate To Peer）操作，将请求复制到该 Eureka Server 当前所知的其它所有节点中。</p><p>当一个新的 Eureka Server 节点启动后，会首先尝试从邻近节点获取所有注册列表信息，并完成初始化。Eureka Server 通过 getEurekaServiceUrls() 方法获取所有的节点，并且会通过心跳契约的方式定期更新。</p><p>默认情况下，如果 Eureka Server 在一定时间内没有接收到某个服务实例的心跳（默认周期为30秒），Eureka Server 将会注销该实例（默认为90秒， eureka.instance.lease-expiration-duration-in-seconds 进行自定义配置）。</p><p>当 Eureka Server 节点在短时间内丢失过多的心跳时，那么这个节点就会进入自我保护模式。</p><p>Eureka的集群中，只要有一台Eureka还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况：</p><ul><li>Eureka不再从注册表中移除因为长时间没有收到心跳而过期的服务；</li><li>Eureka仍然能够接受新服务注册和查询请求，但是不会被同步到其它节点上（即保证当前节点依然可用）</li><li>当网络稳定时，当前实例新注册的信息会被同步到其它节点中；</li></ul><p>因此，Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使得整个注册服务瘫痪。</p><h3 id="Consul"><a href="#Consul" class="headerlink" title="Consul"></a>Consul</h3><p>Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。Consul 使用 Go 语言编写，因此具有天然可移植性（支持Linux、windows和Mac OS X）。</p><p>Consul 内置了服务注册与发现框架、分布一致性协议实现、健康检查、Key/Value 存储、多数据中心方案，不再需要依赖其他工具（比如 ZooKeeper 等），使用起来也较为简单。</p><p>Consul 遵循CAP原理中的CP原则，保证了强一致性和分区容错性，且使用的是Raft算法，比zookeeper使用的Paxos算法更加简单。虽然保证了强一致性，但是可用性就相应下降了，例如服务注册的时间会稍长一些，因为 Consul 的 raft 协议要求必须过半数的节点都写入成功才认为注册成功 ；在leader挂掉了之后，重新选举出leader之前会导致Consul 服务不可用。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/997cf319/3.png" alt></p><p>Consul本质上属于应用外的注册方式，但可以通过SDK简化注册流程。而服务发现恰好相反，默认依赖于SDK，但可以通过Consul Template（下文会提到）去除SDK依赖。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/997cf319/4.png" alt></p><p><strong>Consul Template</strong></p><p>Consul，默认服务调用者需要依赖Consul SDK来发现服务，这就无法保证对应用的零侵入性。</p><p>所幸通过Consul Template，可以定时从Consul集群获取最新的服务提供者列表并刷新LB配置（比如nginx的upstream），这样对于服务调用者而言，只需要配置一个统一的服务调用地址即可。</p><p>Consul强一致性(C)带来的是：</p><p>服务注册相比Eureka会稍慢一些。因为Consul的raft协议要求必须过半数的节点都写入成功才认为注册成功<br>Leader挂掉时，重新选举期间整个consul不可用。保证了强一致性但牺牲了可用性。</p><p>Eureka保证高可用(A)和最终一致性：</p><p>服务注册相对要快，因为不需要等注册信息replicate到其他节点，也不保证注册信息是否replicate成功<br>当数据出现不一致时，虽然A, B上的注册信息不完全相同，但每个Eureka节点依然能够正常对外提供服务，这会出现查询服务信息时如果请求A查不到，但请求B就能查到。如此保证了可用性但牺牲了一致性。<br>其他方面，eureka就是个servlet程序，跑在servlet容器中; Consul则是go编写而成。</p><h3 id="Nacos"><a href="#Nacos" class="headerlink" title="Nacos"></a>Nacos</h3><p>Nacos是阿里开源的，Nacos 支持基于 DNS 和基于 RPC 的服务发现。在Spring Cloud中使用Nacos，只需要先下载 Nacos 并启动 Nacos server，Nacos只需要简单的配置就可以完成服务的注册发现。</p><p>Nacos除了服务的注册发现之外，还支持动态配置服务。动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。</p><p>一句话概括就是Nacos = Spring Cloud注册中心 + Spring Cloud配置中心。</p><p>参考链接：</p><p><a href="https://yq.aliyun.com/articles/698930" rel="noopener" target="_blank">https://yq.aliyun.com/articles/698930</a></p><p><a href="https://nacos.io" rel="noopener" target="_blank">https://nacos.io</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;服务注册中心本质上是为了解耦服务提供者和服务消费者。对于任何一个微服务，原则上都应存在或者支持多个提供者，这是由微服务的分布式属性决定的。更进一步，为了支持弹性扩缩容特性，一个微服务的提供者的数量和分布往往是动态变化的，也是无法预先确定的。因此，原本在单体应用阶段常用的静态LB机制就不再适用了，需要引入额外的组件来管理微服务提供者的注册与发现，而这个组件就是服务注册中心。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>手把手搭建k8s集群和kubesphere</title>
    <link href="https://wandouduoduo.github.io/articles/b645bc81.html"/>
    <id>https://wandouduoduo.github.io/articles/b645bc81.html</id>
    <published>2021-03-17T08:45:52.000Z</published>
    <updated>2021-04-02T11:18:06.871Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>本文将从零开始在干净的机器上安装 <code>Docker、Kubernetes (使用 kubeadm)、Calico、NFS StorageClass等</code>，通过手把手的教程演示如何搭建一个 <code>Kubernetes集群</code>，并在 K8s集群之上安装开源的<code>KubeSphere</code> 容器平台可视化运营集群环境。</p><a id="more"></a><h2 id="环境和版本"><a href="#环境和版本" class="headerlink" title="环境和版本"></a>环境和版本</h2><p>所有机器处于同一内网网段，并且可以互相通信。</p><table><thead><tr><th align="center">机器IP</th><th align="center">工作内容</th></tr></thead><tbody><tr><td align="center">10.220.170.240</td><td align="center">NFS</td></tr><tr><td align="center">10.209.208.238</td><td align="center">master</td></tr><tr><td align="center">10.145.197.182</td><td align="center">nodes0</td></tr><tr><td align="center">10.145.197.176</td><td align="center">nodes1</td></tr><tr><td align="center">10.145.197.120</td><td align="center">nodes2</td></tr><tr><td align="center">10.209.33.24</td><td align="center">nodes3</td></tr></tbody></table><p>Docker版本： v19.03.4</p><p>k8s集群(kubeadm、kubelet 和 kubectl等)版本：v1.17.3</p><p>kubesphere版本：v3.0</p><h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><p>在所有节点上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了方便本操作关闭了防火墙，也建议你这样操作</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭 SeLinux</span></span><br><span class="line">setenforce 0</span><br><span class="line">sed -i <span class="string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭 swap</span></span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br></pre></td></tr></table></figure><p>更换CentOS YUM源为阿里云yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装wget</span></span><br><span class="line">yum install wget -y</span><br><span class="line"><span class="comment"># 备份</span></span><br><span class="line">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br><span class="line"><span class="comment"># 获取阿里云yum源</span></span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"><span class="comment"># 获取阿里云epel源</span></span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"><span class="comment"># 清理缓存并创建新的缓存</span></span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line"><span class="comment"># 系统更新</span></span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><p>时间同步并确认</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">timedatectl</span><br><span class="line">timedatectl <span class="built_in">set</span>-ntp <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h2><h3 id="安装-Docker-1"><a href="#安装-Docker-1" class="headerlink" title="安装 Docker"></a>安装 Docker</h3><p>每台机器上也都要安装Docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 Docker CE</span></span><br><span class="line"><span class="comment"># 设置仓库</span></span><br><span class="line"><span class="comment"># 安装所需包</span></span><br><span class="line">yum install -y yum-utils \</span><br><span class="line">    device-mapper-persistent-data \</span><br><span class="line">    lvm2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 Docker 仓库,速度慢的可以换阿里云的源。</span></span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"><span class="comment"># 阿里云源地址</span></span><br><span class="line"><span class="comment"># http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 Docker CE.</span></span><br><span class="line">yum install -y containerd.io-1.2.10 \</span><br><span class="line">    docker-ce-19.03.4 \</span><br><span class="line">    docker-ce-cli-19.03.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Docker 并添加开机启动</span></span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><h3 id="修改Cgroup-Driver"><a href="#修改Cgroup-Driver" class="headerlink" title="修改Cgroup Driver"></a>修改Cgroup Driver</h3><p>需要将Docker的Cgroup Driver 修改为 <strong>systemd</strong>，不然在为Kubernetes 集群添加节点时会报如下错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行 kubeadm join 的 WARNING 信息</span></span><br><span class="line">[WARNING IsDockerSystemdCheck]: detected <span class="string">"cgroupfs"</span> as the Docker cgroup driver. The recommended driver is <span class="string">"systemd"</span>. Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br></pre></td></tr></table></figure><p>目前 Docker 的 Cgroup Driver 看起来应该是这样的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker info|grep <span class="string">"Cgroup Driver"</span></span><br><span class="line">  Cgroup Driver: cgroupfs</span><br></pre></td></tr></table></figure><p>需要将这个值修改为 <strong>systemd</strong> 。同时将registry替换成国内的一些仓库地址，以免直接在官方仓库拉取镜像会很慢，操作如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup daemon.</span></span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"exec-opts"</span>: [<span class="string">"native.cgroupdriver=systemd"</span>],</span><br><span class="line">    <span class="string">"log-driver"</span>: <span class="string">"json-file"</span>,</span><br><span class="line">    <span class="string">"log-opts"</span>: &#123;</span><br><span class="line">    <span class="string">"max-size"</span>: <span class="string">"100m"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"storage-driver"</span>: <span class="string">"overlay2"</span>,</span><br><span class="line">    <span class="string">"registry-mirrors"</span>:[</span><br><span class="line">        <span class="string">"https://kfwkfulq.mirror.aliyuncs.com"</span>,</span><br><span class="line">        <span class="string">"https://2lqq34jg.mirror.aliyuncs.com"</span>,</span><br><span class="line">        <span class="string">"https://pee6w651.mirror.aliyuncs.com"</span>,</span><br><span class="line">        <span class="string">"http://hub-mirror.c.163.com"</span>,</span><br><span class="line">        <span class="string">"https://docker.mirrors.ustc.edu.cn"</span>,</span><br><span class="line">        <span class="string">"https://registry.docker-cn.com"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line"></span><br><span class="line"><span class="comment"># Restart docker.</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h2 id="安装-kubeadm、kubelet-和-kubectl"><a href="#安装-kubeadm、kubelet-和-kubectl" class="headerlink" title="安装 kubeadm、kubelet 和 kubectl"></a>安装 kubeadm、kubelet 和 kubectl</h2><h3 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h3><p>需要在每台机器上安装以下的软件包：</p><ul><li>kubeadm：用来初始化集群的指令。</li><li>kubelet：在集群中的每个节点上用来启动 pod 和容器等。</li><li>kubectl：用来与集群通信的命令行工具（Worker 节点可以不装，但是我装了，不影响什么）。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置K8S的yum源</span></span><br><span class="line"><span class="comment"># 这部分用是阿里云的源，如果可以访问Google，则建议用官方的源</span></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 官方源配置如下</span></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><p>安装指定版本 kubelet、 kubeadm 、kubectl， 这里选择当前较新的稳定版 Kubernetes 1.17.3，如果选择的版本不一样，在执行集群初始化的时候，注意 –kubernetes-version 的值。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加配置</span></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 加载</span></span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动并设置 kubelet 开机启动</span></span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl <span class="built_in">enable</span> --now kubelet</span><br></pre></td></tr></table></figure><h2 id="使用-Kubeadm创建集群"><a href="#使用-Kubeadm创建集群" class="headerlink" title="使用 Kubeadm创建集群"></a>使用 Kubeadm创建集群</h2><h3 id="初始化Master节点"><a href="#初始化Master节点" class="headerlink" title="初始化Master节点"></a>初始化Master节点</h3><p>在 Master上执行初始化，执行初始化使用 kubeadm init 命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置hosts</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"127.0.0.1 <span class="variable">$(hostname)</span>"</span> &gt;&gt; /etc/hosts</span><br><span class="line"><span class="built_in">export</span> MASTER_IP=10.209.208.238</span><br><span class="line"><span class="built_in">export</span> APISERVER_NAME=kuber4s.api</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;MASTER_IP&#125;</span> <span class="variable">$&#123;APISERVER_NAME&#125;</span>"</span> &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>下面有不带注释的初始化命令，建议先查看带注释的每个参数对应的意义，确保与你的当前配置的环境是一致的，然后再执行初始化操作，避免踩雷。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化 Control-plane/Master 节点</span></span><br><span class="line">kubeadm init \</span><br><span class="line">    --apiserver-advertise-address 0.0.0.0 \</span><br><span class="line">    <span class="comment"># API 服务器所公布的其正在监听的 IP 地址,指定“0.0.0.0”以使用默认网络接口的地址</span></span><br><span class="line">    <span class="comment"># 切记只可以是内网IP，不能是外网IP，如果有多网卡，可以使用此选项指定某个网卡</span></span><br><span class="line">    --apiserver-bind-port 6443 \</span><br><span class="line">    <span class="comment"># API 服务器绑定的端口,默认 6443</span></span><br><span class="line">    --cert-dir /etc/kubernetes/pki \</span><br><span class="line">    <span class="comment"># 保存和存储证书的路径，默认值："/etc/kubernetes/pki"</span></span><br><span class="line">    --control-plane-endpoint kuber4s.api \</span><br><span class="line">    <span class="comment"># 为控制平面指定一个稳定的 IP 地址或 DNS 名称,</span></span><br><span class="line">    <span class="comment"># 这里指定的 kuber4s.api 已经在 /etc/hosts 配置解析为本机IP</span></span><br><span class="line">    --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \</span><br><span class="line">    <span class="comment"># 选择用于拉取Control-plane的镜像的容器仓库，默认值："k8s.gcr.io"</span></span><br><span class="line">    <span class="comment"># 因 Google被墙，这里选择国内仓库</span></span><br><span class="line">    --kubernetes-version 1.17.3 \</span><br><span class="line">    <span class="comment"># 为Control-plane选择一个特定的 Kubernetes 版本， 默认值："stable-1"</span></span><br><span class="line">    --node-name master01 \</span><br><span class="line">    <span class="comment">#  指定节点的名称,不指定的话为主机hostname，默认可以不指定</span></span><br><span class="line">    --pod-network-cidr 10.10.0.0/16 \</span><br><span class="line">    <span class="comment"># 指定pod的IP地址范围</span></span><br><span class="line">    --service-cidr 10.20.0.0/16 \</span><br><span class="line">    <span class="comment"># 指定Service的VIP地址范围</span></span><br><span class="line">    --service-dns-domain cluster.local \</span><br><span class="line">    <span class="comment"># 为Service另外指定域名，默认"cluster.local"</span></span><br><span class="line">    --upload-certs</span><br><span class="line">    <span class="comment"># 将 Control-plane 证书上传到 kubeadm-certs Secret</span></span><br></pre></td></tr></table></figure><p>不带注释的内容如下，如果初始化超时，可以修改DNS为8.8.8.8后重启网络服务再次尝试。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init \</span><br><span class="line"> --apiserver-advertise-address 0.0.0.0 \</span><br><span class="line"> --apiserver-bind-port 6443 \</span><br><span class="line"> --cert-dir /etc/kubernetes/pki \</span><br><span class="line"> --control-plane-endpoint kuber4s.api \</span><br><span class="line"> --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \</span><br><span class="line"> --kubernetes-version 1.17.3 \</span><br><span class="line"> --pod-network-cidr 10.10.0.0/16 \</span><br><span class="line"> --service-cidr 10.20.0.0/16 \</span><br><span class="line"> --service-dns-domain cluster.local \</span><br><span class="line"> --upload-certs</span><br></pre></td></tr></table></figure><p>接下来这个过程有点漫长（初始化会下载镜像、创建配置文件、启动容器等操作），泡杯茶，耐心等待。你也可以执行 tailf /var/log/messages 来实时查看系统日志，观察 Master 的初始化进展，期间碰到一些报错不要紧张，可能只是暂时的错误，等待最终反馈的结果即可。</p><p>如果初始化最终成功执行，你将看到如下信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following <span class="built_in">command</span> on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join kuber4s.api:6443 --token 0j287q.jw9zfjxud8w85tis \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f \</span><br><span class="line">    --control-plane --certificate-key 528b0b9f2861f8f02dfd4a59fc54ad21e42a7dea4dc5552ac24d9c650c5d4d80</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted <span class="keyword">in</span> two hours; If necessary, you can use</span><br><span class="line"><span class="string">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join kuber4s.api:6443 --token 0j287q.jw9zfjxud8w85tis \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</span><br></pre></td></tr></table></figure><p>为普通用户添加 kubectl 运行权限，命令内容在初始化成功后的输出内容中可以看到。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure><p>建议root用户也进行以上操作，作者使用的是root用户执行的初始化操作，然后在操作完成后查看集群状态的时候，出现如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure><p>这时候请备份好 kubeadm init 输出中的 kubeadm join 命令，因为将会需要这个命令来给集群添加节点。</p><h3 id="安装Pod网络附加组件"><a href="#安装Pod网络附加组件" class="headerlink" title="安装Pod网络附加组件"></a>安装Pod网络附加组件</h3><p>集群必须安装Pod网络插件，以使Pod可以相互通信，只需在Master节点操作，其他新加入的节点会自动创建相关pod。必须在任何应用程序之前部署网络组件。另外，在安装网络之前，CoreDNS将不会启动，你可以通过命令 来查看CoreDNS 的状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 CoreDNS 的状态,并不是 Running 状态</span></span><br><span class="line">$ kubectl get pods --all-namespaces|grep coredns</span><br><span class="line">kube-system   coredns-7f9c544f75-bzksd    0/1   Pending   0     14m</span><br><span class="line">kube-system   coredns-7f9c544f75-mtrwq    0/1   Pending   0     14m</span><br></pre></td></tr></table></figure><p>kubeadm 支持多种网络插件，我们选择Calico 网络插件（kubeadm 仅支持基于容器网络接口（CNI）的网络（不支持kubenet），默认情况下，它给出的pod的IP段地址是 192.168.0.0/16 ,如果你的机器已经使用了此IP段，就需要修改这个配置项，将其值改为在初始化 Master 节点时使用 kubeadm init –pod-network-cidr=x.x.x.x/x 的IP地址段，即我们上面配置的 10.10.0.0/16 ，大概在625行左右，操作如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取配置文件</span></span><br><span class="line">mkdir calico &amp;&amp; <span class="built_in">cd</span> calico</span><br><span class="line">wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件</span></span><br><span class="line"><span class="comment"># 找到 625 行左右的 192.168.0.0/16 ，并修改为我们初始化时配置的 10.10.0.0/16</span></span><br><span class="line">vim calico.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 部署 Pod 网络组件</span></span><br><span class="line">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure><p>稍等片刻查询 pod 详情，你也可以使用 watch 命令来实时查看 pod 的状态，等待 Pod 网络组件部署成功后，就可以看到一些信息了，包括 Pod 的 IP 地址信息，这个过程时间可能会有点长。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 2 kubectl get pods --all-namespaces -o wide</span><br></pre></td></tr></table></figure><h3 id="将Worker节点添加到Kubernetes"><a href="#将Worker节点添加到Kubernetes" class="headerlink" title="将Worker节点添加到Kubernetes"></a>将Worker节点添加到Kubernetes</h3><p>请首先确认所有Worker节点满足第一部分的环境说明，并且已经安装了 Docker 和 kubeadm、kubelet 、kubectl，并且已经启动 kubelet。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 Hosts 解析</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"127.0.0.1 <span class="variable">$(hostname)</span>"</span> &gt;&gt; /etc/hosts</span><br><span class="line"><span class="built_in">export</span> MASTER_IP=10.209.208.238</span><br><span class="line"><span class="built_in">export</span> APISERVER_NAME=kuber4s.api</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;MASTER_IP&#125;</span> <span class="variable">$&#123;APISERVER_NAME&#125;</span>"</span> &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>将 Worker 节点添加到集群，这里注意，执行后可能会报错，有幸的话你会跳进这个坑，这是因为 Worker 节点加入集群的命令实际上在初始化 master 时已经有提示出来了，不过两小时后会删除上传的证书，所以如果你此时加入集群的时候提示证书相关的错误，请执行 kubeadm init phase upload-certs –upload-certs 重新加载证书。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join kuber4s.api:6443 --token 0y1dj2.ih27ainxwyib0911 \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:5204b3e358a0d568e147908cba8036bdb63e604d4f4c1c3730398f33144fac61 \</span><br></pre></td></tr></table></figure><p>执行加入操作，你可能会发现卡着不动，大概率是因为令牌ID对此集群无效或已过 2 小时的有效期（通过执行 kubeadm join –v=5 来获取详细的加入过程，看到了内容为 ”token id “0y1dj2” is invalid for this cluster or it has expired“ 的提示），接下来需要在 Master 上通过 kubeadm token create 来创建新的令牌。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm token create --<span class="built_in">print</span>-join-command</span><br><span class="line">W0129 19:10:04.842735   15533 validation.go:28] Cannot validate kube-proxy config - no validator is available</span><br><span class="line">W0129 19:10:04.842808   15533 validation.go:28] Cannot validate kubelet config - no validator is available</span><br><span class="line"><span class="comment"># 输出结果如下</span></span><br><span class="line">kubeadm join kuber4s.api:6443 --token 1hk9bc.oz7f3lmtbzf15x9b     --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</span><br></pre></td></tr></table></figure><p>在 Worker节点上重新执行加入集群命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join kuber4s.api:6443 \</span><br><span class="line">    --token 1hk9bc.oz7f3lmtbzf15x9b \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</span><br></pre></td></tr></table></figure><p>接下来在Master上查看 Worker 节点加入的状况，直到 Worker 节点的状态变为 Ready 便证明加入成功，这个过程可能会有点漫长，30 分钟以内都算正常的，主要看你网络的情况或者说拉取镜像的速度；另外不要一看到 /var/log/messages 里面报错就慌了，那也得看具体报什么错，看不懂就稍微等一下，一般在 Master 上能看到已经加入（虽然没有Ready）就没什么问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch kubectl get nodes -o wide</span><br></pre></td></tr></table></figure><h2 id="安装nfs服务"><a href="#安装nfs服务" class="headerlink" title="安装nfs服务"></a>安装nfs服务</h2><p>kubesphere安装条件是必须k8s集群有StorageClass，这里用nfs。所以需要安装nfs服务来作为StorageClass。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/k8s --创建一个存储目录</span><br><span class="line">vi /etc/exports</span><br><span class="line">/data/k8s    *(rw,sync,no_root_squash)</span><br><span class="line"><span class="comment">#目录/data/k8s共享给10.0.0.0/8网段，允许读写，同步写入</span></span><br><span class="line"><span class="comment">#第一列代表共享哪个目录;第二列代表允许哪个客户端去访问;第三列共享目录的一些权限设置</span></span><br><span class="line"><span class="comment">#权限：ro 只读 rw允许读写 sync同步写入 no_root_squash当客户机以root身份访问时，赋予root权限（即超级用户保留权限）否则，root用户所有请求映射成anonymous用户一样的权限（默认）</span></span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>先启动rpcbind,再启动nfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl start rpcbind</span><br><span class="line">systemctl start nfs</span><br><span class="line">netstat -anptu | grep rpcbind</span><br></pre></td></tr></table></figure><h2 id="安装-StorageClass"><a href="#安装-StorageClass" class="headerlink" title="安装 StorageClass"></a>安装 StorageClass</h2><p>Kubernetes 支持多种 StorageClass，这选择NFS 作为集群的 StorageClass。</p><h3 id="下载所需文件"><a href="#下载所需文件" class="headerlink" title="下载所需文件"></a>下载所需文件</h3><p>下载所需文件，并进行内容调整</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir nfsvolume &amp;&amp; <span class="built_in">cd</span> nfsvolume</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> class.yaml deployment.yaml rbac.yaml ; <span class="keyword">do</span> wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/<span class="variable">$file</span> ; <span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>修改 deployment.yaml 中的两处 NFS 服务器 IP 和目录</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">fuseim.pri/ifs</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">NFS_SERVER</span></span><br><span class="line"><span class="attr">              value:</span> <span class="number">10.220</span><span class="number">.170</span><span class="number">.240</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">NFS_PATH</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">/data/k8s</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nfs-client-root</span></span><br><span class="line"><span class="attr">          nfs:</span></span><br><span class="line"><span class="attr">            server:</span> <span class="number">10.220</span><span class="number">.170</span><span class="number">.240</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/data/k8s</span></span><br></pre></td></tr></table></figure><h3 id="部署创建"><a href="#部署创建" class="headerlink" title="部署创建"></a>部署创建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f rbac.yaml</span><br><span class="line">kubectl create -f class.yaml</span><br><span class="line">kubectl create -f deployment.yaml</span><br></pre></td></tr></table></figure><p>在集群内所有机器上安装nfs-utils并启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum -y install nfs-utils</span><br><span class="line">systemctl start nfs-utils</span><br><span class="line">systemctl <span class="built_in">enable</span> nfs-utils</span><br><span class="line">rpcinfo -p</span><br></pre></td></tr></table></figure><p>查看storageclass</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE</span><br><span class="line">managed-nfs-storage fuseim.pri/ifs Delete Immediate <span class="literal">false</span> 10m</span><br></pre></td></tr></table></figure><h3 id="标记默认的-StorageClass"><a href="#标记默认的-StorageClass" class="headerlink" title="标记默认的 StorageClass"></a>标记默认的 StorageClass</h3><p>操作命令格式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch storageclass managed-nfs-storage -p <span class="string">'&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>请注意，最多只能有一个 StorageClass 能够被标记为默认。</p><p>验证标记是否成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE</span><br><span class="line">managed-nfs-storage (default) fuseim.pri/ifs Delete Immediate <span class="literal">false</span> 12m</span><br></pre></td></tr></table></figure><h2 id="安装KubeSphere3-0"><a href="#安装KubeSphere3-0" class="headerlink" title="安装KubeSphere3.0"></a>安装KubeSphere3.0</h2><p><a href="https://kubesphere.com.cn/docs/zh-CN/" rel="noopener" target="_blank">KubeSphere</a> 是在 <a href="https://kubernetes.io/" rel="noopener" target="_blank">Kubernetes</a> 之上构建的以应用为中心的容器平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大减轻开发、测试、运维的日常工作的复杂度，旨在解决 Kubernetes 本身存在的存储、网络、安全和易用性等痛点。除此之外，平台已经整合并优化了多个适用于容器场景的功能模块，以完整的解决方案帮助企业轻松应对敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、工作负载和集群管理、监控告警、日志查询与收集、服务与网络、应用商店、镜像构建与镜像仓库管理和存储管理等多种场景。后续版本将提供和支持多集群管理、大数据、AI 等场景。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/4.png" alt></p><h3 id="安装要求"><a href="#安装要求" class="headerlink" title="安装要求"></a>安装要求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k8s集群版本必须是1.15.x, 1.16.x, 1.17.x, or 1.18.x</span><br><span class="line">必须有默认的storageclass</span><br><span class="line">内存和cpu最低要求：CPU &gt; 1 Core, Memory &gt; 2 G</span><br></pre></td></tr></table></figure><p>按照上面教程操作，完全符合要求。</p><h3 id="安装yaml文件"><a href="#安装yaml文件" class="headerlink" title="安装yaml文件"></a>安装yaml文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yaml</span><br><span class="line"></span><br><span class="line">kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml</span><br></pre></td></tr></table></figure><h3 id="查看安装日志"><a href="#查看安装日志" class="headerlink" title="查看安装日志"></a>查看安装日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>) -f</span><br></pre></td></tr></table></figure><h3 id="查看所有pod"><a href="#查看所有pod" class="headerlink" title="查看所有pod"></a>查看所有pod</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -A</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/1.png" alt></p><h3 id="登陆kubesphere"><a href="#登陆kubesphere" class="headerlink" title="登陆kubesphere"></a>登陆kubesphere</h3><p>浏览器访问ip:30880    用户名：admin     默认密码：P@88w0rd</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/2.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/3.png" alt></p><h2 id="补充内容"><a href="#补充内容" class="headerlink" title="补充内容"></a>补充内容</h2><ul><li>kubeadm init               初始化Kubernetes主节点</li><li>kubeadm token          管理 kubeadm join 的令牌，包括查看、创建和删除等</li><li>kubeadm reset           将kubeadm init或kubeadm join对主机的更改恢复到之前状态，一般与 -f 参数使用</li></ul><p><strong>POD创建流程</strong></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/5.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/6.png" alt></p><p><strong>Ingress与Service关系图</strong></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b645bc81/7.png" alt></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将从零开始在干净的机器上安装 &lt;code&gt;Docker、Kubernetes (使用 kubeadm)、Calico、NFS StorageClass等&lt;/code&gt;，通过手把手的教程演示如何搭建一个 &lt;code&gt;Kubernetes集群&lt;/code&gt;，并在 K8s集群之上安装开源的&lt;code&gt;KubeSphere&lt;/code&gt; 容器平台可视化运营集群环境。&lt;/p&gt;
    
    </summary>
    
      <category term="容器编排" scheme="https://wandouduoduo.github.io/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
      <category term="K8s" scheme="https://wandouduoduo.github.io/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/K8s/"/>
    
    
      <category term="K8s" scheme="https://wandouduoduo.github.io/tags/K8s/"/>
    
  </entry>
  
  <entry>
    <title>带你彻底了解lstio是什么东东</title>
    <link href="https://wandouduoduo.github.io/articles/8fe28783.html"/>
    <id>https://wandouduoduo.github.io/articles/8fe28783.html</id>
    <published>2021-03-02T02:16:35.000Z</published>
    <updated>2021-03-16T10:33:08.068Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>如果你保持学习新技术的触角来持续拓展技术储备的广度的话，很可能在不同的地方听说过<code>Istio</code>，可能还知道它和Service Mesh有着牵扯。本文可作为了解<code>Istio</code>的入门介绍，介绍什么是<code>Istio</code>，<code>Istio</code>为什么最近这么火，以及<code>Istio</code>解决了哪些问题，能给我们带来什么好处。</p><a id="more"></a><h2 id="什么是Istio"><a href="#什么是Istio" class="headerlink" title="什么是Istio"></a>什么是Istio</h2><p>官方对 Istio 的介绍浓缩成了一句话：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">An open platform to connect, secure, control and observe services.</span><br></pre></td></tr></table></figure><p>翻译过来，就是”连接、安全加固、控制和观察服务的开放平台“。开放平台就是指它本身是开源的，服务对应的是微服务，也可以粗略地理解为单个应用。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/1.jpeg" alt="img"></p><p>中间的四个动词就是 Istio 的主要功能，官方也各有一句话的说明。这里再阐释一下：</p><ul><li>连接（Connect）：智能控制服务之间的调用流量，能够实现灰度升级、AB 测试和红黑部署等功能</li><li>安全加固（Secure）：自动为服务之间的调用提供认证、授权和加密。</li><li>控制（Control）：应用用户定义的 policy，保证资源在消费者中公平分配。</li><li>观察（Observe）：查看服务运行期间的各种数据，比如日志、监控和 tracing，了解服务的运行情况。</li></ul><p>虽然听起来非常高级，功能非常强大，但是一股脑出现这么多名词，还都是非常虚的概念，说了跟没说一样。要想理解上面这几句话的含义，我们还是从头说起，先聊聊 Service Mesh。</p><p><strong>NOTE：</strong>其实 Istio 的源头是微服务，但这又是一个比较大的话题，目前可以参考网络上各种文章。如果有机会，我们再来聊聊微服务。</p><h2 id="什么是Service-Mesh"><a href="#什么是Service-Mesh" class="headerlink" title="什么是Service Mesh"></a>什么是Service Mesh</h2><p>一般介绍 Service Mesh 的文章都会从网络层的又一个抽象说起，把 Service Mesh 看做建立在 TCP 层之上的微服务层。我这次换个思路，从 Service Mesh 的技术根基——网络代理来分析。</p><p>说起网络代理，我们会想到翻墙，如果对软件架构比较熟悉的会想到 Nginx 等反向代理软件。</p><p>其实网络代理的范围比较广，可以肯定的说，有网络访问的地方就会有代理的存在。</p><p>Wikipedia 对代理的定义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In computer networks, a proxy server is a server (a computer system or an application) that acts as an intermediary for requests from clients seeking resources from other servers.</span><br></pre></td></tr></table></figure><p><strong>NOTE：</strong>代理可以是嵌套的，也就是说通信双方 A、B 中间可以多多层代理，而这些代理的存在有可能对 A、B 是透明的。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/2.jpeg" alt="img"></p><p>简单来说，网络代理可以简单类比成现实生活中的中介，本来需要通信的双方因为各种原因在中间再加上一道关卡。本来双方就能完成的通信，为何非要多此一举呢？</p><p>那是因为代理可以为整个通信带来更多的功能，比如：</p><ul><li>拦截：代理可以选择性拦截传输的网络流量，比如一些公司限制员工在上班的时候不能访问某些游戏或者电商网站，再比如把我们和世界隔离开来的 GFW，还有在数据中心中拒绝恶意访问的网关。</li><li>统计：既然所有的流量都经过代理，那么代理也可以用来统计网络中的数据信息，比如了解哪些人在访问哪些网站，通信的应答延迟等。</li><li>缓存：如果通信双方比较”远“，访问比较慢，那么代理可以把最近访问的数据缓存在本地，后面的访问不用访问后端来做到加速。CDN 就是这个功能的典型场景。</li><li>分发：如果某个通信方有多个服务器后端，代理可以根据某些规则来选择如何把流量发送给多个服务器，也就是我们常说的负载均衡功能，比如著名的 Nginx 软件。</li><li>跳板：如果 A、B 双方因为某些原因不能直接访问，而代理可以和双方通信，那么通过代理，双方可以绕过原来的限制进行通信。这应该是广大中国网民比较熟悉的场景。</li><li>注入：既然代理可以看到流量，那么它也可以修改网络流量，可以自动在收到的流量中添加一些数据，比如有些宽带提供商的弹窗广告。</li><li>……</li></ul><p>不是要讲 Service Mesh 吗？为什么扯了一堆代理的事情？因为 Service Mesh 可以看做是传统代理的升级版，用来解决现在微服务框架中出现的问题，可以把 Service Mesh 看做是分布式的微服务代理。</p><p>在传统模式下，代理一般是集中式的单独的服务器，所有的请求都要先通过代理，然后再流入转发到实际的后端。</p><p>而在 Service Mesh 中，代理变成了分布式的，它常驻在了应用的身边（最常见的就是 Kubernetes Sidecar 模式，每一个应用的 Pod 中都运行着一个代理，负责流量相关的事情）。</p><p>这样的话，应用所有的流量都被代理接管，那么这个代理就能做到上面提到的所有可能的事情，从而带来无限的想象力。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/3.jpeg" alt="img"></p><p>此外，原来的代理都是基于网络流量的，一般都是工作在 IP 或者 TCP 层，很少关心具体的应用逻辑。</p><p>但是 Service Mesh 中，代理会知道整个集群的所有应用信息，并且额外添加了热更新、注入服务发现、降级熔断、认证授权、超时重试、日志监控等功能，让这些通用的功能不必每个应用都自己实现，放在代理中即可。</p><p>换句话说，Service Mesh 中的代理对微服务中的应用做了定制化的改进！</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/4.jpeg" alt="img"></p><p>就这样，借着微服务和容器化的东风，传统的代理摇身一变，成了如今炙手可热的 Service Mesh。</p><p>应用微服务之后，每个单独的微服务都会有很多副本，而且可能会有多个版本，这么多微服务之间的相互调用和管理非常复杂，但是有了 Service Mesh，我们可以把这块内容统一在代理层。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/5.jpeg" alt="img"></p><p>有了看起来四通八达的分布式代理，我们还需要对这些代理进行统一的管理。</p><p>手动更新每个代理的配置，对代理进行升级或者维护是个不可持续的事情，在前面的基础上，在加上一个控制中心，一个完整的 Service Mesh 就成了。</p><p>管理员只需要根据控制中心的 API 来配置整个集群的应用流量、安全规则即可，代理会自动和控制中心打交道根据用户的期望改变自己的行为。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/6.jpeg" alt="img"></p><p><strong>NOTE：</strong>所以你也可以理解 Service Mesh 中的代理会抢了 Nginx 的生意，这也是为了 Nginx 也要开始做 NginMesh 的原因。</p><h2 id="再来看Istio"><a href="#再来看Istio" class="headerlink" title="再来看Istio"></a>再来看Istio</h2><p>了解了 Service Mesh 的概念，我们再来看 Istio ，也许就会清楚很多。首先来看 Istio 官方给出的架构图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/7.jpeg" alt="img"></p><p>可以看到，Istio 就是我们上述提到的 Service Mesh 架构的一种实现，服务之间的通信（比如这里的 Service A 访问 Service B）会通过代理（默认是 Envoy）来进行。</p><p>而且中间的网络协议支持 HTTP/1.1，HTTP/2，gRPC 或者 TCP，可以说覆盖了主流的通信协议。</p><p>控制中心做了进一步的细分，分成了 Pilot、Mixer 和 Citadel，它们的各自功能如下：</p><ul><li>Pilot：为 Envoy 提供了服务发现，流量管理和智能路由（AB 测试、金丝雀发布等），以及错误处理（超时、重试、熔断）功能。 用户通过 Pilot 的 API 管理网络相关的资源对象，Pilot 会根据用户的配置和服务的信息把网络流量管理变成 Envoy 能识别的格式分发到各个 Sidecar 代理中。</li><li>Mixer：为整个集群执行访问控制（哪些用户可以访问哪些服务）和 Policy 管理（Rate Limit，Quota 等），并且收集代理观察到的服务之间的流量统计数据。</li><li>Citadel：为服务之间提供认证和证书管理，可以让服务自动升级成 TLS 协议。</li></ul><p>代理会和控制中心通信，一方面可以获取需要的服务之间的信息，另一方面也可以汇报服务调用的 Metrics 数据。</p><p>知道了 Istio 的核心架构，再来看看它的功能描述就非常容易理解了：</p><ul><li>连接：控制中心可以从集群中获取所有服务的信息，并分发给代理，这样代理就能根据用户的期望来完成服务之间的通信（自动地服务发现、负载均衡、流量控制等）。</li><li>安全加固：因为所有的流量都是通过代理的，那么代理接收到不加密的网络流量之后，可以自动做一次封装，把它升级成安全的加密流量。</li><li>控制：用户可以配置各种规则（比如 RBAC 授权、白名单、Rate Limit 或者 Quota 等），当代理发现服务之间的访问不符合这些规则，就直接拒绝掉。</li><li>观察：所有的流量都经过代理，因此代理对整个集群的访问情况知道得一清二楚，它把这些数据上报到控制中心，那么管理员就能观察到整个集群的流量情况了</li></ul><h2 id="Istio解决什么问题"><a href="#Istio解决什么问题" class="headerlink" title="Istio解决什么问题"></a>Istio解决什么问题</h2><p>虽然看起来非常炫酷，功能也很强大，但是一个架构和产品出来都是要解决具体的问题。所以这部分我们来看看微服务架构中的难题以及 Istio 给出的答案。</p><p>首先，原来的单个应用拆分成了许多分散的微服务，它们之间相互调用才能完成一个任务，而一旦某个过程出错（组件越多，出错的概率也就越大），就非常难以排查。</p><p>用户请求出现问题无外乎两个问题：错误和响应慢。如果请求错误，那么我们需要知道那个步骤出错了，这么多的微服务之间的调用怎么确定哪个有调用成功？哪个没有调用成功呢？</p><p>如果是请求响应太慢，我们就需要知道到底哪些地方比较慢？整个链路的调用各阶段耗时是多少？哪些调用是并发执行的，哪些是串行的？这些问题需要我们能非常清楚整个集群的调用以及流量情况。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/8.jpeg" alt="img"></p><p>此外，微服务拆分成这么多组件，如果单个组件出错的概率不变，那么整体有地方出错的概率就会增大。服务调用的时候如果没有错误处理机制，那么会导致非常多的问题。</p><p>比如如果应用没有配置超时参数，或者配置的超时参数不对，则会导致请求的调用链超时叠加，对于用户来说就是请求卡住了。</p><p>如果没有重试机制，那么因为各种原因导致的偶发故障也会导致直接返回错误给用户，造成不好的用户体验。</p><p>此外，如果某些节点异常（比如网络中断，或者负载很高），也会导致应用整体的响应时间变长，集群服务应该能自动避开这些节点上的应用。</p><p>最后，应用也是会出现 Bug 的，各种 Bug 会导致某些应用不可访问。这些问题需要每个应用能及时发现问题，并做好对应的处理措施。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/9.jpeg" alt="img"></p><p>应用数量的增多，对于日常的应用发布来说也是个难题。应用的发布需要非常谨慎，如果应用都是一次性升级的，出现错误会导致整个线上应用不可用，影响范围太大。</p><p>而且，很多情况我们需要同时存在不同的版本，使用 AB 测试验证哪个版本更好。</p><p>如果版本升级改动了 API，并且互相有依赖，那么我们还希望能自动地控制发布期间不同版本访问不同的地址。这些问题都需要智能的流量控制机制。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/10.jpeg" alt="img"></p><p>为了保证整个系统的安全性，每个应用都需要实现一套相似的认证、授权、HTTPS、限流等功能。</p><p>一方面大多数的程序员都对安全相关的功能并不擅长或者感兴趣，另外这些完全相似的内容每次都要实现一遍是非常冗余的。这个问题需要一个能自动管理安全相关内容的系统。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/8fe28783/11.jpeg" alt="img"></p><p>上面提到的这些问题是不是非常熟悉？它们就是 Istio 尝试解决的问题，如果把上面的问题和 Istio 提供的功能做个映射，你会发现它们非常匹配，毕竟 Istio 就是为了解决微服务的这些问题才出现的。</p><h2 id="用什么姿势接入Istio？"><a href="#用什么姿势接入Istio？" class="headerlink" title="用什么姿势接入Istio？"></a>用什么姿势接入Istio？</h2><p>虽然 Istio 能解决那么多的问题，但是引入 Istio 并不是没有代价的。最大的问题是 Istio 的复杂性，强大的功能也意味着 Istio 的概念和组件非常多，要想理解和掌握 Istio ，并成功在生产环境中部署需要非常详细的规划。</p><p>一般情况下，集群管理团队需要对 Kubernetes 非常熟悉，了解常用的使用模式，然后采用逐步演进的方式把 Istio 的功能分批掌控下来。</p><p><strong>第一步，自然是在测试环境搭建一套 Istio 的集群，理解所有的核心概念和组件。</strong></p><p>了解 Istio 提供的接口和资源，知道它们的用处，思考如何应用到自己的场景中，然后是熟悉 Istio 的源代码，跟进社区的 Issues，了解目前还存在的 Issues 和 Bug，思考如何规避或者修复。</p><p>这一步是基础，需要积累到 Istio 安装部署、核心概念、功能和缺陷相关的知识，为后面做好准备。</p><p><strong>第二步，可以考虑接入 Istio 的观察性功能，包括 Logging、Tracing、Metrics 数据。</strong></p><p>应用部署到集群中，选择性地（一般是流量比较小，影响范围不大的应用）为一些应用开启 Istio 自动注入功能，接管应用的流量，并安装 Prometheus 和 Zipkin 等监控组件，收集系统所有的监控数据。</p><p>这一步可以试探性地了解 Istio 对应用的性能影响，同时建立服务的性能测试基准，发现服务的性能瓶颈，帮助快速定位应用可能出现的问题。</p><p>此时，这些功能可以是对应用开发者透明的，只需要集群管理员感知，这样可以减少可能带来的风险。</p><p><strong>第三步，为应用配置 Time Out 超时参数、自动重试、熔断和降级等功能，增加服务的容错性。</strong></p><p>这样可以避免某些应用错误进行这些配置导致问题的出现，这一步完成后需要通知所有的应用开发者删除掉在应用代码中对应的处理逻辑。这一步需要开发者和集群管理员同时参与。</p><p><strong>第四步，和 Ingress、Helm、应用上架等相关组件和流程对接，使用 Istio 接管应用的升级发布流程。</strong></p><p>让开发者可以配置应用灰度发布升级的策略，支持应用的蓝绿发布、金丝雀发布以及 AB 测试。</p><p><strong>第五步，接入安全功能。配置应用的 TLS 互信，添加 RBAC 授权，设置应用的流量限制，提升整个集群的安全性。</strong></p><p>因为安全的问题配置比较繁琐，而且优先级一般会比功能性相关的特性要低，所以这里放在了最后。</p><p>当然这个步骤只是一个参考，每个公司需要根据自己的情况、人力、时间和节奏来调整，找到适合自己的方案。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Istio 的架构在数据中心和集群管理中非常常见，每个 Agent 分布在各个节点上（可以是服务器、虚拟机、Pod、容器）负责接收指令并执行，以及汇报信息。</p><p>控制中心负责汇聚整个集群的信息，并提供 API 让用户对集群进行管理。</p><p>Kubernetes 也是类似的架构，SDN（Software Defined Network） 也是如此。</p><p>相信以后会有更多类似架构的出现，这是因为数据中心要管理的节点越来越多，我们需要把任务执行分布到各节点（Agent 负责的功能）。</p><p>同时也需要对整个集群进行管理和控制（Control Plane 的功能），完全去中心化的架构是无法满足后面这个要求的。</p><p>Istio 的出现为负责的微服务架构减轻了很多的负担，开发者不用关心服务调用的超时、重试、Rate Limit 的实现，服务之间的安全、授权也自动得到了保证。</p><p>集群管理员也能够很方便地发布应用（AB 测试和灰度发布），并且能清楚看到整个集群的运行情况。</p><p>但是这并不表明有了 Istio 就可以高枕无忧了，Istio 只是把原来分散在应用内部的复杂性统一抽象出来放到了统一的地方，并没有让原来的复杂消失不见。</p><p>因此我们需要维护 Istio 整个集群，而 Istio 的架构比较复杂，尤其是它一般还需要架在 Kubernetes 之上，这两个系统都比较复杂，而且它们的稳定性和性能会影响到整个集群。</p><p>因此再采用 Isito 之前，必须做好清楚的规划，权衡它带来的好处是否远大于额外维护它的花费，需要有相关的人才对整个网络、Kubernetes 和 Istio 都比较了解才行。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果你保持学习新技术的触角来持续拓展技术储备的广度的话，很可能在不同的地方听说过&lt;code&gt;Istio&lt;/code&gt;，可能还知道它和Service Mesh有着牵扯。本文可作为了解&lt;code&gt;Istio&lt;/code&gt;的入门介绍，介绍什么是&lt;code&gt;Istio&lt;/code&gt;，&lt;code&gt;Istio&lt;/code&gt;为什么最近这么火，以及&lt;code&gt;Istio&lt;/code&gt;解决了哪些问题，能给我们带来什么好处。&lt;/p&gt;
    
    </summary>
    
      <category term="容器编排" scheme="https://wandouduoduo.github.io/categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    
      <category term="K8s" scheme="https://wandouduoduo.github.io/tags/K8s/"/>
    
  </entry>
  
  <entry>
    <title>详解sql和nosql如何选型</title>
    <link href="https://wandouduoduo.github.io/articles/f84ad4f6.html"/>
    <id>https://wandouduoduo.github.io/articles/f84ad4f6.html</id>
    <published>2021-01-29T10:57:09.000Z</published>
    <updated>2021-01-29T11:06:49.543Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h2><p>你是否在为系统的数据库来一波大流量就几乎打满CPU，日常CPU居高不下烦恼？你是否在各种NoSql间纠结不定，到底该选用那种最好？今天的你就是昨天的我，这也是写这篇文章的初衷。</p><p>这篇文章是我好几个月来一直想写的一篇文章，也是一直想学习的一个内容，作为互联网从业人员，我们要知道关系型数据库（MySql、Oracle）无法满足我们对存储的所有要求，因此对底层存储的选型，对每种存储引擎的理解非常重要。同时也由于过去一段时间的工作经历，对这块有了一些更多的思考，想通过自己的总结把这块写出来分享给大家。</p><a id="more"></a><p><strong>结构化数据、非结构化数据与半结构化数据</strong></p><p>文章的开始，聊一下结构化数据、非结构化数据与半结构化数据，因为数据特点的不同，将在技术上直接影响存储引擎的选型。</p><p>首先是结构化数据，根据定义<strong>结构化数据指的是由二维表结构来逻辑表达和实现的数据，严格遵循数据格式与长度规范，也称作为行数据</strong>，特点为：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。例如：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/1.png" alt="img"></p><p>因此关系型数据库完美契合结构化数据的特点，关系型数据库也是关系型数据最主要的存储与管理引擎。</p><p>非结构化数据，指的是<strong>数据结构不规则或不完整，没有任何预定义的数据模型，不方便用二维逻辑表来表现的数据</strong>，例如办公文档（Word）、文本、图片、HTML、各类报表、视频音频等。</p><p>介于结构化与非结构化数据之间的数据就是半结构化数据了，它是结构化数据的一种形式，虽然<strong>不符合二维逻辑这种数据模型结构，但是包含相关标记，用来分割语义元素以及对记录和字段进行分层</strong>。常见的半结构化数据有XML和JSON，例如：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">person</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>张三<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">age</span>&gt;</span>18<span class="tag">&lt;/<span class="name">age</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">phone</span>&gt;</span>12345<span class="tag">&lt;/<span class="name">phone</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">person</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这种结构也被成为自描述的结构。</p><h2 id="关系型数据库"><a href="#关系型数据库" class="headerlink" title="关系型数据库"></a>关系型数据库</h2><h3 id="以关系型数据库的方式做存储的架构演进"><a href="#以关系型数据库的方式做存储的架构演进" class="headerlink" title="以关系型数据库的方式做存储的架构演进"></a><strong>以关系型数据库的方式做存储的架构演进</strong></h3><p>首先，我们看一下使用关系型数据库的方式，企业一个系统发展的几个阶段的架构演进（由于本文写的是Sql与NoSql，因此只以存储方式作为切入点，不会涉及类似MQ、ZK这些中间件内容）：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/2.png" alt="img"></p><p>阶段一：企业刚发展的阶段，最简单，一个应用服务器配一个关系型数据库，每次读写数据库。</p><p>阶段二：无论是使用MySQL还是Oracle还是别的关系型数据库，数据库通常不会先成为性能瓶颈，通常随着企业规模的扩大，一台应用服务器扛不住上游过来的流量且一台应用服务器会产生单点故障的问题，因此<strong>加应用服务器并且在流量入口使用Nginx做一层负载均衡</strong>，保证把流量均匀打到应用服务器上。</p><p>阶段三：随着企业规模的继续扩大，此时由于读写都在同一个数据库上，数据库性能出现一定的瓶颈，此时简单地做一层<strong>读写分离</strong>，每次写主库，读备库，主备库之间通过binlog同步数据，就能很大程度上解决这个阶段的数据库性能问题</p><p>阶段四：企业发展越来越好了，业务越来越大了，做了读写分离数据库压力还是越来越大，这时候怎么办呢，一台数据库扛不住，那我们就分几台吧，做<strong>分库分表</strong>，对表做垂直拆分，对库做水平拆分。以扩数据库为例，扩出两台数据库，以一定的单号（例如交易单号），以一定的规则（例如取模），交易单号对2取模为0的丢到数据库1去，交易单号对2取模为1的丢到数据库2去，通过这样的方式将写数据库的流量均分到两台数据库上。一般分库分表会使用Shard的方式，通过一个中间件，便于连接管理、数据监控且客户端无需感知数据库ip</p><h3 id="关系型数据库的优点"><a href="#关系型数据库的优点" class="headerlink" title="关系型数据库的优点"></a><strong>关系型数据库的优点</strong></h3><p>上面的方式，看似可以解决问题（实际上确实也能解决很多问题），正常对关系型数据库做一下读写分离 + 分库分表，支撑个1W+的读写QPS还是问题不大的。但是受限于关系型数据库本身，这套架构方案依然有着明显的不足，下面对利用关系型数据库方式做存储的方案的优点先进行一下分析，后一部分再分析一下缺点，对某个技术的优缺点的充分理解是技术选型的前提。</p><ul><li><strong>易理解</strong></li></ul><p>　　因为行 + 列的二维表逻辑是非常贴近逻辑世界的一个概念，关系模型相对网状、层次等其他模型更加容易被理解</p><ul><li><strong>操作方便</strong></li></ul><p>　　通用的SQL语言使得操作关系型数据库非常方便，支持join等复杂查询，Sql + 二维关系是关系型数据库最无可比拟的优点，这种易用性非常贴近开发者</p><ul><li><strong>数据一致性</strong></li></ul><p>　　支持ACID特性，可以维护数据之间的一致性，这是使用数据库非常重要的一个理由之一，例如同银行转账，张三转给李四100元钱，张三扣100元，李四加100元，而且必须同时成功或者同时失败，否则就会造成用户的资损</p><ul><li><strong>数据稳定</strong></li></ul><p>　　数据持久化到磁盘，没有丢失数据风险，支持海量数据存储</p><ul><li><strong>服务稳定</strong></li></ul><p>　　最常用的关系型数据库产品MySql、Oracle服务器性能卓越，服务稳定，通常很少出现宕机异常</p><h3 id="关系型数据库的缺点"><a href="#关系型数据库的缺点" class="headerlink" title="关系型数据库的缺点"></a><strong>关系型数据库的缺点</strong></h3><p>紧接着的，我们看一下关系型数据库的缺点，也是比较明显的。</p><ul><li><strong>高并发下IO压力大</strong></li></ul><p>　　数据按行存储，即使只针对其中某一列进行运算，也会将整行数据从存储设备中读入内存，导致IO较高</p><ul><li><strong>为维护索引付出的代价大</strong></li></ul><p>　　为了提供丰富的查询能力，通常热点表都会有多个二级索引，一旦有了二级索引，数据的新增必然伴随着所有二级索引的新增，数据的更新也必然伴随着所有二级索引的更新，这不可避免地降低了关系型数据库的读写能力，且索引越多读写能力越差。有机会的话可以看一下自己公司的数据库，除了数据文件不可避免地占空间外，索引占的空间其实也并不少</p><ul><li><strong>为维护数据一致性付出的代价大</strong></li></ul><p>　　数据一致性是关系型数据库的核心，但是同样为了维护数据一致性的代价也是非常大的。我们都知道SQL标准为事务定义了不同的隔离级别，从低到高依次是读未提交、读已提交、可重复度、串行化，事务隔离级别越低，可能出现的并发异常越多，但是通常而言能提供的并发能力越强。那么为了保证事务一致性，数据库就需要提供并发控制与故障恢复两种技术，前者用于减少并发异常，后者可以在系统异常的时候保证事务与数据库状态不会被破坏。对于并发控制，其核心思想就是加锁，无论是乐观锁还是悲观锁，只要提供的隔离级别越高，那么读写性能必然越差</p><ul><li><strong>水平扩展后带来的种种问题难处理</strong></li></ul><p>　　前文提过，随着企业规模扩大，一种方式是对数据库做分库，做了分库之后，数据迁移（1个库的数据按照一定规则打到2个库中）、跨库join（订单数据里有用户数据，两条数据不在同一个库中）、分布式事务处理都是需要考虑的问题，尤其是分布式事务处理，业界当前都没有特别好的解决方案</p><ul><li><strong>表结构扩展不方便</strong></li></ul><p>　　由于数据库存储的是结构化数据，因此表结构schema是固定的，扩展不方便，如果需要修改表结构，需要执行DDL（data definition language）语句修改，修改期间会导致锁表，部分服务不可用</p><ul><li><strong>全文搜索功能弱</strong></li></ul><p>　　例如like “%中国真伟大%”，只能搜索到”2019年中国真伟大，爱祖国”，无法搜索到”中国真是太伟大了”这样的文本，即不具备分词能力，且like查询在”%中国真伟大”这样的搜索条件下，无法命中索引，将会导致查询效率大大降低</p><p>写了这么多，我的理解核心还是前三点，它反映出的一个问题是<strong>关系型数据库在高并发下的能力是有瓶颈的</strong>，尤其是写入/更新频繁的情况下，出现瓶颈的结果就是数据库CPU高、Sql执行慢、客户端报数据库连接池不够等错误，因此例如万人秒杀这种场景，我们绝对不可能通过数据库直接去扣减库存。</p><p>可能有朋友说，数据库在高并发下的能力有瓶颈，我公司有钱，加CPU、换固态硬盘、继续买服务器加数据库做分库不就好了，问题是这是一种性价比非常低的方式，花1000万达到的效果，换其他方式可能100万就达到了，不考虑人员、服务器投入产出比的Leader就是个不合格的Leader，且关系型数据库的方式，受限于它本身的特点，可能花了钱都未必能达到想要的效果。至于什么是花100万就能达到花1000万效果的方式呢？可以继续往下看，这就是我们要说的NoSql。</p><h2 id="非关系型数据库"><a href="#非关系型数据库" class="headerlink" title="非关系型数据库"></a>非关系型数据库</h2><h3 id="结合NoSql的方式做存储的架构演进"><a href="#结合NoSql的方式做存储的架构演进" class="headerlink" title="结合NoSql的方式做存储的架构演进"></a><strong>结合NoSql的方式做存储的架构演进</strong></h3><p>像上文分析的，数据库作为一种关系型数据的存储引擎，存储的是关系型数据，它有优点，同时也有明显的缺点，因此通常在企业规模不断扩大的情况下，不会一味指望通过增强数据库的能力来解决数据存储问题，而是会引入其他存储，也就是我们说的NoSql。</p><p>NoSql的全称为Not Only SQL，泛指非关系型数据库，是对关系型数据库的一种<strong>补充</strong>，特别注意补充这两个字，这意味着NoSql与关系型数据库并不是对立关系，二者各有优劣，取长补短，在合适的场景下选择合适的存储引擎才是正确的做法。</p><p>比较简单的NoSql就是缓存：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/3.png" alt="img"></p><p>针对那些读远多于写的数据，引入一层缓存，每次读从缓存中读取，缓存中读取不到，再去数据库中取，取完之后再写入到缓存，对数据做好失效机制通常就没有大问题了。通常来说，缓存是性能优化的第一选择也是见效最明显的方案。</p><p>但是，缓存通常都是KV型存储且容量有限（基于内存），无法解决所有问题，于是再进一步的优化，我们继续引入其他NoSql：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/4.png" alt="img"></p><p>数据库、缓存与其他NoSql并行工作，充分发挥每种NoSql的特点。当然NoSql在性能方面大大优于关系挺数据库的同时，往往也伴随着一些特性的缺失，比较常见的就是事务功能的缺失。</p><p>下面看一下常用的NoSql及他们的代表产品，并对每种NoSql的优缺点和适用场景做一下分析，便于熟悉每种NoSql的特点，方便技术选型。</p><h3 id="KV型NoSql（代表—-Redis）"><a href="#KV型NoSql（代表—-Redis）" class="headerlink" title="KV型NoSql（代表—-Redis）"></a><strong>KV型NoSql（代表—-Redis）</strong></h3><p>KV型NoSql顾名思义就是以键值对形式存储的非关系型数据库，是最简单、最容易理解也是大家最熟悉的一种NoSql，因此比较快地带过。Redis、MemCache是其中的代表，Redis又是KV型NoSql中应用最广泛的NoSql，KV型数据库以Redis为例，最大的优点我总结下来就两点：</p><ul><li>数据基于内存，读写效率高</li><li>KV型数据，时间复杂度为O(1)，查询速度快</li></ul><p>因此，KV型NoSql最大的优点就是<strong>高性能</strong>，利用Redis自带的BenchMark做基准测试，TPS可达到10万的级别，性能非常强劲。同样的Redis也有所有KV型NoSql都有的比较明显的缺点：</p><ul><li>只能根据K查V，无法根据V查K</li><li>查询方式单一，只有KV的方式，不支持条件查询，多条件查询唯一的做法就是数据冗余，但这会极大的浪费存储空间</li><li>内存是有限的，无法支持海量数据存储</li><li>同样的，由于KV型NoSql的存储是基于内存的，会有丢失数据的风险</li></ul><p>综上所述，KV型NoSql最合适的场景就是<strong>缓存</strong>的场景：</p><ul><li>读远多于写</li><li>读取能力强</li><li>没有持久化的需求，可以容忍数据丢失，反正丢了再查询一把写入就是了</li></ul><p>例如根据用户id查询用户信息，每次根据用户id去缓存中查询一把，查到数据直接返回，查不到去关系型数据库里面根据id查询一把数据写到缓存中去。</p><h3 id="搜索型NoSql（代表—-ElasticSearch）"><a href="#搜索型NoSql（代表—-ElasticSearch）" class="headerlink" title="搜索型NoSql（代表—-ElasticSearch）"></a><strong>搜索型NoSql（代表—-ElasticSearch）</strong></h3><p>传统关系型数据库主要通过索引来达到快速查询的目的，但是在全文搜索的场景下，索引是无能为力的，like查询一来无法满足所有模糊匹配需求，二来使用限制太大且使用不当容易造成慢查询，<strong>搜索型NoSql的诞生正是为了解决关系型数据库全文搜索能力较弱的问题</strong>，ElasticSearch是搜索型NoSql的代表产品。</p><p>全文搜索的原理是<strong>倒排索引</strong>，我们看一下什么是倒排索引。要说倒排索引我们先看下什么是正排索引，传统的正排索引是<strong>文档–&gt;关键字</strong>的映射，例如”Tom is my friend”这句话，会将其切分为”Tom”、”is”、”my”、”friend”四个单词，在搜索的时候对文档进行扫描，符合条件的查出来。这种方式原理非常简单，但是由于其检索效率太低，基本没什么实用价值。</p><p>倒排索引则完全相反，它是<strong>关键字–&gt;文档</strong>的映射，我用张表格展示一下就比较清楚了：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/5.png" alt="img"></p><p>意思是我现在这里有四个短句：</p><ul><li>“Tom is Tom”</li><li>“Tom is my friend”</li><li>“Thank you, Betty”</li><li>“Tom is Betty’s husband”</li></ul><p>搜索引擎会根据一定的切分规则将这句话切成N个关键字，并以关键字的维度维护关键字在每个文本中的出现次数。这样下次搜索”Tom”的时候，由于Tom这个词语在”Tom is Tom”、”Tom is my friend”、”Tom is Betty’s husband”三句话中都有出现，因此这三条记录都会被检索出来，且由于”Tom is Tom”这句话中”Tom”出现了2次，因此这条记录对”Tom”这个单词的匹配度最高，最先展示。这就是搜索引擎倒排索引的基本原理，假设某个关键字在某个文档中出现，那么倒排索引中有两部分内容：</p><ul><li>文档ID</li><li>在该文档中出现的位置情况</li></ul><p>可以举一反三，我们搜索”Betty Tom”这两个词语也是一样，搜索引擎将”Betty Tom”切分为”Tom”、”Betty”两个单词，根据开发者指定的满足率，比如满足率=50%，那么只要记录中出现了两个单词之一的记录都会被检索出来，再按照匹配度进行展示。</p><p>搜索型NoSql以ElasticSearch为例，它的优点为：</p><ul><li>支持分词场景、全文搜索，这是区别于关系型数据库最大特点</li><li>支持条件查询，支持聚合操作，类似关系型数据库的Group By，但是功能更加强大，适合做数据分析</li><li>数据写文件无丢失风险，在集群环境下可以方便横向扩展，可承载PB级别的数据</li><li>高可用，自动发现新的或者失败的节点，重组和重新平衡数据，确保数据是安全和可访问的</li></ul><p>同样，ElasticSearch也有比较明显的缺点：</p><ul><li><p>性能全靠内存来顶，也是使用的时候最需要注意的点，非常吃硬件资源、吃内存，大数据量下64G + SSD基本是标配，算得上是数据库中的爱马仕了。为什么要专门提一下内存呢，因为内存这个东西是很值钱的，相同的配置多一倍内存，一个月差不多就要多花几百块钱，至于ElasticSearch内存用在什么地方，大概有如下这些：</p></li><li><ul><li>Indexing Buffer—-ElasticSearch基于Luence，Lucene的倒排索引是先在内存里生成，然后定期以Segment File的方式刷磁盘的，每个Segment File实际就是一个完整的倒排索引</li><li>Segment Memory—-倒排索引前面说过是基于关键字的，Lucene在4.0后会将所有关键字以FST这种数据结构的方式将所有关键字在启动的时候全量加载到内存，加快查询速度，官方建议至少留系统一半内存给Lucene</li><li>各类缓存—-Filter Cache、Field Cache、Indexing Cache等，用于提升查询分析性能，例如Filter Cache用于缓存使用过的Filter的结果集</li><li>Cluter State Buffer—-ElasticSearch被设计为每个Node都可以响应用户请求，因此每个Node的内存中都包含有一份集群状态的拷贝，一个规模很大的集群这个状态信息可能会非常大</li></ul></li><li><p>读写之间有延迟，写入的数据差不多1s样子会被读取到，这也正常，写入的时候自动加入这么多索引肯定影响性能</p></li><li><p>数据结构灵活性不高，ElasticSearch这个东西，字段一旦建立就没法修改类型了，假如建立的数据表某个字段没有加全文索引，想加上，那么只能把整个表删了再重建</p></li></ul><p>因此，搜索型NoSql最适用的场景就是<strong>有条件搜索尤其是全文搜索的场景</strong>，作为关系型数据库的一种替代方案。</p><p>另外，搜索型数据库还有一种特别重要的应用场景。我们可以想，一旦对数据库做了分库分表后，原来可以在单表中做的聚合操作、统计操作是否统统失效？例如我把订单表分16个库，1024张表，那么订单数据就散落在1024张表中，我想要统计昨天浙江省单笔成交金额最高的订单是哪笔如何做？我想要把昨天的所有订单按照时间排序分页展示如何做？<strong>这就是搜索型NoSql的另一大作用了，我们可以把分表之后的数据统一打在搜索型NoSql中，利用搜索型NoSql的搜索与聚合能力完成对全量数据的查询</strong>。</p><p>至于为什么把它放在KV型NoSql后面作为第二个写呢，因为通常搜索型NoSql也会作为一层前置缓存，来对关系型数据库进行保护。</p><h3 id="列式NoSql（代表—-HBase）"><a href="#列式NoSql（代表—-HBase）" class="headerlink" title="列式NoSql（代表—-HBase）"></a><strong>列式NoSql（代表—-HBase）</strong></h3><p>列式NoSql，大数据时代最具代表性的技术之一了，以HBase为代表。</p><p>列式NoSql是基于列式存储的，那么什么是列式存储呢，列式NoSql和关系型数据库一样都有主键的概念，区别在于关系型数据库是按照行组织的数据：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/6.png" alt="img"></p><p>看到每行有name、phone、address三个字段，这是行式存储的方式，且可以观察id = 2的这条数据，即使phone字段没有，它也是占空间的。</p><p>列式存储完全是另一种方式，它是按每一列进行组织的数据：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/7.png" alt="img"></p><p><img src="https://img2018.cnblogs.com/blog/801753/201908/801753-20190810184238996-511911560.png" alt="img"></p><p>这么做有什么好处呢？大致有以下几点：</p><ul><li>查询时只有指定的列会被读取，不会读取所有列</li><li>存储上节约空间，Null值不会被存储，一列中有时候会有很多重复数据（尤其是枚举数据，性别、状态等），这类数据可压缩，行式数据库压缩率通常在3:1<del>5:1之间，列式数据库的压缩率一般在8:1</del>30:1左右</li><li>列数据被组织到一起，一次磁盘IO可以将一列数据一次性读取到内存中</li></ul><p>第二点说到了数据压缩，什么意思呢，以比较常见的字典表压缩方式举例：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/8.jpg" alt="img"></p><p>自己看图理解一下，应该就懂了。 </p><p>接着继续讲讲优缺点，列式NoSql，以HBase为代表的，优点为：</p><ul><li>海量数据无限存储，PB级别数据随便存，底层基于HDFS（Hadoop文件系统），数据持久化</li><li>读写性能好，只要没有滥用造成数据热点，读写基本随便玩</li><li>横向扩展在关系型数据库及非关系型数据库中都是最方便的之一，只需要添加新机器就可以实现数据容量的线性增长，且可用在廉价服务器上，节省成本</li><li>本身没有单点故障，可用性高</li><li>可存储结构化或者半结构化的数据</li><li>列数理论上无限，HBase本身只对列族数量有要求，建议1~3个</li></ul><p>说了这么多HBase的优点，又到了说HBase缺点的时候了：</p><ul><li>HBase是Hadoop生态的一部分，因此它本身是一款比较重的产品，依赖很多Hadoop组件，数据规模不大没必要用，运维还是有点复杂的</li><li>KV式，不支持条件查询，或者说条件查询非常非常弱吧，HBase在Scan扫描一批数据的情况下还是提供了前缀匹配这种API的，条件查询除非定义多个RowKey做数据冗余</li><li>不支持分页查询，因为统计不了数据总数</li></ul><p>因此<strong>HBase比较适用于那种KV型的且未来无法预估数据增长量的场景</strong>，另外HBase使用还是需要一定的经验，主要体现在RowKey的设计上。</p><h3 id="文档型NoSql（代表—-MongoDB）"><a href="#文档型NoSql（代表—-MongoDB）" class="headerlink" title="文档型NoSql（代表—-MongoDB）"></a><strong>文档型NoSql（代表—-MongoDB）</strong></h3><p>坦白讲，根据我的工作经历，文档型NoSql我只有比较浅的使用经验，因此这部分只能结合之前的使用与网上的文章大致给大家介绍一下。</p><p>什么是文档型NoSql呢，文档型NoSql指的是将半结构化数据存储为文档的一种NoSql，文档型NoSql通常以JSON或者XML格式存储数据，因此文档型NoSql是没有Schema的，由于没有Schema的特性，我们可以随意地存储与读取数据，因此文档型NoSql的出现是<strong>解决关系型数据库表结构扩展不方便的问题的</strong>。</p><p>MongoDB是文档型NoSql的代表产品，同时也是所有NoSql产品中的明星产品之一，因此这里以MongoDB为例。按我的理解，作为文档型NoSql，MongoDB是一款完全和关系型数据库对标的产品，就我们从存储上来看：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/9.png" alt="img"></p><p>看到，关系型数据库是按部就班地每个字段一列存，在MongDB里面就是一个JSON字符串存储。关系型数据可以为name、phone建立索引，MongoDB使用createIndex命令一样可以为列建立索引，建立索引之后可以大大提升查询效率。其他方面而言，就大的基本概念，二者之间基本也是类似的：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/10.png" alt="img"></p><p>因此，对于MongDB，我们只要理解成一个Free-Schema的关系型数据库就完事了，它的优缺点比较一目了然，优点：</p><ul><li>没有预定义的字段，扩展字段容易</li><li>相较于关系型数据库，读写性能优越，命中二级索引的查询不会比关系型数据库慢，对于非索引字段的查询则是全面胜出</li></ul><p>缺点在于：</p><ul><li>不支持事务操作，虽然Mongodb4.0之后宣称支持事务，但是效果待观测</li><li>多表之间的关联查询不支持（虽然有嵌入文档的方式），join查询还是需要多次操作</li><li>空间占用较大，这个是MongDB的设计问题，空间预分配机制 + 删除数据后空间不释放，只有用db.repairDatabase()去修复才能释放</li><li>目前没发现MongoDB有关系型数据库例如MySql的Navicat这种成熟的运维工具</li></ul><p>总而言之，MongDB的使用场景很大程度上可以对标关系型数据库，但是比较适合处理那些没有join、没有强一致性要求且表Schema会常变化的数据。</p><h2 id="总结：数据库与NoSql及各种NoSql间的对比"><a href="#总结：数据库与NoSql及各种NoSql间的对比" class="headerlink" title="总结：数据库与NoSql及各种NoSql间的对比"></a><strong>总结：数据库与NoSql及各种NoSql间的对比</strong></h2><p>最后一部分，做一个总结，本文归根到底是两个话题：</p><ul><li>何时选用关系型数据库，何时选用非关系型数据库</li><li>选用非关系型数据库，使用哪种非关系型数据库</li></ul><p>首先是第一个话题，关系型数据库与非关系型数据库的选择，在我理解里面无非就是两点考虑：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/11.png" alt="img"></p><p>第一点，不多解释应该都理解，非关系型数据库都是通过牺牲了ACID特性来获取更高的性能的，假设两张表之间有比较强的一致性需求，那么这类数据是不适合放在非关系型数据库中的。</p><p>第二点，核心数据不走非关系型数据库，例如用户表、订单表，但是这有一个前提，就是这一类核心数据会有多种查询模式，例如用户表有ABCD四个字段，可能根据AB查，可能根据AC查，可能根据D查，假设核心数据，但是就是个KV形式，比如用户的聊天记录，那么HBase一存就完事了。</p><p>这几年的工作经验来看，非核心数据尤其是日志、流水一类中间数据千万不要写在关系型数据库中，这一类数据通常有两个特点：</p><ul><li>写远高于读</li><li>写入量巨大</li></ul><p>一旦使用关系型数据库作为存储引擎，将大大降低关系型数据库的能力，正常读写QPS不高的核心服务会受这一类数据读写的拖累。</p><p>接着是第二个问题，如果我们使用非关系型数据库作为存储引擎，那么如何选型？其实上面的文章基本都写了，这里只是做一个总结（所有的缺点都不会体现事务这个点，因为这是所有NoSql相比关系型数据库共有的一个问题）：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/f84ad4f6/12.png" alt="img"></p><p>但是这里特别说明，<strong>选型一定要结合实际情况而不是照本宣科</strong>，比如：</p><ul><li>企业发展之初，明明一个关系型数据库就能搞定且支撑一年的架构，搞一套大而全的技术方案出来</li><li>有一些数据条件查询多，更适合使用ElasticSearch做存储降低关系型数据库压力，但是公司成本有限，这种情况下这类数据可以尝试继续使用关系型数据库做存储</li><li>有一类数据格式简单，就是个KV类型且增长量大，但是公司没有HBase这方面的人才，运维上可能会有一定难度，出于实际情况考虑，可先用关系型数据库顶一阵子</li></ul><p>所以，如果不考虑实际情况，虽然合适有些存储引擎更加合适，但是强行使用反而适得其反，总而言之，适合自己的才是最好的。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;你是否在为系统的数据库来一波大流量就几乎打满CPU，日常CPU居高不下烦恼？你是否在各种NoSql间纠结不定，到底该选用那种最好？今天的你就是昨天的我，这也是写这篇文章的初衷。&lt;/p&gt;
&lt;p&gt;这篇文章是我好几个月来一直想写的一篇文章，也是一直想学习的一个内容，作为互联网从业人员，我们要知道关系型数据库（MySql、Oracle）无法满足我们对存储的所有要求，因此对底层存储的选型，对每种存储引擎的理解非常重要。同时也由于过去一段时间的工作经历，对这块有了一些更多的思考，想通过自己的总结把这块写出来分享给大家。&lt;/p&gt;
    
    </summary>
    
      <category term="数据库" scheme="https://wandouduoduo.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="Sql" scheme="https://wandouduoduo.github.io/tags/Sql/"/>
    
  </entry>
  
  <entry>
    <title>Opensips+RTPEngine+FreeSwitch实现FS高可用</title>
    <link href="https://wandouduoduo.github.io/articles/ec271ad5.html"/>
    <id>https://wandouduoduo.github.io/articles/ec271ad5.html</id>
    <published>2021-01-14T03:26:59.000Z</published>
    <updated>2021-01-14T13:16:44.981Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/ec271ad5/1.png" alt="image.png"></p><a id="more"></a><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><p><strong>对于初学者，整个架构涉及的知识点很多，配置项复杂，建议使用下面的调试方法：</strong></p><ol><li>保证UA直连freeswitch 已经都正常通话且有声音，这也是本文的前提</li><li>软电话注册正常</li><li>FS直接originate 到软电话 playback一段录音能听到声音</li><li>FS直接originate 到软电话 echo能听到声音</li><li>webrtc 使用ws 注册正常，重复3、4 步骤</li><li>互拨测试，每一阶段不正常使用chrome的debug和抓包查看SIP、SDP、ICE是否正常：<ul><li>软电话-&gt;软电话</li><li>软电话-&gt;webrtc</li><li>webrtc-&gt;软电话</li><li>webrtc-&gt;webrtc</li></ul></li><li>webrtc从ws 切换到wss，重复上面的2、3、4、5、6步骤。</li></ol><h4 id="熟练使用抓包工具"><a href="#熟练使用抓包工具" class="headerlink" title="熟练使用抓包工具"></a>熟练使用抓包工具</h4><ol><li>chrome 调试模式，可以查看websocket的每一个frame内容</li><li>软电话可以使用Wireshark 之类工具抓包查看内容</li><li>不确定声音问题时可以抓包查看是否有持续的UDP包收到或者发送。</li></ol><h2 id="经历历程"><a href="#经历历程" class="headerlink" title="经历历程"></a>经历历程</h2><ul><li>最初是想UA 经过 opensips转发 REGISTER 后直接在fs保存穿透后的地址，UA连上Opensips之后将自己的地址经过转发传给FS1存储，这样是不是FS1可以直接通过contact找到坐席了呢？<code>三者在同一个局域网是可以的</code>，但是如果op和fs在外网，这是UA链接op就会涉及到NAT打洞的相关问题，即使FS保存了UA打洞后的ip和端口，由于NAT的限制，该端口可能只能UA和OP能够使用，FS是不能够使用的（但是不同的NAT环境可能又可以，但是我们不能指望用户的环境是最优环境）。<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/ec271ad5/2.png" alt="image.png"></li></ul><p>这里有一定的联想：<code>是否有一种可以通过一种keepalive的机制随时监控多台fs的状态，UA发送SIP时通过某个接口实时获取可用的一台FS主机IP，实现客户端级别的高可用。这里后面想了想：首先如果一台fs宕机则一半的客户端需要重新登录，其次比如INVITE失败不能自动重试其他机器。</code></p><ul><li>那么使用使用op转发，fs 用户的contact 保存 op 的地址。<ul><li>坐席呼叫坐席,INVITE消息流转：<br>UA1 ——&gt; OpenSips ——-&gt; FS ——-&gt; OpenSips ——&gt; UA2</li><li>FS直接originate：<br>FS —-(udp)—-&gt; OpenSips ——-(ws)—-&gt; UA</li></ul></li><li>保存上op的地址之后，对于软电话其实到这里注册已经可以了，接下来是声音。</li><li>发现从fs直接<code>originate user/1000 &amp;echo()</code>是没有声音的，通过抓包会发现UA收到的INVITE的SDP信息有问题(查看o=的IP)，可能会是FS的内网地址。<br>这里直接在opensips的脚本中判断从内部收到的INVITE，fix_nated_sdp到fs的外网IP即可，参考：<a href="https://opensips.org/pipermail/users/2013-August/026471.html" rel="noopener" target="_blank">https://opensips.org/pipermail/users/2013-August/026471.html</a></li><li>软电话一切调通后开始测试webrtc，使用web登录1000 账号之后显示登录成功，但是<code>originate user/1000 &amp;echo()</code>会直接失败，并且开始siptrace也不显示有任何sip发送出去，猜测<code>U-&gt;O 注册时的contact带有“transport=ws”，如果不删除，在FS保存时也会保存transport=ws，使得FS以为这是一个web client，于是从FS呼叫用户时会向Opensips 的5060端口发送websocket请求，导致根本无法呼出（本来应该是udp通讯）</code></li><li>【最终采用的方案】那么就删除“transport=ws”（目前是直接操作contact字段字符串，暂时没有更好的方案），这样在FS保存的就没有了（OP自己保存的信息依然知道是ws，要问OP怎么保存：save(“location”) + lookup()），FS会使用udp和OP通讯，信令正常，但是发送到web UA的时候会报“SIP/2.0 603 Failed to get local SDP”，原因是FS发送的INVITE中的SDP 音频编码不包含 Chrome 用的 SAVPF，说白了就是FS发的是一个给软电话的请求，web client处理不了。</li><li>解决：originate 需要加上 “originate {media_webrtc=true}user/8800 &amp;echo” 这样就会当成一个web发送请求，但是这样如果一个FS有软电话也有web，不是很好区别（因为对于fs来说不知道客户端类型。如果是软电话但是加上了media_webrtc=true，会报IP/2.0 488 Not Acceptable Here）</li><li>无法在OP中做判断然后修改，有些加密信息需要fs生成</li><li>【最终采用的方案】使用RTPEngine 协商双方的编码以及转码</li><li>注意SDP对应的IP，如果是websocket 还需要有ICE的candidate ip 协商过程。此外如果rtpengine所在机器有内网和公网ip还需要在offer中指定方向。</li></ul><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ul><li>websocket连接opensips出现自动登出的问题。fs返回的REGISTER的OK中的contact包含的expire时间时180秒，也就是UA会在180秒内重新发起REGITER进行续期。但是opensips的websocket连接时间为120秒，否则连接会被关闭。<br>解决：1、调小FS的<code>sip-force-expires</code>,或者2、调整opensips的<code>tcp_connection_lifetime</code>参数大于180秒，或者3、<code>modparam(&quot;registrar&quot;, &quot;tcp_persistent_flag&quot;, &quot;TCP_PERSISTENT&quot;)</code>并且<code>setflag(TCP_PERSISTENT);</code>,设置超时时间为REGISTER中的exipre时间。</li></ul><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>（和顶部的官方推荐架构一样。。。。）</p><ul><li>opensips 仅作为消息转发，不负责语音通讯</li><li>使用rtpengine来进行rtp转发以及sdp的协商</li><li>一台opensips后对应多台freeswitch</li><li>opensips需要数据库存储相关负载以及保活信息</li><li>多台FS共用数据库</li><li>多台opensips间使用负载均衡中间件（阿里SLB，提供端口检测心跳）</li></ul><h2 id="开启端口-公网"><a href="#开启端口-公网" class="headerlink" title="开启端口(公网)"></a>开启端口(公网)</h2><ul><li>Opensips 开启：7443(wss) 5060(UDP)</li><li>Freeswitch 开启RTP端口段（线路方侧） 5080（线路方SIP） 5060(理论上不需要开启公网，但是出现了Opensip转发BYE通过公网转发给FS，待研究)</li><li>RTPengine 开启RTP端口段（坐席侧）</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="FS安装"><a href="#FS安装" class="headerlink" title="FS安装"></a>FS安装</h3><p>略，修改多台FS的数据库指向同一mysql，用于共享数据。</p><h3 id="RTPEngine安装"><a href="#RTPEngine安装" class="headerlink" title="RTPEngine安装"></a>RTPEngine安装</h3><p><a href="https://github.com/sipwise/rtpengine" rel="noopener" target="_blank">https://github.com/sipwise/rtpengine</a></p><h3 id="Opensips安装"><a href="#Opensips安装" class="headerlink" title="Opensips安装"></a>Opensips安装</h3><ul><li>下载源码并选择模块</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost /]<span class="comment"># cd /usr/local/src </span></span><br><span class="line">[root@localhost src]<span class="comment"># git clone https://github.com/OpenSIPS/opensips.git -b 2.4 opensips-2.4</span></span><br><span class="line">[root@localhost src]<span class="comment"># cd opensips-2.4</span></span><br><span class="line">[root@localhost src]<span class="comment"># yum install mysql mysql-devel gcc gcc-c++ ncurses-devel flex bison</span></span><br><span class="line">[root@localhost opensips-2.4]<span class="comment"># make all</span></span><br></pre></td></tr></table></figure><p>如果这里报错，停止，装好依赖再make all</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opensips-2.4]# make menuconfig</span><br></pre></td></tr></table></figure><p>进入这个菜单后，根据需要使用这个工具（左右键进入返回，空格键选中，回车键确定），但有个必须的是进入<code>Configure Compile Options</code> —&gt; Configure Excluded Modules，按空格选中[*] db_mysql，返回上一级，Save Chnahes, 返回主菜单选择<code>Compile And Install OpenSIPS</code>编译安装即可。完成后会回到这个界面，保存退出。</p><ul><li>重要目录</li></ul><h3 id="配置文件目录"><a href="#配置文件目录" class="headerlink" title="配置文件目录"></a>配置文件目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost /]<span class="comment"># ls /usr/local/etc/opensips/</span></span><br><span class="line">opensips.cfg  opensips.cfg.sample  opensipsctlrc  opensipsctlrc.sample  osipsconsolerc  osipsconsolerc.sample  scenario_callcenter.xml</span><br></pre></td></tr></table></figure><h3 id="运行程序目录"><a href="#运行程序目录" class="headerlink" title="运行程序目录"></a>运行程序目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost /]<span class="comment"># ls /usr/local/sbin</span></span><br><span class="line">curses.out  opensips  opensipsctl  opensipsdbctl  opensipsunix  osipsconfig  osipsconsole</span><br></pre></td></tr></table></figure><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost /]<span class="comment"># cd /usr/local/etc/opensips/</span></span><br><span class="line">[root@localhost opensips]<span class="comment"># vi opensipsctlrc</span></span><br></pre></td></tr></table></figure><h3 id="修改后的配置"><a href="#修改后的配置" class="headerlink" title="修改后的配置"></a>修改后的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SIP_DOMAIN=192.168.0.191</span><br><span class="line">DBENGINE=MYSQL</span><br><span class="line">DBPORT=3306</span><br><span class="line">DBHOST=localhost</span><br><span class="line">DBNAME=opensips</span><br><span class="line">DB_PATH=<span class="string">"/usr/local/etc/opensips/dbtext"</span></span><br><span class="line">DBRWUSER=opensips</span><br><span class="line">DBRWPW=<span class="string">"opensipsrw"</span></span><br></pre></td></tr></table></figure><p>这里主要是mysql连接信息，保证能正常连接即可。还有一个SIP_DOMAIN能连接到本服务的域名或者IP地址即可。</p><h3 id="修改opensips-cfg"><a href="#修改opensips-cfg" class="headerlink" title="修改opensips.cfg"></a>修改opensips.cfg</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opensips]<span class="comment"># vi opensips.cfg</span></span><br></pre></td></tr></table></figure><h3 id="修改配置项"><a href="#修改配置项" class="headerlink" title="修改配置项"></a>修改配置项</h3><p>注意，下面的配置文件只是简单的示例，很多功能比如 save location、wss、rtpengine没有使用到，鉴于公司项目原因，暂时不公开脚本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">log_level=3  #老版本日志级别参数为debug，级别范围1-4，建议生产上设置为1，为3或者4时将产生大量日志，磁盘空间很快就不够了</span><br><span class="line">sip_warning=0</span><br><span class="line">log_stderror=yes</span><br><span class="line">log_facility=LOG_LOCAL0</span><br><span class="line">log_name=&quot;opensips&quot;</span><br><span class="line">#debug_mode=yes#开启时需要直接opensips 启动前端模式</span><br><span class="line">children=2</span><br><span class="line">dns_try_ipv6=no</span><br><span class="line">auto_aliases=no</span><br><span class="line">listen=udp:10.0.11.84:5060</span><br><span class="line">mpath=&quot;/usr/local//lib64/opensips/modules/&quot;</span><br><span class="line">#fork=no</span><br><span class="line">loadmodule &quot;db_mysql.so&quot;</span><br><span class="line">loadmodule &quot;signaling.so&quot;</span><br><span class="line">loadmodule &quot;sl.so&quot;</span><br><span class="line">loadmodule &quot;tm.so&quot;</span><br><span class="line">loadmodule &quot;rr.so&quot;</span><br><span class="line">loadmodule &quot;uri.so&quot;</span><br><span class="line">loadmodule &quot;dialog.so&quot;</span><br><span class="line">loadmodule &quot;maxfwd.so&quot;</span><br><span class="line">loadmodule &quot;textops.so&quot;</span><br><span class="line">loadmodule &quot;mi_fifo.so&quot;</span><br><span class="line">loadmodule &quot;dispatcher.so&quot;</span><br><span class="line">loadmodule &quot;load_balancer.so&quot;</span><br><span class="line">loadmodule &quot;sipmsgops.so&quot;</span><br><span class="line">loadmodule &quot;proto_udp.so&quot;</span><br><span class="line">modparam(&quot;mi_fifo&quot;, &quot;fifo_name&quot;, &quot;/tmp/opensips_fifo&quot;)</span><br><span class="line">modparam(&quot;dialog&quot;, &quot;db_mode&quot;, 1)</span><br><span class="line">modparam(&quot;dialog&quot;, &quot;db_url&quot;,&quot;mysql://root:root@10.0.11.84/opensips&quot;)</span><br><span class="line">modparam(&quot;rr&quot;, &quot;enable_double_rr&quot;, 1)</span><br><span class="line">modparam(&quot;rr&quot;, &quot;append_fromtag&quot;, 1)</span><br><span class="line">modparam(&quot;tm&quot;, &quot;fr_timer&quot;, 2)</span><br><span class="line">modparam(&quot;dispatcher&quot;, &quot;ds_ping_method&quot;, &quot;OPTIONS&quot;)</span><br><span class="line">modparam(&quot;dispatcher&quot;, &quot;ds_ping_interval&quot;, 5)</span><br><span class="line">modparam(&quot;dispatcher&quot;, &quot;ds_probing_threshhold&quot;, 2)</span><br><span class="line">modparam(&quot;dispatcher&quot;, &quot;ds_probing_mode&quot;, 1)</span><br><span class="line">modparam(&quot;dispatcher&quot;, &quot;db_url&quot;,&quot;mysql://root:root@10.0.11.84/opensips&quot;)</span><br><span class="line">modparam(&quot;load_balancer&quot;, &quot;db_url&quot;,&quot;mysql://root:root@10.0.11.84/opensips&quot;)</span><br><span class="line">modparam(&quot;load_balancer&quot;, &quot;probing_method&quot;, &quot;OPTIONS&quot;)</span><br><span class="line">modparam(&quot;load_balancer&quot;, &quot;probing_interval&quot;, 5)</span><br><span class="line">route&#123;</span><br><span class="line">        if (!mf_process_maxfwd_header(&quot;10&quot;)) &#123;</span><br><span class="line">                sl_send_reply(&quot;483&quot;,&quot;Too Many Hops&quot;);</span><br><span class="line">                exit;</span><br><span class="line">        &#125;</span><br><span class="line">        if (!has_totag()) &#123;</span><br><span class="line">                record_route();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">                loose_route();</span><br><span class="line">                t_relay();</span><br><span class="line">                exit;</span><br><span class="line">        &#125;</span><br><span class="line">        if (is_method(&quot;CANCEL&quot;)) &#123;</span><br><span class="line">                if ( t_check_trans() )</span><br><span class="line">                        t_relay();</span><br><span class="line">                exit;</span><br><span class="line">        &#125;</span><br><span class="line">        if (is_method(&quot;INVITE&quot;)) &#123;</span><br><span class="line">                if ( !lb_start(&quot;1&quot;,&quot;pstn&quot;)) &#123;</span><br><span class="line">                        send_reply(&quot;500&quot;,&quot;No Destination available&quot;);</span><br><span class="line">                        exit;</span><br><span class="line">                &#125;</span><br><span class="line">                t_on_failure(&quot;GW_FAILOVER&quot;);</span><br><span class="line">                #if (!load_balance(&quot;1&quot;,&quot;pstn&quot;,&quot;1&quot;)) &#123;</span><br><span class="line">                #        send_reply(&quot;503&quot;,&quot;Service Unavailable&quot;);</span><br><span class="line">                #        exit;</span><br><span class="line">                #&#125;</span><br><span class="line">        &#125;</span><br><span class="line">        else if (is_method(&quot;REGISTER&quot;)) &#123;</span><br><span class="line">                if (!ds_select_dst(&quot;1&quot;, &quot;0&quot;)) &#123;</span><br><span class="line">                        send_reply(&quot;503&quot;,&quot;Service Unavailable&quot;);</span><br><span class="line">                        exit;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">                send_reply(&quot;405&quot;,&quot;Method Not Allowed&quot;);</span><br><span class="line">                exit;</span><br><span class="line">        &#125;</span><br><span class="line">        if (!t_relay()) &#123;</span><br><span class="line">                sl_reply_error();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">failure_route[GW_FAILOVER] &#123;</span><br><span class="line">        if (t_was_cancelled()) &#123;</span><br><span class="line">                exit;</span><br><span class="line">        &#125;</span><br><span class="line">        # failure detection with redirect to next available trunk</span><br><span class="line">        if (t_check_status(&quot;(408)|([56][0-9][0-9])&quot;)) &#123;</span><br><span class="line">                xlog(&quot;Failed trunk $rd/$du detected \n&quot;);</span><br><span class="line">                if ( lb_next() ) &#123;</span><br><span class="line">                        t_on_failure(&quot;GW_FAILOVER&quot;);</span><br><span class="line">                        t_relay();</span><br><span class="line">                        exit;</span><br><span class="line">                &#125;</span><br><span class="line">                send_reply(&quot;500&quot;,&quot;All GW are down&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里listen如果你不确定该怎么填的话，运行下面的命令看一下，一般是本机IP。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opensips]<span class="comment"># ip route get 8.8.8.8 | head -n +1 | tr -s " " | cut -d " " -f 7</span></span><br></pre></td></tr></table></figure><h3 id="部分关键脚本"><a href="#部分关键脚本" class="headerlink" title="部分关键脚本"></a>部分关键脚本</h3><ul><li>删除<code>transport=wss</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$var(ct) = $ct;</span><br><span class="line">$var(str) = &apos;;transport=wss&apos;;</span><br><span class="line">$var(pos) = $(var(ct)&#123;s.index, $var(str)&#125;);</span><br><span class="line">$var(end) = $var(pos) + $(var(str)&#123;s.len&#125;);</span><br><span class="line">$var(res) = $(var(ct)&#123;s.substr, 0, $var(pos)&#125;) + $(var(ct)&#123;s.substr, $var(end), 0&#125;);</span><br><span class="line">remove_hf(&quot;Contact&quot;);</span><br><span class="line">append_hf(&quot;Contact: $var(res)\r\n&quot;);</span><br></pre></td></tr></table></figure><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opensips]<span class="comment"># cd /usr/local/sbin</span></span><br><span class="line">[root@localhost sbin]<span class="comment"># opensipsdbctl create</span></span><br><span class="line">……</span><br><span class="line">INFO: creating database opensips ...</span><br><span class="line">INFO: Using table engine MyISAM.</span><br><span class="line">INFO: Core OpenSIPS tables successfully created.</span><br><span class="line">Install presence related tables? (Y/n): y</span><br><span class="line">INFO: creating presence tables into opensips ...</span><br><span class="line">INFO: Presence tables successfully created.</span><br><span class="line">Install tables <span class="keyword">for</span> </span><br><span class="line">    b2b</span><br><span class="line">    cachedb_sql</span><br><span class="line">    call_center</span><br><span class="line">    carrierroute</span><br><span class="line">    cpl</span><br><span class="line">    domainpolicy</span><br><span class="line">    emergency</span><br><span class="line">    fraud_detection</span><br><span class="line">    freeswitch_scripting</span><br><span class="line">    imc</span><br><span class="line">    registrant</span><br><span class="line">    siptrace</span><br><span class="line">    userblacklist</span><br><span class="line">? (Y/n): y</span><br><span class="line">INFO: creating extra tables into opensips ...</span><br><span class="line">INFO: Extra tables successfully created.</span><br></pre></td></tr></table></figure><p>之后就是根据提示傻瓜操作创建数据库就好了，如果前面的mysql环境没装好，数据库连接有问题，这里就会报错，如果提示类似下面的编码问题，输入latin1即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING: Your current default mysql characters set cannot be used to create DB. Please choice another one from the following list:</span><br></pre></td></tr></table></figure><p>这一步完成之后，会在数据库新建一个opensips（名字是在上面的配置文件里设置的）的数据库。</p><h3 id="将日志输出到指定文件"><a href="#将日志输出到指定文件" class="headerlink" title="将日志输出到指定文件"></a>将日志输出到指定文件</h3><p>在/etc/rsyslog.conf追加OpenSIPS日志输出配置。（注：OpenSIPS配置文件中log_stderror和debug_mode需设置成no，否则可能不能输出单独日志）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"local0.*                        /var/log/opensips.log"</span> &gt;&gt;/etc/rsyslog.conf</span><br></pre></td></tr></table></figure><p>修改配置文件后需要重启日志服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service rsyslog restart</span><br></pre></td></tr></table></figure><h3 id="启动opensips"><a href="#启动opensips" class="headerlink" title="启动opensips"></a>启动opensips</h3><ul><li><p>启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost sbin]<span class="comment"># opensipsctl start</span></span><br><span class="line">INFO: Starting OpenSIPS : </span><br><span class="line">INFO: started (pid: 26051)</span><br></pre></td></tr></table></figure></li><li><p>查看opensips进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost sbin]<span class="comment"># ps -aux | grep opensips</span></span><br><span class="line">root      3504  0.0  0.4  70536  4420 ?        S    3月07   0:00 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3505  3.1  0.1  70776  1368 ?        S    3月07  12:35 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3506  0.1  0.0  70536   476 ?        S    3月07   0:29 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3507  0.0  0.0  70536   688 ?        S    3月07   0:08 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3508  0.0  0.2  70536  2396 ?        S    3月07   0:03 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3509  0.0  0.1  70536  1424 ?        S    3月07   0:01 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3510  0.0  0.1  70536  1912 ?        S    3月07   0:01 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3511  0.0  0.2  70536  2392 ?        S    3月07   0:01 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br><span class="line">root      3512  0.0  0.1  70536  1164 ?        S    3月07   0:01 /usr/<span class="built_in">local</span>/sbin/opensips -P /var/run/opensips.pid</span><br></pre></td></tr></table></figure></li></ul><h2 id="配置Opensips"><a href="#配置Opensips" class="headerlink" title="配置Opensips"></a>配置Opensips</h2><h3 id="添加负载节点"><a href="#添加负载节点" class="headerlink" title="添加负载节点"></a>添加负载节点</h3><p>在数据库添加两个负载节点信息，地址对应多个FS的地址等<br>添加后可以通过<code>opensipsctl fifo lb_reload</code> 进行reload</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`opensips`</span>.<span class="string">`load_balancer`</span> (<span class="string">`id`</span>, <span class="string">`group_id`</span>, <span class="string">`dst_uri`</span>, <span class="string">`resources`</span>, <span class="string">`probe_mode`</span>, <span class="string">`description`</span>) <span class="keyword">VALUES</span> (<span class="string">'1'</span>, <span class="string">'1'</span>, <span class="string">'sip:172.16.100.10'</span>, <span class="string">'vm=100;conf=100;transc=100;pstn=500'</span>, <span class="string">'1'</span>, <span class="string">'FS1'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`opensips`</span>.<span class="string">`load_balancer`</span> (<span class="string">`id`</span>, <span class="string">`group_id`</span>, <span class="string">`dst_uri`</span>, <span class="string">`resources`</span>, <span class="string">`probe_mode`</span>, <span class="string">`description`</span>) <span class="keyword">VALUES</span> (<span class="string">'2'</span>, <span class="string">'1'</span>, <span class="string">'sip:172.16.100.11'</span>, <span class="string">'vm=100;conf=100;transc=100;pstn=500'</span>, <span class="string">'1'</span>, <span class="string">'FS2'</span>);</span><br></pre></td></tr></table></figure><h3 id="添加FreeSWITCH到调度列表-转发SIP消息的路由"><a href="#添加FreeSWITCH到调度列表-转发SIP消息的路由" class="headerlink" title="添加FreeSWITCH到调度列表(转发SIP消息的路由)"></a>添加FreeSWITCH到调度列表(转发SIP消息的路由)</h3><p>执行以下命令将两个节点添加到调度列表，（这里添加调度器的命令和1.7版本是有区别的）<br>添加后可以通过<code>opensipsctl fifo ds_reload</code> 进行reload</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">opensipsctl dispatcher addgw 1 sip:172.16.100.10 5060 0 50 <span class="string">'FS1'</span> <span class="string">'节点1'</span></span><br><span class="line">opensipsctl dispatcher addgw 1 sip:172.16.100.11 5060 0 50 <span class="string">'FS2'</span> <span class="string">'节点2'</span></span><br></pre></td></tr></table></figure><p>添加调度列表成功后，在两个FreeSWITCH的控制通过siptrace on打开sip消息跟踪即可看到OpenSIPS节点不停在跟FreeSWITCH通过”OPTIONS“消息进行握手。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/ec271ad5/1.png&quot; alt=&quot;image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="运维技术" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"/>
    
      <category term="服务搭建" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Linux" scheme="https://wandouduoduo.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>做好运维真的不容易</title>
    <link href="https://wandouduoduo.github.io/articles/71d403c4.html"/>
    <id>https://wandouduoduo.github.io/articles/71d403c4.html</id>
    <published>2020-12-31T09:09:04.000Z</published>
    <updated>2020-12-31T09:17:30.024Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>以前和一个项目经理沟通时，提到能否提前半天将变更申请提交过来，而这位项目经理很不理解的看着我问：“你们运维不就是在生产环境部署个程序嘛，这么简单的工作还需要提前半天？而且你们不懂程序，也评审不出什么吧？”。运维这么多年来，听到对运维工作抱有这种认识和看法的人很多。它侧面反映了企业中其他团队团队对运维的认识往往局限于一些简单操作性的工作。例如：应用服务故障时重启、应用配置变更、增删改查数据或者所有软硬件使用问题等等。运维真的是这样吗？</p><a id="more"></a><p><strong>如何理解运维呢？</strong></p><p>百度百科对运维的定义是：企业 IT 部门采用相关的方法、手段、技术、制度、流程和文档等，对IT 软硬运行环境(软件环境、网络环境等)、IT 业务系统和 IT 运维人员进行的综合的管理。从定义中看，运维岗位是一个综合了技术和管理能力复合型岗位，并且需要掌握大量的方法论与技术栈，是这么一个角色。</p><p>运维就运维技术与资源可以狭义定义为“监、管、控”三点。技术与资源主要是支撑运维/运营的质量、效率和成本的平衡。以下简单摘录了运维的一些能力要求：</p><ul><li><strong>运维规范的落地</strong>：以ITIL、ISO20000、ITSS.1等方法论，结合外部监管及内部规范的落地；</li><li><strong>监管机构的要求落地</strong>：理解、快速响应、落地监管机构的管理要求；</li><li><strong>基本保障</strong>：配置、监控、应用发布、资源扩容、事件、问题等；</li><li><strong>基础能力</strong>：网络、服务器、操作系统、数据库、中间件、JVM、应用等基本使用与调优；</li><li><strong>业务服务能力</strong>：SLA，服务台、业务咨询、维护、经验库、等支持能力；</li><li><strong>可用性管理能力</strong>：巡检、业务系统连续性、可用性，基础架构及应用系统的高可用、备件冗余资源；</li><li><strong>风险、安全管理能力</strong>：操作、审计、监管风险，漏洞、攻击管控；</li><li><strong>故障管理能力</strong>：事件、问题管理水平与能力；</li><li><strong>持续交付能力</strong>：应用变更、基础资源、办公服务交付能力；</li><li><strong>主动优化能力</strong>：架构优化、性能响应效率、客户体验等</li><li><strong>应急演练</strong>：架构高可用、突发事件、业务故障的架构、方案、文档、人员熟练程度等</li><li><strong>业务支撑</strong>：数据维护、数据提取、参数维护等；</li><li><strong>运行分析能力</strong>：容量、性能、可用性分析等；</li><li><strong>运营能力</strong>：促进业务痛点的发现与解决、客户及业务业务体验等；</li><li><strong>成本控制</strong>：更好的评估人力、硬件、带宽、软件，节省成本；</li><li><strong>运维开发</strong>：运维自动化工具的建设，运维开发能力的培养；</li><li><strong>其它</strong></li></ul><p>不同企业需要运维的能力会有不同的扩展，同时上述能力要求的每一点展开来说都是一个复杂的技术栈。例如：<code>基础能力</code>中的linux操作系统内核优化，就让运维人员对技术能力的深度有了严格要求。要不你搞不定的。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/1.png" alt="图片"></p><p>说道这里肯定会有人说：上述技术栈的能力要求通常是存在于个别公司中，这种公司运维团队仍处在传统式运维阶段，并且自动化程度不高。的确理论上所有运维操作性、执行命令的工作都可以整合为经验，并通过自动化工具落地实现。现在大部分互联网企业对外都鼓吹说公司自动化运维工作覆盖面很高，己经开始迈向智能化，做到了AIOps，甚至提出了NoOps的解决方案等等。对于这些互联网公司的自动化对日常运维工作真实覆盖面暂时无法考证，但以我个人经验来看，至少金融企业的自动化覆盖面还有很长的路要走，并且肯定还会很大一部份工作很难自动化。毕竟工作类型太多，在有限的投入上只能集中精力去做投入产出比更高的运维自动化工作。下图以一个运维工具思维导图来简单列示一些常规的运维操作，可以看出其实很难有一套能解决所有运维操作的工具或平台。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/2.jpg" alt="img"></p><p>并且随着业务要求越来越高、规模越来越大、监管要求越来越严，纵使外部如何宣称自动化或智能化对运维人员经验、技术和管理能力替代，企业内的运维还需要认清实际情况，并结合企业整体战略定位，强调运维团队在运维管理与技术能力方面的广度和深度，再有侧重、分先后的逐步实现自动化。在未来相当长一段时间，企业的运维岗位仍是一个复杂、综合性比较强的工作岗。</p><h2 id="运维之痛"><a href="#运维之痛" class="headerlink" title="运维之痛"></a>运维之痛</h2><p>近年来互联网的飞速普及，运维技术也快速发展，各行业运维水平也得到了较大提升，同时运维圈技术分享也越来越开放，从国外google的SRE理念，到国内腾讯的蓝鲸、织云等平台，以及借助于各种运维专题的公众号和运维大会等等，有大量的互联网人、企业的运维团队进行分享。就对我的影响来说，萧田国的《运维 2.0：危机前的自我拯救》，将我头脑中零碎理解整合成一个概念，再接下来运维人员开始有了一些反思与自嘲，如下截图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/3.jpg" alt="图片"></p><h3 id="团队之痛"><a href="#团队之痛" class="headerlink" title="团队之痛"></a><strong>团队之痛</strong></h3><p>前面讲到，在企业内部其它团队对运维的认识通常只是简单操作，出故障时才会想到，才回去找的团队。随着信息技术的发展与业务的发展，运维团队的痛点越来越明显，企业内对运维团队的不满声音也越来越多，反思一下原因，可分为外部客观原因和内部因素。</p><h4 id="外部原因"><a href="#外部原因" class="headerlink" title="外部原因"></a>外部原因</h4><p>当前在大数据的背景下，所有互联网企业的运维都面临着业务规模不断扩大，业务竞争越来越激烈，监管要求越来越严格，数据中心条件也越来越高，大量新技术、开源架构、平台或系统的引入取代了传统稳定系统架构的等等因素影响。</p><ul><li><strong>运维团队的角色</strong>：大部分运维团队都是一个技术支持部门，企业对运维团队的重视程度通常不如业务开发团队，更不用说是前端业务部门。这就造成了运维部门的规模和人员储备不大。像google在《google sre运维解密》一书中提到，由于google的数据中心规模急剧扩大，系统越来越复杂，而运维人员规模又跟不上，所以他们的运维团队采用组建sre的运维开发团队来实现自救。</li><li><strong>业务对运维服务质量的要求</strong>：互联网的发展，让现在越来越多的业务从线下走到了线上。 为了赢得更多用户的青睐和提高用户体验。一方面业务要求更多和体验更佳的业务性能；另一方面业务对应用发布的交付速度有了更高的要求。前者会产生更复杂的系统架构设计，后者需要更高效的应用发布迭代支持。但两者都会对系统响应效率和稳定性带来影响。</li><li><strong>外部监管要求：</strong>长期以来，为了防范金融风险，监管机构对金融企业保持强监管的方式，十九大后，监管对金融企业的信息技术的稳定性和规范性进一步加强。在强监管情况下，信息系统的稳定性有了进一步保证，但也给运维团队带来更高的要求，客观上也加大了工作量，并由于规范流程而带来的工作效率的下降。</li><li><strong>业务并发要求：</strong>用户量的激增和营销活动不断推出，需要系统具备更高的并发处理能力，架构不断引入大量分布式、开源架构来替代传统相对成熟稳定的架构来满足业务需要，这些变化都给运维的能力带来挑战。</li><li><strong>数据中心规模增大：</strong>数据中心的多中心建设，虚拟化上云，去中心化等等，分布式架构的引入也使得应用系统规模成倍的增加。</li></ul><h4 id="内部因素"><a href="#内部因素" class="headerlink" title="内部因素"></a>内部因素</h4><p>网上有一个调查数据，在整个运维成本的分配中，软硬件和网络设备的维护成本占 30%，维护服务成本占30%，而内部运维的人力成本则占了40%。这里的人力成本包括人员招聘、培训和流失等成本，如果将维护服务成本也纳入到人力成本上，则人力这一块的成本将上升为70%，影响这个人力成本的因素主要有：</p><ul><li><strong>运维能力模型：</strong>ITIL、ISO20000、ITSS.1是运维领域中比较成体系化的方法论（目前更为火爆的devops更倾向于是一种思路），其中只有ITSS.1提出了运维能力模型的概念，但在量化运维人员具体能力的实际操作上比较难落地。简单的说你很难判断一个运维人员如何做才是做得优，如何是中，如何差，而这些评价通常比较主观，这也客观影响了运维人员不断增加技能、优化工作效率的动力。</li><li><strong>运维规范化：</strong>团队扩大到一定规模，以口口相传的传授，结合个体责任心、工作习惯为主的工作方式很容易出现操作风险，且无法进行量化绩效管理，管理规范无法落地。</li><li><strong>运维精细化程度：</strong>运维团队通常是从纵向职能型的方式形成，这种方式能培养全能型、经验丰富的专家式人才，这些专家式人才利用经验能快速解决职责下的常规问题，且效率比较高，适合小型的团队。随着团队的不断壮大，面对的问题越来越复杂，技术要求越来越多，一方面人的精力有限，很多人不能满足这种专家式人才的要求；另一方面也会产生很多重复性的工作；同时对于人员流失带来的影响比较大。这时就需要将纵向工作精细化，再辅助横向人员对工作进行持续的优化。</li><li><strong>运维目标：</strong>运维的目标往往以被动式的目标为主，被动处理故障、被动解决问题、被动提供应用交付、被动节省成本等，这种被动式的运维目标导致计划性工作无法开展，缺乏持续不断的自我提高优化机制，主动提高效率、质量，降低成本，并由运维向主动运营目标去转变。</li><li><strong>自动化能力：</strong>IT软硬件体量庞大，且增长迅速，手工操作的机器任务太多；运维数据越来越多；故障定位越来越难，人工经验依赖高；监控手段不够及时、全面；应用发布、资源交付效率低下；没有主动的容量、性能分析、体验分析能力……这些都是常见的一些痛点。</li></ul><h3 id="个体之痛"><a href="#个体之痛" class="headerlink" title="个体之痛"></a>个体之痛</h3><p>​       作为运维人员同样面临不少痛点：工作时间、工作性质、工作压力、学习压力、职业发展等等，下面简单罗列：</p><ul><li><strong>7*24小时制的工作时间</strong>：运维人员的节假日是不完整的，通常节假日需要运维值班保障或在家通过VPN远程操作、甚至和家人团聚时还需要远程指导进行应急排障；运维人员工作性质不同其他人，为了不影响业务，应用的发布、基础设施的变更、演练等工作都会放到晚上，对客的业务系统还可能要安排到深夜凌晨。这种随时可能发生，随时处理的工作状态是其它人员所不具备的痛点。</li><li><strong>高度压力的工作</strong>：“如履薄冰”这个词很好的形容了运维的工作状态。因为任何一个生产操作都可能对业务带来影响，所以运维的操作必须十分细心谨慎。同时在运维故障处理时，需要面对来自业务、客户、开发、领导的各层压力，细心冷静的排除故障，是一个考研心理素质的高压工作。</li><li><strong>被动的工作</strong>：常有人形容运维就是一个“消防员”，常常被动救火，这个形容很贴切。在缺乏一些主动分析、优化、预测性的工作的背景下，运维团队的大部份工作常以被动为主，是负责应急救火、打扫战场、负责收尾的那群默默的人。</li><li><strong>对工作的认识</strong>：运维人通常会认为运维就是一个背锅侠的角色。像开发程序问题、硬件问题、系统软件问题、业务需求问题等等都需要运维去解决处理，而这些问题对业务可用性的影响也要运维来承担，这也是运维的跗骨之痛，很影响个人心情和工作积极性。</li><li><strong>职业压力</strong>：运维工作一方面主要要和机器或系统软件打交道，相对于开发、项目管理等IT岗位来说，转型机会的面比较窄；同时，运维岗位中重复操作性的工作占比较多，如缺乏引导容易让运维人员产生麻木的状态，失去持续改善的动力和积极性；另外运维需要掌握的技能和管理理念很多，这对于运维人员的学习能力也有很高要求。</li></ul><h2 id="自救"><a href="#自救" class="headerlink" title="自救"></a>自救</h2><h3 id="SRE"><a href="#SRE" class="headerlink" title="SRE"></a><strong>SRE</strong></h3><p>SRE这个名词最早是从《google sre 运维解密》一书中获得，全称是Site Reliability Engineering，翻译过来就是：站点可靠性工程师。google对SRE的职责描述是：确保站点的可用性。</p><p>为了达到这个目的，一方面他需要对站点涉及的系统、组件熟悉，也要关注生产运行时的状态，为此，他需要自研并维护很多工具和系统支撑系统的运行，比如自动化发布系统，监控系统，日志系统，服务器资源分配和编排等等。SRE是一个综合素质很高的全能手，能力进行分解主要有三块：</p><ul><li><strong>熟悉系统架构与运行状态</strong>：SRE需要懂服务器基础架构、操作系统、网络、中间件容器、常用编程语言、全局的架构意识、非常强的问题分析能力、极高的抗压能力（以便沉着高效地排障），他们还需要懂性能调优理论。为了保证系统架构的高可用，SRE甚至会有意识的破坏自己的系统，以提高系统可用性。</li><li><strong>熟悉运维涉及的管理方法</strong>：SRE需根据企业自身发展需要，清楚运维涉及的各项工作的流程方法论，比如故障处理、应用发布、可用性管理等等，SRE十分重视运维流程的持续改善，比如对故障的追根溯源，怀疑一切的方式持续改进。</li><li><strong>运维开发+产品经理</strong>：SRE在运行保障过程中的手段更加自动化，更高效，这种高效来源于自动化工具、监控工具的支撑，且他们还需要是这些工具的主要开发者，他们要不断优化和调整，使整个工具使用起来更加得心应手。为此SRE有一个一半一半理念，就是50%用于日常保障，50%用于项目性的工作，这个项目性的工作主要体现在运维开发与运维产品经理的角色。</li></ul><h3 id="运维开发"><a href="#运维开发" class="headerlink" title="运维开发"></a>运维开发</h3><p>运维开发主要体现在运维工具层面，不同的团队有不同的理解，通常有三类：</p><ul><li><strong>完全自建：</strong>运维开发团队利用开源技术结合自身需要进行二次开发。这种方式在互联网企业比较流行，具体的成效大小和收效与这个运维开发团队的整体规划或资源投入有关；</li><li><strong>外购开发资源或工具产品</strong>：运维开发团队主要是结合企业痛点承担产品经理的角色。调研、外购、使用、维护，这种方式常出现在传统的企业，尤其适用于投入运维开发人员较少的企业，这种方式是投入收效快，但是对外部资源依赖比较大，不利于后续持续优化和改善；</li><li><strong>外购与自建相结合：</strong>运维开发团队在整个工具体系下，针对部份组件选择性的引入一些成熟的工具体系，同时要求这类成熟的工具需要开放一定的接口或源码支持，对于一些与公司个性强的环节采用自研的方式。这种方式目前逐渐被运一些传统企业，比如金融企业所接受。</li></ul><p>总的来说，不管选用上面哪一种方式，运维开发团队都应该有一个整体、统一的一体化工具建设规划，并在建设过程中始终保持对运维工具体系的掌控能力，并在工具体系的上层为其它运维人员提供简易的、可创造性的“开发能力”，比如所见即所得的工具可视化、可定制的运维报表、拖拉拽方式的流程及脚本组件的拼装等运维开发方式。</p><h3 id="DevOps"><a href="#DevOps" class="headerlink" title="DevOps"></a><strong>DevOps</strong></h3><p><strong>DevOps概述</strong></p><p>DevOps一词的来自于Development和Operations的组合，突出软件开发人员和运维人员的沟通合作，通过自动化流程来使得软件构建、测试、发布更加快捷、频繁和可靠，他是一种方法论，包含一套基本原则和实践，工具是为有效落实这套方法论提供支持。</p><p>在软件全生命周期管理过程中，包括开发，构建，测试，发布，运营，在这个全生命周期管理过程中出现了开发团队与运维团队的部门墙，这是因为开发团队关注需求的实现，希望尽快实现变更；而运维团队更关注于系统运行的稳定，而变更往往是生产应用不稳定的原因。DEVOPS方法论的出现主要是为了解决这个协作问题，目的是让软件交付更加高效，质量更高，生产端更加敏捷，生产运行过程中的问题能更加高效的反馈到开发，形成一个全生命周期的闭环。随着业务对运维交付能力的时效性要求越来越高，运维团队面临“吃力不讨好”的问题：</p><p><strong>吃力</strong>：花费大量时间在应用部署的操作性工作中。这部份部署变更包括新功能的上线以及修复功能BUG两方法。</p><p><strong>不讨好</strong>：操作性的工作越多，带来的操作风险就越大。有个统计，如果手工运行5条命令的情况下，成功部署的概率就已跌至86%；如需手工运行55条命令，成功部署的概率将跌至22%；如需手工运行100条命令，成功部署的概率将趋近于0（仅为2%）。</p><p>DevOps这一理念鼓励开发者和运维人员之间所进行的<strong>沟通</strong>、<strong>协作</strong>、<strong>集成</strong>和<strong>自动化</strong>，借此有助于改善双方在交付软件过程中的速度和质量。侧重于通过标准化开发环境和自动化交付流程改善交付工作的可预测性、效率、安全性，以及可维护性。</p><h3 id="实践中的DevOps"><a href="#实践中的DevOps" class="headerlink" title="实践中的DevOps"></a><strong>实践中的DevOps</strong></h3><p>可以从工具链、团队文化、自动化、敏捷看板等角度讲DevOps，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/4.jpg" alt="图片"></p><p><strong>从DevOps的落地效率来看</strong>：需要将DevOps进行聚焦，聚焦到交付能力上，这方面，行业里比较标准化的评估是去年底由中国信息通信研究院，联合一些互联网企业、运维社区，以及一些金融、传统企业联合进行编制的DevOps标准（券商行业中华泰参加了编制）。从这个能力模型公布出来的一些介绍看，标准对DevOps范围比较克制主要以交付能力来分解敏捷开发、持续交付、技术运营、应用架构、团队架构，这和最早的DevOps能力环比较吻合：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/5.jpg" alt="图片"></p><p><strong>从运维的交付场景看</strong>：主要是资源交付与应用交付，其中资源交付以IAAS、PAAS云的建设为主，通过云管平台的工具链将基础设施、网络、硬件、虚拟化、容器、运行中间件等系统软硬件交付能力自动化，并通过CMDB整合DevOps能力环之上的应用场景，实现资源的快速交付。资源交付能力主要在于IAAS、PAAS层的云平台标准化、自动化、平台扩展性等方面的建设程度。应用的快速交付比资源交付更为复杂，应用交付涉及全链路的整合，链路上的节点越多落地的难度越大，因为它不仅涉及技术，还涉及理念的认同与聚焦。应用交付能力要实现，最简单的技术栈工具需要CMDB、应用发布工具、应用版本库、监控工具，上述工具对内要与云平台对接，对外要提供接口给开发、测试工具。当然如开发、测试也能和运维使用同一套发布工具、应用版本库则效果更好，不过，实际实施过程中团队之间还是会有不少冲突，比如开发关注源代码版本管理，测试、运维关注运行版本的管理，需各个团队共同付出共建技术链。</p><h3 id="运营"><a href="#运营" class="headerlink" title="运营"></a><strong>运营</strong></h3><p>关于运维圈里运营的概念，以转型口号喊得比较多，我对运维当中的运营有业务运营与技术运营两个维度的理解。业务运营是通过功能优化或工具开发等方式解决业务工作痛点，或通过运行分析发现影响业务开展的因素，并推动相关的优化，最终提升业务能力。技术运营则主要从技术角度去降低IT成本，提升IT服务质量与效率。具体的实施内容可以考虑如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/6.jpg" alt="img"></p><p>从上述概括可以看出，当前运维里面的运营，与运维数据密切相关，需要基于运维大数据平台来提升运营质量。</p><p>为了进一步说明运营，这里举两个例子：</p><h4 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h4><p>优锘科技CEO的陈傲寒在2016年写过一篇文章《IT：从运维到运营》，虽然己过去1年多，仍是我读过最好的一篇。全文从企业、运维团队角度出发分析什么是运维、什么是运营，再将运营分解到不同角色上的理解与落地的方向，全文均是干货，值得通读，这里只列出一个思维导图。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/7.jpg" alt="图片"></p><h4 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h4><p>腾讯QQ逐渐被微信团队替代过程中，QQ技术运维团队是如何通过各种方式去为企业带来效益，比如他们通过运维分析，得到如何更加合理的使用带宽、资源，大大减少了公司在基础设施方面的投入。在企业中，也同样有很多空间可以去尝试，比如分析业务痛点，为业务提供快速的策略性的工具来替代重复操作性的业务操作；通过运维数据分析，发现客户体验方面的痛点，推动业务功能的优化等等。</p><h3 id="AIOPS"><a href="#AIOPS" class="headerlink" title="AIOPS"></a>AIOPS</h3><p>AIOps这个词最早是在2016年由Gartner提出。AIOps是Algorithmic IT Operations的缩写，是基于算法的IT运维，即通过使用统计分析和机器学习的方法处理从各IT设备、业务应用、运维工具收集的数据，从而加强增强运维自动化能力，以便更快、更有效、更全面的实现自动化效果。以下是Gartner提出AIOps的一张图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/8.jpg" alt="图片"></p><p>Gartner通过使用图1中的图解释了AIOps平台的工作原理。AIOps有两个主要组件：大数据和机器学习。它需要从孤立的IT数据中移除，以便将大量数据平台内的观察数据（例如监控系统和作业日志中发现的数据）与参与数据（通常在故障单，事件和事件记录中找到）相结合。AI然后针对组合的IT数据实施全面的分析和机器学习（ML）策略。期望的结果是持续的见解，通过自动化产生持续的改进和修复。AI可以被认为是核心IT功能的持续集成和部署（CI / CD）。</p><ul><li><strong>广泛和多样化的IT数据源</strong>：如日志类的设备日志、系统日志，应用日志、运维操作日志；指标类的监控性能指标、事件。</li><li><strong>具备针对海量数据处理与分析的运算平台</strong>：能够从现有的IT数据生成新的数据和元数据、计算和分析还消除噪音，识别模式或趋势，隔离可能的原因，揭示潜在问题，并实现其他IT特定目标。</li><li><strong>算法</strong>，充分利用IT领域的专业知识，更适当，高效的处理数据。</li><li><strong>机器学习</strong>，从根据算法分析的输出和引入系统的新数据自动更改或创建新的算法。</li><li><strong>可视化</strong>，以易于消费的方式向IT行动提供洞察和建议，以促进理解和行动。</li><li><strong>自动化</strong>，其使用分析和机器学习产生的结果自动创建和应用响应或改进已识别的问题。</li></ul><p>关于分层的思路，Gartner这样理解：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/9.jpg" alt="img"></p><h3 id="AIOps与自动化的关系"><a href="#AIOps与自动化的关系" class="headerlink" title="AIOps与自动化的关系"></a><strong>AIOps与自动化的关系</strong></h3><p>AIOps很火，所以对AIOps和自动化做了一些对比。暂以一句话作个区别：AIOps是基于对运维数据（日志类、指标类数据等）的机器学习，进一步解决自动化成本高或无法解决的问题，属于<strong>运维自动化的优化，</strong>细化一下区别有：</p><ul><li><strong>概念</strong>：</li></ul><p>狭义的自动化则提运维“监、管、控”的工具。AIOps是将AI技术应用到运维领域，需要有学习、类人交互、主动决策的特征。</p><ul><li><strong>实现思路</strong>：</li></ul><p>自动化往往以过程为导向，AIOps则以目标为导向，通过对数据进行学习，得到如何实现目标。</p><ul><li><strong>门槛高度</strong>：</li></ul><p>自动化手段有丰富的落地解决方案，适合作为替代标准化的运维操作性工作，即“面”的问题。AIOps目前仍处起步阶段，不是适合替代现有的自动化，而是应该用于解决自动化不能解决或解决成本很高的问题，即“点”的问题。</p><ul><li><strong>如何整合</strong>：</li></ul><p>AIOps并非是要取代现有的自动化运维体系，而是赋予现有体系智能。AIOps就要“学习，了解”自动化工具 ，并且更好的“使用”这些工具，这个过程就是深度集成，它的核心是对这些工具API的自主认知和自主使用。</p><p>虽然行业内的智能运维理念十分火热，但实际落地成效上还主要处于研究阶段。从<strong>运维工具</strong>技术解决方案的角度看，对于智能的解读也有差别，如果将智能的特点解读为具备”模拟人，具备自学习，能够从数据中获取知识，进而进行预测/决策“来判断是否智能，<strong>智能是自动化的一个辅助手段，自动化才是终态</strong>。建立在这个认识下，我们首先需要通过自动化手段解决痛点，提高工作效率，控制风险；利用运维数字化的建设为运维智能化提供数据、数据计算的能力；在自动化、数字化水平得到一定程度后，再通过人工智能的技术去解决自动化手段解决起来费力或无法解决的局部问题，让自动化具备智能的水平。</p><h2 id="体系"><a href="#体系" class="headerlink" title="体系"></a>体系</h2><h3 id="运维的可持续改进"><a href="#运维的可持续改进" class="headerlink" title="运维的可持续改进"></a><strong>运维的可持续改进</strong></h3><p>在管理领域，戴明推出的PDCA循环可以解释运维体系需要具备的可持续改进的能力条件。PDCA循环为四个阶段，即计划（plan）、执行（do）、检查（check）、调整（Action），即在实际工作开展过程中，把各项工作按照作出计划、计划实施、检查实施效果，然后将成功的纳入标准，并不断循环改进的过程。将这个思路引入到企业的运维体系中则是针对企业业务发展的需求，制定运维体系的整体发展目标，通过不断改进的措施提高运维工作效率、控制风险，以达到理高效、更优化的资源配置，进而推动业务的发展。要做到运维体系的可持续改进，需要做到以业务导向，整体部局；团队、流程、工具三位一体；不断审视优化。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/11.jpg" alt></p><p><strong>P：以业务导向、整体部局</strong></p><p>运维的最根本作用是保障IT数据的连续性，这里的IT数据包括业务，以及反映业务的数据，或者换句话可以表达为：网络不断、系统不瘫、数据不丢。随着业务对IT系统依赖程度越来越高，运维又会承担更高的期望，也就是运维向运营的转化，这就需要从业务角度去不断完善运维，以促进业务为大目标。有了这个目标，那我们的运维体系的构建就需要与企业业务的发展保持同步，要让运维体系具备可持续改进的能力。</p><p>另外，可持续改进的过程不应该是大拐弯的方式进行改进，而应该不断的小调整，这就需要确保首先要建立一个整体、全局的运维体系，对运维各项工作做一个整体的规划，把眼光看得更远，往往可以更好的把控当前。</p><p><strong>D：团队、流程、工具的三位一体</strong></p><p>可持续改进的运维体系需要让运维的团队、流程、工具三位一体的作用，比方说：提高工作效率，需要团队的专业化分工、流程的标准化、工具的自动化配合作用；推动业务的发展，既需精细化运维分析、业务服务、运营等维度的工作资源投入，也需要有工具的建设来减少操作性的工作来释放人力，需要工具提供更高效的数据来源。</p><p>这里说的团队主要是从运维人力资源的分工、团队建设、工作目标导向、运维KPI等；流程是指以成熟的运维方法论为主体，结合企业和外部监管的规章制度、企业业务发展需要，而落地的标准化工作方法；工具既包括狭义运维的“监、管、控”，也包括运营体系所需要数字化、智能化的工具平台。</p><p><strong>C+A : 不断审视优化</strong></p><p>在实际工作过程中，审视检查的过程很容易被忽略，但实际上最大的收获可能就来自于这个总结、归纳的过程中，这也是可持续改进的运维体系的关键所在。比方说，运维团队可以考虑在必要环节增加横向的优化团队；运维流程也需要定期对流程的落地进行分析，并对规章制度进行查漏补缺、删减不合理的流程规范、调整无法执行的规范要求；工具的建设要不断的分析工具的使用覆盖率，如何提高覆盖率，分析是否提高了运维的效率，还是带来了反作用等分析，并不断调整优化工具的建设。</p><h3 id="转型思路"><a href="#转型思路" class="headerlink" title="转型思路"></a>转型思路</h3><p>在提出可持续的运维体系前，我们先归纳一下运维团队常见的运维痛点，以提出运维转型的思路，再看看如何构建一个可持续改进的运维体系来支撑运维转型。前面的运维之痛中提到了 “救火”、“背锅”、“低价值”、”重复操作“等标签，我们归纳下己有特点再看转型：</p><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p><strong>被动救火式</strong>：以被动保障业务系统运行，日常计划性工作容易被打断、搁置；</p><p><strong>问题驱动式</strong>：以系统可用性、可靠性、业务请求等问题驱动运维工作；</p><p><strong>操作运维</strong>：重复性、操作类点主要工作量的运维模式；</p><p><strong>经验式运维</strong>：由人工经验驱动的运维模式，尤其是一些经验丰富的老员工的离职在短期内会对运维质量带来一定的冲击。</p><h4 id="转型"><a href="#转型" class="headerlink" title="转型"></a><strong>转型</strong></h4><p><strong>从被动救火式向主动精细化转型</strong>：专业化分工、主动分析，主动优化，驱动开发，促进DEVOPS的落地；</p><p><strong>从问题驱动向价值驱动转型</strong>：以企业业务发展目标为主线，业务体验、服务满意度、促进业务更好发展；</p><p><strong>从操作运维向运维开发转型</strong>：通过为运维人员提供运维开发平台，降低运维开发门槛，快速落地一些紧迫的运维工具，降低操作性、重复性的运维工作；</p><p><strong>从依靠经验向智能化驱动运维转型</strong>：结合数据分析、知识库、机器学习技术促进运维智能化。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/71d403c4/10.jpg" alt="图片"></p><h3 id="构建运维体系"><a href="#构建运维体系" class="headerlink" title="构建运维体系"></a><strong>构建运维体系</strong></h3><p>上面提到运维体系以业务导向，整体部局，团队、流程、工具三位一体，不断审视优化的建设思路，也提出了”主动精细化“、”价值驱动“、”运维开发“、”智能化运维“的转型目标，我们再将这些思路分解到团队、流程、工具的建设中，并归纳为：三大建设，十个文化的实践方法：</p><ul><li><strong>团队建设：专业化、精细化、运营化</strong></li></ul><p>我们将运维实施主体运维团队理解为团队，理想情况下，优秀的团队应该具备有合适的工作、合适的时间、合适的人、合适的行为四个要素组成。即团队要结合企业实际发展方向，制定符合企业、运维团队、个人发展的工作内容，并选择具备合适的知识、技能、认知、能力的人去完成工作，去实际个人的自我价值。</p><p>前面也提到，目前的运维织是一个被动保障业务系统运行，日常计划性工作容易被打断、搁置的工作，这种工作状态下的运维团队往往工作效率不高、容易出现操作风险。为了让运维团队具备可持续改进的能力，需要提高运维团队的工作效率，我们需要将运维工作专业化，整合通用性、操作性的工作，提高工作效率，在释放运维人员工作量后，引导运维人员有计划、可量化的去做更多分析类、优化类、业务运营的主动性工作。</p><ul><li><strong>流程建设：标准化、可视化、可量化</strong></li></ul><p>大部份运维团队会以内部企业积累的规章制度、外部监管机构的监管要求为基础，依照ITIL、ISO20000、ITSS.1、DevOps的方法论中的一个或多个组合的方式开展运维工作。这些规章制度、监管要求、方法论的整合、落地、持续改进的过程即为流程建设的过程。</p><p>流程建设首先需要标准化流程，要先梳理好己有的流程制度，约定工作的流转方式，再通过可视化将流程整合在日常工作中，最后通过流程落地数据的分析与工具建设，持续改善提高流程落地的效率，控制操作风险。</p><ul><li><strong>工具建设：自动化、数字化、智能化、服务化</strong></li></ul><p>工具的建设也以可持续改进的思路构建，以整合存量资源、引入成熟或开源技术为主，建立一体化的运维工具体系，通过体系化的思路实现运维工具（“监、管、控”）的互联互通，有序建设，实现自动化运维，全面控制风险、提高工作效率、释放人力；通过建立运维数据分析平台，实现数字化运营，提供运维数据集中与治理、主动分析的能力；在数字化运营的基础上通过运维数据挖掘、学习，优化运维或运营场景，向智能化发展；服务化则是以IT服务的方式将运维能力向处输出。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以前和一个项目经理沟通时，提到能否提前半天将变更申请提交过来，而这位项目经理很不理解的看着我问：“你们运维不就是在生产环境部署个程序嘛，这么简单的工作还需要提前半天？而且你们不懂程序，也评审不出什么吧？”。运维这么多年来，听到对运维工作抱有这种认识和看法的人很多。它侧面反映了企业中其他团队团队对运维的认识往往局限于一些简单操作性的工作。例如：应用服务故障时重启、应用配置变更、增删改查数据或者所有软硬件使用问题等等。运维真的是这样吗？&lt;/p&gt;
    
    </summary>
    
      <category term="心得体会" scheme="https://wandouduoduo.github.io/categories/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="Experiences" scheme="https://wandouduoduo.github.io/tags/Experiences/"/>
    
  </entry>
  
  <entry>
    <title>详解docker-compose安装sentry集群解决方案&lt;二&gt;</title>
    <link href="https://wandouduoduo.github.io/articles/312eda9c.html"/>
    <id>https://wandouduoduo.github.io/articles/312eda9c.html</id>
    <published>2020-12-29T09:32:11.000Z</published>
    <updated>2021-01-14T02:51:50.235Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>接上篇文章，上篇文章详细介绍了架构和单节点的搭建、配置和优化。本篇详细介绍集群方案。特别提醒没看上篇文章的请返回先看，本篇集群是在上篇文章的基础上配置的，并且上篇文章中的简单步骤，本篇不再说明，直接跳过。详解docker-compose安装sentry集群篇现在开始。</p><a id="more"></a><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>上篇文章中的3个节点，并都已执行完install.sh没有报错。</p><p><strong>主节点</strong>：192.168.1.100</p><p><strong>从节点</strong>：192.168.1.101，192.168.1.102</p><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><h3 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h3><h4 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a><strong>任务目标</strong></h4><p>Sentry平台三台服务器集群部署</p><h4 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a><strong>部署方式</strong></h4><p>采用docker平台，准确说应该是：docker-compose</p><h4 id="主节点"><a href="#主节点" class="headerlink" title="主节点"></a>主节点</h4><p>服务端模块：redis、postgres、memcached、stmp、sentry-web、sentry-worker和sentry-corn</p><p>功能如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">redis  <span class="comment">#支持消息队列和任务调度</span></span><br><span class="line">postgres <span class="comment">#数据存储</span></span><br><span class="line">memcached <span class="comment">#kv存储，用于worker数据管理</span></span><br><span class="line">stmp <span class="comment">#邮件服务</span></span><br><span class="line">sentry-cron <span class="comment">#实现定时任务，如定时群发邮件</span></span><br><span class="line">sentry-worker <span class="comment">#处理数据解析等任务，完成异常的分类、入库等操作</span></span><br><span class="line">sentry-web <span class="comment">#网络服务，后台网站和报错接口</span></span><br></pre></td></tr></table></figure><h4 id="从节点"><a href="#从节点" class="headerlink" title="从节点"></a>从节点</h4><p>剩下2台作为从节点，组件有：sentry-worker、sentry-web和memcached。从节点可以随负载变化动态扩容</p><h3 id="修改主节点配置"><a href="#修改主节点配置" class="headerlink" title="修改主节点配置"></a>修改主节点配置</h3><p>修改主节点配置文件docker-compose.yml，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">version: &apos;3.4&apos;</span><br><span class="line"></span><br><span class="line">x-defaults: &amp;defaults</span><br><span class="line">  restart: unless-stopped</span><br><span class="line">  build:</span><br><span class="line">    context: .</span><br><span class="line">  depends_on:</span><br><span class="line">    - redis</span><br><span class="line">    - postgres</span><br><span class="line">    - memcached</span><br><span class="line">    - smtp</span><br><span class="line">  env_file: .env</span><br><span class="line">  environment:</span><br><span class="line">    SENTRY_MEMCACHED_HOST: memcached</span><br><span class="line">    SENTRY_REDIS_HOST: redis</span><br><span class="line">    SENTRY_POSTGRES_HOST: postgres</span><br><span class="line">    SENTRY_DB_PASSWORD: postgres</span><br><span class="line">    SENTRY_EMAIL_HOST: smtp</span><br><span class="line">  volumes:</span><br><span class="line">    - sentry-data:/var/lib/sentry/files</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  smtp:</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    image: tianon/exim4</span><br><span class="line">    ports:</span><br><span class="line">      - &apos;192.168.1.100:25:25&apos;</span><br><span class="line"></span><br><span class="line">  memcached:</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    image: memcached:1.5-alpine</span><br><span class="line">    ports:</span><br><span class="line">      - &apos;192.168.1.100:11211:11211&apos;</span><br><span class="line"></span><br><span class="line">  redis:</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    image: redis:3.2-alpine</span><br><span class="line">    ports:</span><br><span class="line">      - &apos;192.168.1.100:6379:6379&apos;</span><br><span class="line"></span><br><span class="line">  postgres:</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    image: postgres:9.5</span><br><span class="line">    ports:</span><br><span class="line">      - &apos;192.168.1.100:5432:5432&apos;</span><br><span class="line">    volumes:</span><br><span class="line">      - sentry-postgres:/var/lib/postgresql/data</span><br><span class="line"></span><br><span class="line">  web:</span><br><span class="line">    &lt;&lt;: *defaults</span><br><span class="line">    ports:</span><br><span class="line">      - &apos;9000:9000&apos;</span><br><span class="line">  cron:</span><br><span class="line">    &lt;&lt;: *defaults</span><br><span class="line">    command: run cron</span><br><span class="line"></span><br><span class="line">  worker:</span><br><span class="line">    &lt;&lt;: *defaults</span><br><span class="line">    command: run worker</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">    sentry-data:</span><br><span class="line">      external: true</span><br><span class="line">    sentry-postgres:</span><br><span class="line">      external: true</span><br></pre></td></tr></table></figure><h3 id="修改从节点配置"><a href="#修改从节点配置" class="headerlink" title="修改从节点配置"></a>修改从节点配置</h3><p>修改从节点配置文件docker-compose.yml，配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">version: &apos;3.4&apos;</span><br><span class="line"></span><br><span class="line">x-defaults: &amp;defaults</span><br><span class="line">  restart: unless-stopped</span><br><span class="line">  build:</span><br><span class="line">    context: .</span><br><span class="line">  env_file: .env</span><br><span class="line">  environment:</span><br><span class="line">    SENTRY_MEMCACHED_HOST: memcached</span><br><span class="line">    SENTRY_REDIS_HOST: 192.168.1.100</span><br><span class="line">    SENTRY_POSTGRES_HOST: 192.168.1.100</span><br><span class="line">    SENTRY_DB_PASSWORD: postgres</span><br><span class="line">    SENTRY_EMAIL_HOST: 192.168.1.100</span><br><span class="line">  volumes:</span><br><span class="line">    - sentry-data:/var/lib/sentry/files</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    &lt;&lt;: *defaults</span><br><span class="line">    ports:</span><br><span class="line">      - &apos;9000:9000&apos;</span><br><span class="line">  memcached:</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    image: memcached:1.5-alpine</span><br><span class="line">  worker:</span><br><span class="line">    &lt;&lt;: *defaults</span><br><span class="line">    command: run worker -c 3</span><br><span class="line">volumes:</span><br><span class="line">    sentry-data:</span><br><span class="line">      external: true</span><br></pre></td></tr></table></figure><h3 id="统一存储"><a href="#统一存储" class="headerlink" title="统一存储"></a>统一存储</h3><p>集群分布式部署需要一个统一的存储服务，用于sourcemap等文件的存储。采用minio，部署在主节点。</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /opt/minio/&#123;data,config&#125;</span><br><span class="line"></span><br><span class="line">docker run -p 6000:9000 --name minio \</span><br><span class="line">    -d --restart=always \</span><br><span class="line">    -e <span class="string">"MINIO_ACCESS_KEY=wandouduoduo"</span> \</span><br><span class="line">    -e <span class="string">"MINIO_SECRET_KEY=wdddxxxx"</span> \</span><br><span class="line">    -v /opt/minio/data:/data \</span><br><span class="line">    -v /opt/minio/config:/root/.minio \</span><br><span class="line">    minio/minio server /data</span><br></pre></td></tr></table></figure><h4 id="创建bucket"><a href="#创建bucket" class="headerlink" title="创建bucket"></a>创建bucket</h4><p>访问minio页面</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/312eda9c/1.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/312eda9c/2.png" alt></p><p>主从节点的配置文件config.yml添加filestore信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">filestore.backend: &apos;s3&apos;</span><br><span class="line">filestore.options:</span><br><span class="line">  access_key: &apos;wandouduoduo&apos;</span><br><span class="line">  secret_key: &apos;wdddxxxx&apos;</span><br><span class="line">  bucket_name: &apos;sentry&apos;</span><br><span class="line">  endpoint_url: &apos;http://172.16.1.100:6000&apos;</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>启动服务时，可worker数量可根据机器核数进行配置。</p><h4 id="主节点启动"><a href="#主节点启动" class="headerlink" title="主节点启动"></a>主节点启动</h4><p>启动2个worker和1个web实例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d --scale worker=2</span><br></pre></td></tr></table></figure><h4 id="从节点启动"><a href="#从节点启动" class="headerlink" title="从节点启动"></a>从节点启动</h4><p>启动4个worker和一个web实例 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d --scale worker=4</span><br></pre></td></tr></table></figure><p><em>注意：以上配置需要根据服务器和流量的情况调整，以最大化利用机器资源。前端上传sourcemap文件较多时，worker耗费cpu资源会比较厉害</em></p><h3 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h3><p>然后负载均衡到所有节点的9000端口，访问即可，到这里集群搭建完成。</p><p><em>注意：这里做负载均衡时调用策略要用ip  hash方式。如用轮训方式，web页面输入密码，会轮训到下个节点登录不了。</em></p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="redis拆分"><a href="#redis拆分" class="headerlink" title="redis拆分"></a>redis拆分</h3><p>上报时的并发太大，可以考虑将redis单独拆分出来。拆分出来redis，自行搭建集群。</p><p>sentry的配置更改如下：</p><p>连接外部redis集群环境配置需添加下面几项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SENTRY_REDIS_HOST: xx.xx.xx.xx</span><br><span class="line">SENTRY_REDIS_PASSWORD: xxxxxxxxxxxx</span><br><span class="line">SENTRY_REDIS_PORT: xxx</span><br></pre></td></tr></table></figure><p>主节点docker-compose.yaml配置</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/312eda9c/3.png" alt></p><p>从节点docker-compose.yml配置</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/312eda9c/4.png" alt></p><h3 id="添加清理定时任务"><a href="#添加清理定时任务" class="headerlink" title="添加清理定时任务"></a>添加清理定时任务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -i onpremise-912_worker_1 sentry cleanup --days 60 &amp;&amp; docker <span class="built_in">exec</span> -i -u postgres onpremise-912_postgres_1 vacuumdb -U postgres -d postgres -v -f --analyze</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>sentry集群方案已全部搭建完成，当然该集群还可以横向扩展等等优化。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接上篇文章，上篇文章详细介绍了架构和单节点的搭建、配置和优化。本篇详细介绍集群方案。特别提醒没看上篇文章的请返回先看，本篇集群是在上篇文章的基础上配置的，并且上篇文章中的简单步骤，本篇不再说明，直接跳过。详解docker-compose安装sentry集群篇现在开始。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>详解docker-compose安装sentry集群解决方案&lt;一&gt;</title>
    <link href="https://wandouduoduo.github.io/articles/9e1f41fe.html"/>
    <id>https://wandouduoduo.github.io/articles/9e1f41fe.html</id>
    <published>2020-12-29T03:57:00.000Z</published>
    <updated>2020-12-29T21:20:09.998Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p><code>Sentry</code>是一个开源的实时错误报告和日志聚合工具平台。它专门监测错误并提取所有有用信息用于分析，不再麻烦地依赖用户反馈来定位问题。支持 web 前后端、移动应用以及游戏，支持 Python、OC、Java、Go、Node.js、Django、RoR 等主流编程语言和框架 ，还提供了 GitHub、Slack、Trello 等常见开发工具的集成。因sentry版本、安装方式和smtp要求的不同，新版本sentry不支持集群方案，本篇文章旨在用于sentry9.1.2版本+支持ssl/tls加密的smtp服务的单节点安装指南和集群解决方案。</p><a id="more"></a><h2 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h2><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/1.png" alt="在这里插入图片描述"></p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p><strong>版本</strong>: sentry 9.1.2<br><strong>安装方式</strong>: docker-compose<br><strong>系统环境</strong>: centos7，docker 17.05.0+、docker-Compose 1.17.0+<br><strong>集群节点</strong>：192.168.1.100、192.168.1.101、192.168.1.102</p><h2 id="集群方案"><a href="#集群方案" class="headerlink" title="集群方案"></a>集群方案</h2><p><strong>所有节点上都需要如下操作</strong></p><h3 id="拉取sentry配置"><a href="#拉取sentry配置" class="headerlink" title="拉取sentry配置"></a>拉取sentry配置</h3><p>sentry的配置可以在<a href="https://github.com/getsentry/onpremise/releases/tag/9.1.2" rel="noopener" target="_blank">官方github</a>里面下载release版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon ~]$ mkdir sentry &amp;&amp; cd sentry</span><br><span class="line">[root@bogon sentry]$ wget https://github.com/getsentry/onpremise/archive/9.1.2.tar.gz</span><br><span class="line">[root@bogon sentry]$ tar -zxvf 9.1.2.tar.gz </span><br><span class="line">[root@bogon sentry]$ tree</span><br><span class="line">.</span><br><span class="line">├── 9.1.2.tar.gz</span><br><span class="line">└── onpremise-9.1.2</span><br><span class="line">    ├── config.yml# 配置文件yaml，键值对yaml格式导入</span><br><span class="line">    ├── docker-compose.yml# docker-compose文件，用于构建镜像</span><br><span class="line">    ├── Dockerfile</span><br><span class="line">    ├── install.sh# 自动安装脚本</span><br><span class="line">    ├── LICENSE</span><br><span class="line">    ├── Makefile</span><br><span class="line">    ├── README.md</span><br><span class="line">    ├── requirements.txt# 依赖包声明</span><br><span class="line">    ├── sentry.conf.py# 配置文件python，通过python程序导入</span><br><span class="line">    └── test.sh</span><br><span class="line"></span><br><span class="line">1 directory, 11 files</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>这里主要修改邮件模块配置。但有个小问题，需要区分你的邮件服务器是用普通smtp 25端口还是ssl/tls的465/587端口。</p><p>以下这个gmail表格解释的比较清楚，以便使用正确的信息更新您的客户端</p><table><thead><tr><th>项目</th><th>Value</th></tr></thead><tbody><tr><td>接收邮件 (IMAP) 服务器</td><td>imap.gmail.com 要求 SSL：是 端口：993</td></tr><tr><td>发送邮件 (SMTP) 服务器</td><td>smtp.gmail.com 要求 SSL：是 要求 TLS：是（如适用） 使用身份验证：是 SSL 端口：465 TLS/STARTTLS 端口：587</td></tr><tr><td>完整名称或显示名称</td><td>您的姓名</td></tr><tr><td>帐号名、用户名或电子邮件地址</td><td>您的完整电子邮件地址</td></tr><tr><td>密码</td><td>您的 Gmail 密码</td></tr></tbody></table><p>如果你的smtp服务器支持ssl/tls，或者只支持这两种方式（可以提前telnet 服务器和端口验证），那么需要改动几个地方，下面以163邮箱为例：</p><p><strong>config.yml文件</strong></p><p>邮箱服务器支持ssl安全的，配置如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@VM_0_5_centos onpremise-9.1.2]# cat config.yml </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##############</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Mail Server <span class="comment">#</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##############</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">mail.backend: <span class="string">'smtp'</span>  <span class="comment"># Use dummy if you want to disable email entirely</span></span></span><br><span class="line">mail.host: 'smtp.163.com'</span><br><span class="line">mail.port: 465</span><br><span class="line">mail.username: 'wandouduoduo@163.com'</span><br><span class="line">mail.password: '************'</span><br><span class="line">mail.use-tls: true</span><br><span class="line">mail.from: 'wandouduoduo@163.com'</span><br></pre></td></tr></table></figure><p>若邮件smtp服务器只支持普通的25端口，配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@VM_0_5_centos</span> <span class="string">onpremise-9.1.2]#</span> <span class="string">cat</span> <span class="string">config.yml</span> </span><br><span class="line"><span class="comment">###############</span></span><br><span class="line"><span class="comment"># Mail Server #</span></span><br><span class="line"><span class="comment">###############</span></span><br><span class="line"></span><br><span class="line"><span class="string">mail.backend:</span> <span class="string">'smtp'</span>  <span class="comment"># Use dummy if you want to disable email entirely</span></span><br><span class="line"><span class="string">mail.host:</span> <span class="string">'smtp.xxxx.com'</span></span><br><span class="line"><span class="string">mail.port:</span> <span class="number">25</span></span><br><span class="line"><span class="string">mail.username:</span> <span class="string">'wandouduoduo@xxxx.com'</span></span><br><span class="line"><span class="string">mail.password:</span> <span class="string">'**********'</span></span><br><span class="line"><span class="string">mail.use-tls:</span> <span class="literal">false</span></span><br><span class="line"><span class="string">mail.from:</span> <span class="string">'root@localhost'</span></span><br></pre></td></tr></table></figure><p><strong>修改requirements.txt文件</strong></p><p>通过pip安装上一步配置里面用到的<code>django_stmp_ssl</code>模块，在requirements.txt里面的模块会在所有相关容器里面自动安装，如邮件为普通25端口，省略此步：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@VM_0_5_centos onpremise-9.1.2]# cat requirements.txt </span><br><span class="line"><span class="meta">#</span><span class="bash"> Add plugins here</span></span><br><span class="line">django-smtp-ssl==1.0</span><br></pre></td></tr></table></figure><p><strong>修改sentry.conf.py文件</strong></p><p>在头部插入如下两行代码，如邮件为普通25端口，省略此步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">socket.setdefaulttimeout(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><p>主要是为了解决邮件模块socket超时的问题，默认的socket超时时间是5s。否则很容易报错，特别是邮件用ssl协议的，如下图：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/2.png" alt="在这里插入图片描述"></p><p><strong>修改docker-compose.yml文件</strong></p><p>主要修改配置邮件和pg数据库相关的环境变量：<br>（1）修改：<code>x-defaults.environment.SENTRY_EMAIL_HOST: smtp.163.com</code><br>（2）新增：<code>x-defaults.environment.SENTRY_DB_PASSWORD: postgres</code><br>（3）新增：<code>services.postgres.environment.POSTGRES_PASSWORD=postgres</code></p><p>完整配置如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> This docker-compose.yml is meant to be just an example of how</span></span><br><span class="line"><span class="comment"># you could accomplish this on your own. It is not intended to work in</span></span><br><span class="line"><span class="comment"># all use-cases and must be adapted to fit your needs. This is merely</span></span><br><span class="line"><span class="comment"># a guideline.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See docs.getsentry.com/on-premise/server/ for full</span></span><br><span class="line"><span class="comment"># instructions</span></span><br><span class="line"></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3.4'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">x-defaults:</span> <span class="meta">&amp;defaults</span></span><br><span class="line"><span class="attr">  restart:</span> <span class="string">unless-stopped</span></span><br><span class="line"><span class="attr">  build:</span></span><br><span class="line"><span class="attr">    context:</span> <span class="string">.</span></span><br><span class="line"><span class="attr">  depends_on:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">redis</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">postgres</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">memcached</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">smtp</span></span><br><span class="line"><span class="attr">  env_file:</span> <span class="string">.env</span></span><br><span class="line"><span class="attr">  environment:</span></span><br><span class="line"><span class="attr">    SENTRY_MEMCACHED_HOST:</span> <span class="string">memcached</span></span><br><span class="line"><span class="attr">    SENTRY_REDIS_HOST:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">    SENTRY_POSTGRES_HOST:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">    SENTRY_DB_PASSWORD:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">    SENTRY_EMAIL_HOST:</span> <span class="string">smtp.126.com</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">    - sentry-data:</span><span class="string">/var/lib/sentry/files</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  smtp:</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">unless-stopped</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">tianon/exim4</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  memcached:</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">unless-stopped</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">memcached:1.5-alpine</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  redis:</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">unless-stopped</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">redis:3.2-alpine</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  postgres:</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">unless-stopped</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">postgres:9.5</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">POSTGRES_PASSWORD=postgres</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">      - sentry-postgres:</span><span class="string">/var/lib/postgresql/data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  web:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">'9000:9000'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  cron:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">run</span> <span class="string">cron</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  worker:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="meta">*defaults</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">run</span> <span class="string">worker</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="attr">    sentry-data:</span></span><br><span class="line"><span class="attr">      external:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    sentry-postgres:</span></span><br><span class="line"><span class="attr">      external:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="自动安装并启动"><a href="#自动安装并启动" class="headerlink" title="自动安装并启动"></a>自动安装并启动</h3><h4 id="准备并构建镜像"><a href="#准备并构建镜像" class="headerlink" title="准备并构建镜像"></a>准备并构建镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 准备构建镜像</span></span><br><span class="line">./install.sh</span><br></pre></td></tr></table></figure><p>构建过程中在终端里需要创建用户账号，输入正确的邮箱和密码，设置为超级管理员即可。如下所示：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/3.png" alt="在这里插入图片描述"></p><p>如如果没有弹出来让你填写或者填错了取消也都没关系，在intall.sh脚本跑完之后可以单独创建用户，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose run --rm web createuser</span><br></pre></td></tr></table></figure><h4 id="启动应用"><a href="#启动应用" class="headerlink" title="启动应用"></a>启动应用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动</span></span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>然后通过docker命令可以看到容器都起来了，如开头的框架图，sentry服务默认用到了7个容器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@VM_0_5_centos onpremise-9.1.2]# docker ps |grep onpremise</span><br><span class="line">8ec4557197f4        onpremise-912_worker                                "/entrypoint.sh run …"   14 hours ago        Up 14 hours            9000/tcp                    onpremise-912_worker_1</span><br><span class="line">9c173154c5a6        onpremise-912_web                                   "/entrypoint.sh run …"   14 hours ago        Up 14 hours            0.0.0.0:9000-&gt;9000/tcp      onpremise-912_web_1</span><br><span class="line">0b513728479b        onpremise-912_cron                                  "/entrypoint.sh run …"   14 hours ago        Up 14 hours            9000/tcp                    onpremise-912_cron_1</span><br><span class="line">e4e98dbaf0e2        postgres:9.5                                        "docker-entrypoint.s…"   23 hours ago        Up 22 hours            5432/tcp                    onpremise-912_postgres_1</span><br><span class="line">dc4281107dad        tianon/exim4                                        "docker-entrypoint.s…"   23 hours ago        Up 22 hours            25/tcp                      onpremise-912_smtp_1</span><br><span class="line">c888e6567f55        redis:3.2-alpine                                    "docker-entrypoint.s…"   23 hours ago        Up 22 hours            6379/tcp                    onpremise-912_redis_1</span><br><span class="line">d4a3f95533ba        memcached:1.5-alpine                                "docker-entrypoint.s…"   23 hours ago        Up 22 hours            11211/tcp                   onpremise-912_memcached_1</span><br></pre></td></tr></table></figure><p>这几个容器应用的作用：</p><table><thead><tr><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>cron</td><td>定时任务，使用的是celery-beat</td></tr><tr><td>memcached</td><td>memcached</td></tr><tr><td>postgres</td><td>pgsql数据库</td></tr><tr><td>redis</td><td>运行celery需要的服务</td></tr><tr><td>smtp</td><td>邮件服务</td></tr><tr><td>web</td><td>使用django+drf写的一套Sentry Web界面</td></tr><tr><td>worker</td><td>celery的worker服务，用来跑异步任务的</td></tr></tbody></table><p><strong>离线安装</strong></p><p>上述都是在线安装的，涉及到的所有docker image都是从docker hub registry当中远程拉取的。如果是本地局域网或离线安装，需要哪些镜像呢？</p><p>在docker-compose.yml和docker-compose build执行中找到答案。其中的sentry镜像经过docker-compose build会基于它构建出三个镜像，分别是：web、cron和worker。从执行结果中可知，最终就得到了sentry系统7个容器镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> [root@VM_0_5_centos onpremise-9.1.2]# docker-compose build</span><br><span class="line"> smtp uses an image, skipping</span><br><span class="line"> memcached uses an image, skipping</span><br><span class="line"> redis uses an image, skipping</span><br><span class="line"> postgres uses an image, skipping</span><br><span class="line"> Building web</span><br><span class="line"> Step 1/2 : ARG SENTRY_IMAGE</span><br><span class="line"> Step 2/2 : FROM $&#123;SENTRY_IMAGE:-sentry:9.1.2&#125;-onbuild</span><br><span class="line"> Successfully built 1ce74ed80f84</span><br><span class="line"> Successfully tagged onpremise-912_web:latest</span><br><span class="line"> Building cron</span><br><span class="line"> Step 1/2 : ARG SENTRY_IMAGE</span><br><span class="line"> Step 2/2 : FROM $&#123;SENTRY_IMAGE:-sentry:9.1.2&#125;-onbuild</span><br><span class="line"> Successfully built 1ce74ed80f84</span><br><span class="line"> Successfully tagged onpremise-912_cron:latest</span><br><span class="line"> Building worker</span><br><span class="line"> Step 1/2 : ARG SENTRY_IMAGE</span><br><span class="line"> Step 2/2 : FROM $&#123;SENTRY_IMAGE:-sentry:9.1.2&#125;-onbuild</span><br><span class="line"> Successfully built 1ce74ed80f84</span><br><span class="line"> Successfully tagged onpremise-912_worker:latest</span><br><span class="line">1234567891011121314151617181920</span><br></pre></td></tr></table></figure><p>所以，离线安装只需要将上述5个容器镜像提前下载下来并导入到内网即可：</p><ul><li>sentry:9.1.2-onbuild</li><li>redis:3.2-alpine</li><li>postgres:9.5</li><li>tianon/exim4</li><li>memcached:1.5-alpine</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker save -o xxx.tar xxxx:latest</span><br><span class="line">docker load -i xxx.tar</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><h3 id="页面访问验证"><a href="#页面访问验证" class="headerlink" title="页面访问验证"></a>页面访问验证</h3><p>浏览器中输入<a href="http://xxxx:9000访问，输入在install时创建的超级管理员账号密码登陆即可。" rel="noopener" target="_blank">http://xxxx:9000访问，输入在install时创建的超级管理员账号密码登陆即可。</a><br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/4.png" alt="在这里插入图片描述"></p><p><strong>调整了配置要重启应用</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker-compose build# 重新构建镜像</span><br><span class="line">docker-compose run --rm web upgrade# 同步数据</span><br><span class="line">docker-compose up -d        # 重新启动容器</span><br></pre></td></tr></table></figure><h3 id="验证邮箱"><a href="#验证邮箱" class="headerlink" title="验证邮箱"></a>验证邮箱</h3><p>在web界面上，用户下拉，管理–&gt;Email–&gt;测试发送邮件<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/6.png" alt="在这里插入图片描述"><br>然后在邮箱中即可收到验证测试邮件。</p><h3 id="配置SDK并验证异常捕获及通知"><a href="#配置SDK并验证异常捕获及通知" class="headerlink" title="配置SDK并验证异常捕获及通知"></a>配置SDK并验证异常捕获及通知</h3><p>创建一个python的除0异常测试项目来验证，比较简单。页面中会给出了使用提示。即在项目运行环境中安装sdk，然后在项目代码里面插入模块即可。<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/7.png" alt="在这里插入图片描述"><br>运行python程序，页面可以看到这个错误并收到了邮件通知。大功告成！！！<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/8.png" alt="在这里插入图片描述"><br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/9.png" alt="在这里插入图片描述"></p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="调整语言"><a href="#调整语言" class="headerlink" title="调整语言"></a>调整语言</h3><p>默认为英文，这里改为中文。在界面左上角Sentry账户那下拉选择User Settings，然后再Account Detail里面选择language为Simplified Chinese，看到提示成功后刷新页面即可。<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/9e1f41fe/5.png" alt="在这里插入图片描述"></p><h3 id="集成其它模块示例"><a href="#集成其它模块示例" class="headerlink" title="集成其它模块示例"></a>集成其它模块示例</h3><p>如果要集成其它模块，如钉钉通知，只需要两步：</p><h4 id="在requirements-txt里面添加模块"><a href="#在requirements-txt里面添加模块" class="headerlink" title="在requirements.txt里面添加模块"></a>在requirements.txt里面添加模块</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Add plugins here</span></span><br><span class="line">django-smtp-ssl~=1.0  # 发邮件支持SSL协议</span><br><span class="line">sentry-dingding~=0.0.2  # 钉钉通知插件</span><br><span class="line">redis-py-cluster==1.3.4  # redis-cluster连接</span><br></pre></td></tr></table></figure><h4 id="配置通知"><a href="#配置通知" class="headerlink" title="配置通知"></a>配置通知</h4><p>页面中任意选择一个项目 –&gt; 设置 –&gt; 点击Legacy Integrations –&gt; 搜索到DingDing开启并配置钉钉机器人Access Token即可完成。</p><h3 id="清理历史数据"><a href="#清理历史数据" class="headerlink" title="清理历史数据"></a>清理历史数据</h3><h4 id="只保留60天数据"><a href="#只保留60天数据" class="headerlink" title="只保留60天数据"></a>只保留60天数据</h4><p>命令cleanup删除postgresql数据，但postgrdsql对于delete, update等操作，只是将对应行标志为DEAD，属于”软删除“，并没有真正释放磁盘空间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -it sentry_worker_1 bash</span><br><span class="line">$ sentry cleanup  --days 60</span><br></pre></td></tr></table></figure><h4 id="postgres数据清理"><a href="#postgres数据清理" class="headerlink" title="postgres数据清理"></a>postgres数据清理</h4><p>清理完后会彻底删除数据，并释放磁盘空间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -it sentry_postgres_1 bash</span><br><span class="line">$ vacuumdb -U postgres -d postgres -v -f --analyze</span><br></pre></td></tr></table></figure><h4 id="定时清理脚本"><a href="#定时清理脚本" class="headerlink" title="定时清理脚本"></a>定时清理脚本</h4><p>直接设定定时执行上述两步，避免每次要手动清理或时间太久难以清理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line">docker <span class="built_in">exec</span> -i sentry_worker_1 sentry cleanup --days 60 &amp;&amp; docker <span class="built_in">exec</span> -i -u postgres sentry_postgres_1 vacuumdb -U postgres -d postgres -v -f --analyze</span><br></pre></td></tr></table></figure><h3 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h3><p>Sentry有非常好的数据迁移的设计，升级Sentry非常方便。每次使用pip更新Sentry包之后执行升级命令”sentry upgrade”即可。对应到docker-compose的方式，原则上我们只需要dockerfile里面更新sentry的docker镜像的版本号（如果是离线版本则要提前下载新镜像），然后按照上面提到的命令重启容器即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker-compose build# 重新构建镜像</span><br><span class="line">docker-compose run --rm web upgrade# 同步数据</span><br><span class="line">docker-compose up -d        # 重新启动容器</span><br></pre></td></tr></table></figure><h2 id="排错"><a href="#排错" class="headerlink" title="排错"></a>排错</h2><p>如果报错没有组织团队，需要进入到db里面查看，可能是数据库没有初始化：</p><h3 id="进入postgres数据库容器"><a href="#进入postgres数据库容器" class="headerlink" title="进入postgres数据库容器"></a>进入postgres数据库容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it onpremise_postgres_1 bash</span><br></pre></td></tr></table></figure><h3 id="进入postgres数据库"><a href="#进入postgres数据库" class="headerlink" title="进入postgres数据库"></a>进入postgres数据库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psql -h 127.0.0.1 -d postgres -U postgres</span><br></pre></td></tr></table></figure><h3 id="查看数据表"><a href="#查看数据表" class="headerlink" title="查看数据表"></a>查看数据表</h3><p>查看sentry_project，sentry_organization两个表是否有数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">postgres=# select * from sentry_project;</span><br><span class="line">postgres=# select * from sentry_organization ;</span><br></pre></td></tr></table></figure><p>如果确认是没有数据，进行创建。</p><h3 id="打开shell"><a href="#打开shell" class="headerlink" title="打开shell"></a>打开shell</h3><p>进入sentry的web的shell里面，其实就是一个python端：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose run --rm web shell</span><br></pre></td></tr></table></figure><h3 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentry.models <span class="keyword">import</span> Project</span><br><span class="line"><span class="keyword">from</span> sentry.receivers.core <span class="keyword">import</span> create_default_projects</span><br><span class="line">create_default_projects([Project])</span><br></pre></td></tr></table></figure><h3 id="退出shell创建用户"><a href="#退出shell创建用户" class="headerlink" title="退出shell创建用户"></a>退出shell创建用户</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose run --rm web createuser</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文详细介绍了单节点sentry9.1.2容器化搭建过程，并详解了邮件和其他集成的组件的优化等等。但限于篇幅问题，本文就写到这里，下一篇详细介绍集群方案。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Sentry&lt;/code&gt;是一个开源的实时错误报告和日志聚合工具平台。它专门监测错误并提取所有有用信息用于分析，不再麻烦地依赖用户反馈来定位问题。支持 web 前后端、移动应用以及游戏，支持 Python、OC、Java、Go、Node.js、Django、RoR 等主流编程语言和框架 ，还提供了 GitHub、Slack、Trello 等常见开发工具的集成。因sentry版本、安装方式和smtp要求的不同，新版本sentry不支持集群方案，本篇文章旨在用于sentry9.1.2版本+支持ssl/tls加密的smtp服务的单节点安装指南和集群解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="运维技术" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"/>
    
      <category term="服务部署" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2/"/>
    
    
      <category term="Sentry" scheme="https://wandouduoduo.github.io/tags/Sentry/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7安装字体</title>
    <link href="https://wandouduoduo.github.io/articles/dfe24446.html"/>
    <id>https://wandouduoduo.github.io/articles/dfe24446.html</id>
    <published>2020-12-29T02:57:34.000Z</published>
    <updated>2020-12-29T03:42:11.845Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Linux字体确实是个问题。场景一：验证码已成为了用户认证的标配，动态变动可以有效防止注入，提高用户认证的安全。场景二：开发的系统中依赖系统的字体，如报表系统中发现中文乱码或中文字体重叠的情况等问题，这些都是linux字体惹的祸。本文以安装中文字体为例帮你解决字体这一小问题。</p><a id="more"></a><h2 id="查看字体"><a href="#查看字体" class="headerlink" title="查看字体"></a>查看字体</h2><p>首先考虑的就是操作系统是否有中文字体，在<a href="http://www.linuxidc.com/topicnews.aspx?tid=14" rel="noopener" target="_blank">CentOS</a> 7中发现输入命令查看字体列表是提示命令无效：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/1.png" alt="这里写图片描述"><br>如上图可以看出，不仅没有中文字体，连字体库都没有呢。</p><h2 id="安装字体库"><a href="#安装字体库" class="headerlink" title="安装字体库"></a>安装字体库</h2><p>从CentOS 4.x开始就用fontconfig来管理系统字体，如没有字体库安装即可，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install fontconfig</span><br></pre></td></tr></table></figure><p>当看到下图的提示信息时说明已安装成功：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/2.png" alt="这里写图片描述"></p><p>这时在/usr/share目录就可以看到fonts和fontconfig目录（之前没有）：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/3.png" alt="这里写图片描述"></p><p>字体库已安装完成。</p><h2 id="添加中文字体"><a href="#添加中文字体" class="headerlink" title="添加中文字体"></a>添加中文字体</h2><p>在CentOS中，字体库的存放位置正是上图中看到的fonts目录（/usr/share/fonts）中，我们首先要做的就是找到中文字体文件放到该目录下，而中文字体文件在windows系统中就可以找到，打开c盘下的Windows/Fonts目录：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/4.png" alt="这里写图片描述"></p><p>如上图，我们只需将需要的字体拷贝出来并上传至linux服务器即可。</p><p>这里选择宋体和黑体（报表中用到了这两种字体），可以看到是两个后缀名为ttf和ttc的文件：<img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/5.png" alt="这里写图片描述"><br>先新建目录，首先在/usr/shared/fonts目录下新建一个目录chinese：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/6.png" alt="这里写图片描述"></p><p>然后将上面的两个字体上传至/usr/shared/fonts/chinese目录下即可：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/7.png" alt="这里写图片描述"></p><p>修改chinese目录的权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/share/fonts/chinese</span><br></pre></td></tr></table></figure><p>安装ttmkfdir来搜索目录中所有的字体信息，并汇总生成fonts.scale文件，输入命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ttmkfdir</span><br></pre></td></tr></table></figure><p>当看到下图的提示信息时说明已安装成功：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/8.png" alt="这里写图片描述"></p><p>执行ttmkfdir命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ttmkfdir -e /usr/share/X11/fonts/encodings/encodings.dir</span><br></pre></td></tr></table></figure><p>修改字体配置文件，首先通过编辑器打开配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/fonts/fonts.conf</span><br></pre></td></tr></table></figure><p>在Font list标签，即字体列表下，需要添加的中文字体位置加进去：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/9.png" alt="这里写图片描述"><br>刷新内存中的字体缓存或reboot重启：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">fc</span>-cache</span><br></pre></td></tr></table></figure><p><strong>校验</strong></p><p>这样中文字体已安装完成，最后再次通过fc-list看一下字体列表：<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/dfe24446/10.png" alt="这里写图片描述"></p><p>可以看到已经成功安装上了中文字体，至此安装过程就全部结束。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Linux中的字体问题不是大问题，但为了系统的功能和易读性也需要按照上面教程切实解决下。安装其他字体和上面教程类似。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux字体确实是个问题。场景一：验证码已成为了用户认证的标配，动态变动可以有效防止注入，提高用户认证的安全。场景二：开发的系统中依赖系统的字体，如报表系统中发现中文乱码或中文字体重叠的情况等问题，这些都是linux字体惹的祸。本文以安装中文字体为例帮你解决字体这一小问题。&lt;/p&gt;
    
    </summary>
    
      <category term="操作系统" scheme="https://wandouduoduo.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="Linux" scheme="https://wandouduoduo.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux/"/>
    
    
      <category term="Linux" scheme="https://wandouduoduo.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>非root的普通用户使用tmux方法教程</title>
    <link href="https://wandouduoduo.github.io/articles/4dd98c85.html"/>
    <id>https://wandouduoduo.github.io/articles/4dd98c85.html</id>
    <published>2020-12-24T07:00:29.000Z</published>
    <updated>2020-12-24T07:35:58.108Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p><code>Tmux</code>是一款终端复用的神器，实现了会话与终端窗后的分离。用过的同学都欲罢不能，但是使用它时必须root用户，如果是普通用户就会出现<code>can&#39;t create socket</code>错误不能使用，那么本教程就教你用普通用户使用tmux这个神器。</p><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h2><h3 id="root权限"><a href="#root权限" class="headerlink" title="root权限"></a>root权限</h3><p>若有root权限，tmux 安装十分简单，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install tmux      <span class="comment">#ubuntu</span></span><br><span class="line">yum install tmux -y  <span class="comment">#centos</span></span><br></pre></td></tr></table></figure><p>然后即可使用</p><h3 id="普通用户"><a href="#普通用户" class="headerlink" title="普通用户"></a>普通用户</h3><p>若你没有root权限，则就需要下载源码安装了。由于Tmux的安装依赖libevent以及ncurses，这两个库要先安装。</p><p>安装目录限定为：<code>/home/username/.local</code></p><p><strong>安装libevent</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/libevent/libevent/releases/download/release-2.0.22-stable/libevent-2.0.22-stable.tar.gz</span><br><span class="line">tar -xzvf libevent-2.0.22-stable.tar.gz</span><br><span class="line"><span class="built_in">cd</span> libevent-2.0.22-stable</span><br><span class="line">./configure --prefix=<span class="variable">$HOME</span>/.<span class="built_in">local</span> --<span class="built_in">disable</span>-shared</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p><strong>安装ncurses</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget http://ftp.gnu.org/gnu/ncurses/ncurses-6.0.tar.gz</span><br><span class="line">tar -xzvf ncurses-6.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> ncurses-6.0</span><br><span class="line">./configure --prefix=<span class="variable">$HOME</span>/.<span class="built_in">local</span></span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p><strong>安装tmux</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/tmux/tmux/releases/download/2.8/tmux-2.8.tar.gz</span><br><span class="line">tar -xzvf tmux-2.8.tar.gz</span><br><span class="line"><span class="built_in">cd</span> tmux-2.8</span><br><span class="line">./configure CFLAGS=<span class="string">"-I<span class="variable">$HOME</span>/.local/include"</span> LDFLAGS=<span class="string">"-L<span class="variable">$HOME</span>/.local/lib"</span> </span><br><span class="line">make</span><br><span class="line">cp tmux ~/.<span class="built_in">local</span>/bin</span><br></pre></td></tr></table></figure><p><strong>添加环境变量</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:~/.<span class="built_in">local</span>/bin      <span class="comment">#将该行添加到.bashrc 中</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/wandouduoduo/.tmux.git</span><br><span class="line">ln -s -f .tmux/.tmux.conf</span><br><span class="line">cp .tmux/.tmux.conf.local .</span><br></pre></td></tr></table></figure><p>祝你好运，享受并使用它吧！！</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Tmux&lt;/code&gt;是一款终端复用的神器，实现了会话与终端窗后的分离。用过的同学都欲罢不能，但是使用它时必须root用户，如果是普通用户就会出现&lt;code&gt;can&amp;#39;t create socket&lt;/code&gt;错误不能使用，那么本教程就教你用普通用户使用tmux这个神器。&lt;/p&gt;
    
    </summary>
    
      <category term="运维技术" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/"/>
    
      <category term="命令详解" scheme="https://wandouduoduo.github.io/categories/%E8%BF%90%E7%BB%B4%E6%8A%80%E6%9C%AF/%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"/>
    
    
      <category term="Linux" scheme="https://wandouduoduo.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>centos7时间自动同步</title>
    <link href="https://wandouduoduo.github.io/articles/6bab7f2f.html"/>
    <id>https://wandouduoduo.github.io/articles/6bab7f2f.html</id>
    <published>2020-12-22T07:50:28.000Z</published>
    <updated>2020-12-22T08:02:11.784Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>运维的工作高可用高并发，负载均衡，可靠稳定等等要求，很多系统都是集群模式并采用分布式部署。但是有时系统时间不同步，集群服务器之间时间不同，就会造成一些困扰或服务根本就启动不起来。所以时间自动同步，是很有必要的。</p><a id="more"></a><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><h3 id="修改时区"><a href="#修改时区" class="headerlink" title="修改时区"></a>修改时区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /etc/localtime</span><br><span class="line">ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"></span><br><span class="line">vim /etc/sysconfig/clock</span><br><span class="line">ZONE= &quot;Asia/Shanghai&quot;</span><br><span class="line">UTC= false</span><br><span class="line">ARC= false</span><br></pre></td></tr></table></figure><h3 id="安装并设置开机自启"><a href="#安装并设置开机自启" class="headerlink" title="安装并设置开机自启"></a>安装并设置开机自启</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ntp</span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl enable ntpd</span><br></pre></td></tr></table></figure><h3 id="配置开机启动"><a href="#配置开机启动" class="headerlink" title="配置开机启动"></a>配置开机启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/rc.d/rc.local</span><br><span class="line">/usr/sbin/ntpdate ntp1.aliyun.com &gt; /dev/ null  2 &gt;&amp; 1 ; /sbin/hwclock -w</span><br></pre></td></tr></table></figure><h3 id="配置定时任务"><a href="#配置定时任务" class="headerlink" title="配置定时任务"></a>配置定时任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line">0  */ 1  * * * ntpdate ntp1.aliyun.com &gt; /dev/ null  2 &gt;&amp; 1 ; /sbin/hwclock -w</span><br></pre></td></tr></table></figure><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">rm -rf /etc/localtime</span><br><span class="line">ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"></span><br><span class="line">cat &gt;/etc/sysconfig/clock &lt;&lt;EOF</span><br><span class="line">ZONE= <span class="string">"Asia/Shanghai"</span></span><br><span class="line">UTC= <span class="literal">false</span></span><br><span class="line">ARC= <span class="literal">false</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y ntp &amp;&amp; systemctl start ntpd &amp;&amp; systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"/usr/sbin/ntpdate ntp1.aliyun.com &gt; /dev/ null  2 &gt;&amp; 1 ; /sbin/hwclock -w"</span> &gt;&gt;/etc/rc.d/rc.local</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"0 */1  * * * ntpdate ntp1.aliyun.com &gt; /dev/ null  2 &gt;&amp; 1 ; /sbin/hwclock -w"</span> &gt;&gt; /var/spool/cron/root</span><br></pre></td></tr></table></figure></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;运维的工作高可用高并发，负载均衡，可靠稳定等等要求，很多系统都是集群模式并采用分布式部署。但是有时系统时间不同步，集群服务器之间时间不同，就会造成一些困扰或服务根本就启动不起来。所以时间自动同步，是很有必要的。&lt;/p&gt;
    
    </summary>
    
      <category term="操作系统" scheme="https://wandouduoduo.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="Linux" scheme="https://wandouduoduo.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux/"/>
    
    
      <category term="Linux" scheme="https://wandouduoduo.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Zimbra邮件服务器证书部署教程</title>
    <link href="https://wandouduoduo.github.io/articles/b778abf7.html"/>
    <id>https://wandouduoduo.github.io/articles/b778abf7.html</id>
    <published>2020-12-21T05:50:05.000Z</published>
    <updated>2020-12-21T06:08:38.547Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>zimbra邮件服务搭建完成后，为了安全和用户体验，通常用域名解析和证书配置。本文详细介绍zimbra服务的证书配置，当然证书过期替换也是一样的操作方法。</p><a id="more"></a><p>此文档采用命令行的形式配置证书。</p><h2 id="获取SSL证书"><a href="#获取SSL证书" class="headerlink" title="获取SSL证书"></a><strong>获取SSL证书</strong></h2><p>从沃通申请SSL证书后，将会下载一个以域名命名的.zip压缩包，解压该压缩包，会得到for Apache.zip、for Nginx.zip、for IIS.zip、for other server.zip，Zimbra将会用到for other server.zip里面的四个.crt文件(test.wosign.com为测试证书域名)以及自主生成的私钥.key文件（申请证书过程创建CSR时生成）。<br><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b778abf7/1.png" alt="Zimbra邮件服务器证书部署教程"></p><h2 id="证书合成以及重命名"><a href="#证书合成以及重命名" class="headerlink" title="证书合成以及重命名**"></a>证书合成以及重命名**</h2><h3 id="Windows环境："><a href="#Windows环境：" class="headerlink" title="Windows环境："></a>Windows环境：</h3><p>在windows环境下，可以用记事本或写字板打开.crt文件，将issuer.crt、cross.crt、root.crt按顺序合成后保存，并将合成后的.crt文件重命名为commercial_ca.crt。<br>将自主生成的.key文件重命名为commercial.key。<br>将test.wosign.com.crt重命名为commercial.crt。</p><h3 id="Linux环境"><a href="#Linux环境" class="headerlink" title="Linux环境:"></a>Linux环境:</h3><p>在Linux服务器上，用命令合成证书文件，具体命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat issuer.crt cross.crt root.crt &gt; commercial_ca.crt</span><br><span class="line">Mv test.wosign.com.crt commercial.crt</span><br><span class="line">Mv yourdomain.com.key commercial.key</span><br></pre></td></tr></table></figure><h2 id="证书安装"><a href="#证书安装" class="headerlink" title="证书安装**"></a>证书安装**</h2><h3 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h3><p>将重命名后的三个文件上传至/opt/zimbra/ssl/zimbra/commercial目录。</p><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>用命令验证证书文件是否匹配，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/zimbra/bin/zmcertmgr verifycrt comm /opt/zimbra/ssl/zimbra/commercial/commercial.key /opt/zimbra/ssl/zimbra/commercial/commercial.crt /opt/zimbra/ssl/zimbra/commercial/commercial_ca.crt</span><br></pre></td></tr></table></figure><p>当出现如下提示Valid Certificate:/opt/zimbra/ssl/zimbra/commercial/commercial.crt: OK，则可继续安装，如果报错，请根据报错查看具体原因。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b778abf7/2.png" alt></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>命令如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/zimbra/bin/zmcertmgr deploycrt comm /opt/zimbra/ssl/zimbra/commercial/commercial.crt /opt/zimbra/ssl/zimbra/commercial/commercial_ca.crt</span><br></pre></td></tr></table></figure><h2 id="重启Zimbra"><a href="#重启Zimbra" class="headerlink" title="重启Zimbra"></a>重启Zimbra</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zmcontrol restart</span><br></pre></td></tr></table></figure><h2 id="测试HTTPS访问"><a href="#测试HTTPS访问" class="headerlink" title="测试HTTPS访问"></a><strong>测试HTTPS访问</strong></h2><p>打开浏览器，输入<a href="https://wandouduoduo.github.io（你自己的域名）" rel="noopener" target="_blank">https://wandouduoduo.github.io（你自己的域名）</a></p><p>如浏览器地址栏显示加密小锁，则表示证书配置成功。若显示无法连接，请确保防火墙或安全组等策略有放行443端口（SSL配置端口）。</p><h2 id="证书备份"><a href="#证书备份" class="headerlink" title="证书备份**"></a>证书备份**</h2><p>请将下载的.zip压缩包和自主生成的私钥.key文件备份。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;zimbra邮件服务搭建完成后，为了安全和用户体验，通常用域名解析和证书配置。本文详细介绍zimbra服务的证书配置，当然证书过期替换也是一样的操作方法。&lt;/p&gt;
    
    </summary>
    
      <category term="应用服务" scheme="https://wandouduoduo.github.io/categories/%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1/"/>
    
    
      <category term="Mail" scheme="https://wandouduoduo.github.io/tags/Mail/"/>
    
  </entry>
  
  <entry>
    <title>漫谈微服务RPC架构</title>
    <link href="https://wandouduoduo.github.io/articles/fc4b89f7.html"/>
    <id>https://wandouduoduo.github.io/articles/fc4b89f7.html</id>
    <published>2020-12-11T07:46:02.000Z</published>
    <updated>2020-12-11T21:20:10.260Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>通读了前面两篇内容，我们了解到微服务离不开RPC，知道了RPC架构的原理，那么开源RPC架构有哪些，你知道多少呢？本文就针对<code>开源RPC框架</code>的种类进行漫谈。</p><a id="more"></a><h2 id="开源RPC框架种类"><a href="#开源RPC框架种类" class="headerlink" title="开源RPC框架种类"></a>开源RPC框架种类</h2><p>大致可分为两类：一类是与特定开发语言绑定的；另一类是与开发语言无关即跨语言平台的。</p><h3 id="和语言平台绑定开源RPC框架："><a href="#和语言平台绑定开源RPC框架：" class="headerlink" title="和语言平台绑定开源RPC框架："></a><strong>和语言平台绑定开源RPC框架</strong>：</h3><ul><li><p><strong>Dubbo</strong>：国内最早的开源RPC框架。它是由阿里公司开发并于 2011 年末对外开源，仅支持 Java 语言。</p></li><li><p><strong>Motan</strong>：微博内部使用的RPC框架。于2016年对外开源，仅支持 Java 语言。</p></li><li><p><strong>Tars</strong>：腾讯内部使用的RPC框架。于 2017 年对外开源，仅支持 C++ 语言。</p></li><li><p><strong>Spring Cloud</strong>：国外Pivotal公司2014年对外开源的RPC框架，仅支持 Java 语言。</p></li></ul><h3 id="跨语言平台开源RPC框架："><a href="#跨语言平台开源RPC框架：" class="headerlink" title="跨语言平台开源RPC框架："></a>跨语言平台开源RPC框架：</h3><ul><li><strong>gRPC</strong>：Google 于2015年对外开源的跨语言RPC框架，支持多种语言。</li><li><strong>Thrift</strong>：最初是由Facebook开发的内部系统跨语言的RPC框架，2007年贡献给了Apache基金，成为Apache 开源项目之一，支持多种语言。</li></ul><h2 id="语言绑定框架"><a href="#语言绑定框架" class="headerlink" title="语言绑定框架"></a>语言绑定框架</h2><h3 id="Dubbo"><a href="#Dubbo" class="headerlink" title="Dubbo"></a><strong>Dubbo</strong></h3><p>Dubbo可以说是国内开源最早的RPC框架了，目前只支持 Java 语言。架构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/fc4b89f7/1.jpeg" alt></p><p>图中可知Dubbo架构主要包含四个角色：其中Consumer为服务消费者，Provider为服务提供者，Registry是注册中心，Monitor是监控系统。</p><p><strong>交互流程</strong>：Consumer通过注册中心获取到Provider节点信息后，通过Dubbo客户端SDK与Provider建立连接，并发起调用。Provider通过Dubbo服务端SDK接收到请求，处理后再把结果返回。</p><h3 id="Motan"><a href="#Motan" class="headerlink" title="Motan"></a><strong>Motan</strong></h3><p>Motan是国内另外一个比较有名的开源的RPC框架，同样也只支持 Java 语言实现。架构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/fc4b89f7/2.jpeg" alt></p><p>Motan与Dubbo架构类似，都需在Client端(服务消费者)和Server端(服务提供者)引入SDK。Motan框架主要包含下面几个功能模块。</p><p>register：用来和注册中心交互。包括注册服务、订阅服务、服务变更通知、服务心跳发送等功能。</p><p>protocol：用来进行RPC服务的描述和RPC服务的配置管理。还可以添加不同功能的filter来完成统计、并发限制等功能。</p><p>serialize：将RPC请求中的参数、结果等对象进行序列化与反序列化</p><p>transport：用来进行远程通信，默认使用Netty NIO 的TCP长链接方式。</p><p>cluster：请求时会根据不同的高可用与负载均衡策略选择一个可用的 Server 发起远程调用。</p><h3 id="Tars"><a href="#Tars" class="headerlink" title="Tars"></a><strong>Tars</strong></h3><p>Tars是腾讯根据内部多年使用微服务架构的实践，总结而成的开源项目，仅支持 C++ 语言。架构图如下。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/fc4b89f7/3.jpeg" alt></p><p><strong>交互流程</strong>:</p><p>服务发布流程：在web系统上传 server的发布包到patch，上传成功后，在 web上提交发布server请求，由 registry服务传达到node，然后node拉取server的发布包到本地，拉起server服务。</p><p>管理命令流程：web 系统上的可以提交管理 server 服务命令请求，由 registry 服务传达到 node 服务，然后由 node 向 server 发送管理命令。</p><p>心跳上报流程：server 服务运行后，会定期上报心跳到 node，node 然后把服务心跳信息上报到 registry 服务，由 registry 进行统一管理。</p><p>信息上报流程：server 服务运行后，会定期上报统计信息到 stat，打印远程日志到 log，定期上报属性信息到 prop、上报异常信息到 notify、从 config 拉取服务配置信息。</p><p>client 访问 server 流程：client 可以通过 server 的对象名 Obj 间接访问 server，client 会从 registry 上拉取 server 的路由信息(如 IP、Port 信息)，然后根据具体的业务特性(同步或者异步，TCP 或者 UDP 方式)访问 server(当然 client 也可以通过 IP/Port 直接访问 server)。</p><h3 id="Spring-Cloud"><a href="#Spring-Cloud" class="headerlink" title="Spring Cloud"></a><strong>Spring Cloud</strong></h3><p>Spring Cloud 是利用 Spring Boot 特性整合了开源行业中优秀的组件，整体对外提供了一套在微服务架构中服务治理的解决方案。只支持 Java 语言平台，架构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/fc4b89f7/4.jpeg" alt></p><p><strong>交互流程</strong>:</p><p>请求统一通过 API 网关 Zuul 来访问内部服务，先经过 Token 进行安全认证。</p><p>通过安全认证后，网关 Zuul 从注册中心 Eureka 获取可用服务节点列表。</p><p>从可用服务节点中选取一个可用节点，然后把请求分发到这个节点。</p><p>整个请求过程中，Hystrix 组件负责处理服务超时熔断，Turbine 组件负责监控服务间的调用和熔断相关指标，Sleuth 组件负责调用链监控，ELK 负责日志分析。</p><h2 id="跨平台框架"><a href="#跨平台框架" class="headerlink" title="跨平台框架"></a>跨平台框架</h2><h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a><strong>gRPC</strong></h3><p>gRPC原理是通过 IDL(Interface Definition Language)文件定义服务接口的参数和返回值类型，然后通过代码生成程序生成服务端和客户端的具体实现代码，这样在 gRPC 里，客户端应用可以像调用本地对象一样调用另一台服务器上对应的方法。调用图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/fc4b89f7/5.jpeg" alt></p><p>主要特性三个方面:</p><p>通信协议采用了 HTTP/2。因为 HTTP/2 提供了连接复用、双向流、服务器推送、请求优先级、首部压缩等机制。</p><p>IDL使用了ProtoBuf。ProtoBuf 是由 Google 开发的一种数据序列化协议，它的压缩和传输效率极高，语法也简单</p><p>多语言支持，能够基于多种语言自动生成对应语言的客户端和服务端的代码。</p><h3 id="Thrift"><a href="#Thrift" class="headerlink" title="Thrift"></a><strong>Thrift</strong></h3><p>Thrift 是一种轻量级的跨语言 RPC 通信方案，支持多达 25 种编程语言。为了支持多种语言，跟 gRPC 一样，Thrift 也有一套自己的接口定义语言 IDL，可以通过代码生成器，生成各种编程语言的 Client 端和 Server 端的 SDK 代码，这样就保证了不同语言之间可以相互通信。架构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/fc4b89f7/6.jpeg" alt></p><p>Thrift RPC框架的特性:</p><p>支持多种序列化格式：如 Binary、Compact、JSON、Multiplexed 等。</p><p>支持多种通信方式：如 Socket、Framed、File、Memory、zlib 等。</p><p>服务端支持多种处理方式：如 Simple 、Thread Pool、Non-Blocking 等。</p><h2 id="平台选择"><a href="#平台选择" class="headerlink" title="平台选择"></a>平台选择</h2><p>选择哪个平台需要根据实际场景出发。如果你的业务场景仅仅局限于一种开发语言，那么选择比较多，可以选择和开发语言的绑定的RPC框架平台，可以选择支持该种开发语言的跨平台框架；但如果开发中涉及多种开发语言之间的相互调用，那就应该选择跨语言平台的RPC框架。当然也要考虑学习和维护成本。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通读了前面两篇内容，我们了解到微服务离不开RPC，知道了RPC架构的原理，那么开源RPC架构有哪些，你知道多少呢？本文就针对&lt;code&gt;开源RPC框架&lt;/code&gt;的种类进行漫谈。&lt;/p&gt;
    
    </summary>
    
      <category term="心得体会" scheme="https://wandouduoduo.github.io/categories/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="Java" scheme="https://wandouduoduo.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>详解为什么微服务架构绕不开RPC&lt;二&gt;</title>
    <link href="https://wandouduoduo.github.io/articles/1ad48174.html"/>
    <id>https://wandouduoduo.github.io/articles/1ad48174.html</id>
    <published>2020-12-10T07:50:17.000Z</published>
    <updated>2020-12-11T07:35:09.778Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>读了上篇文章充分了解了传统架构的痛点和微服务架构的好处。本篇文章进入主题详解RPC的原理和架构。</p> <a id="more"></a><h2 id="RPC定义"><a href="#RPC定义" class="headerlink" title="RPC定义"></a><strong>RPC定义</strong></h2><p>RPC（Remote Procedure Call Protocol），即远程过程调用。远相对于近，本地函数调用是为近，那么远即是垮服务器或容器的调用。</p><h3 id="本地调用"><a href="#本地调用" class="headerlink" title="本地调用"></a>本地调用</h3><p>当我们写下如下函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int result = Add(1, 2);</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/3.png" alt="图片"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">动作：</span><br><span class="line">传递两个参数</span><br><span class="line">调用了本地代码中的函数，执行逻辑运算</span><br><span class="line">返回一个出参</span><br><span class="line"></span><br><span class="line">三个动作在同一个进程中，这就本地函数调用。</span><br></pre></td></tr></table></figure><h3 id="跨进程调用"><a href="#跨进程调用" class="headerlink" title="跨进程调用"></a><strong>跨进程调用</strong></h3><p>典型的是被调进程部署在另一台服务器上。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/4.png" alt="图片"></p><p>那怎么调用呢？最容易想到的是两个进程约定一个协议格式，如使用Socket来传输通信，然后同样执行调用即可。如果实现，那这就是<code>远程过程调用</code>即RPC。</p><h2 id="RPC传递"><a href="#RPC传递" class="headerlink" title="RPC传递"></a>RPC传递</h2><p>我们都知道Socket通信只能传递连续的字节流，那么如何将入参和函数都放到连续的字节流中呢？</p><p>还以Add函数为例，假设它是一个11字节的请求报文，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/5.png" alt="图片"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">描述：</span><br><span class="line">前3个字节填入函数名“add”</span><br><span class="line">中间4个字节填入第一个参数“1”</span><br><span class="line">末尾4个字节填入第二个参数“2”</span><br></pre></td></tr></table></figure><p>同理一个4字节响应报文：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/6.png" alt="图片"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">描述：</span><br><span class="line">4个字节填入处理结果“3”</span><br></pre></td></tr></table></figure><p><strong>调用方的代码</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">request = MakePacket(“add”, 1, 2);</span><br><span class="line">SendRequest_ToService_B(request);</span><br><span class="line">response = RecieveRespnse_FromService_B();</span><br><span class="line">int result = unMakePacket(respnse);</span><br></pre></td></tr></table></figure><p>步骤如下：</p><ol><li>将入参变为字节流；</li><li>将字节流发给服务方B；</li><li>服务B接受字节流，并逻辑处理后返回字节流；</li><li>将返回字节流变为传出结果；</li></ol><p><strong>服务方的代码</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">request = RecieveRequest();</span><br><span class="line">args/function = unMakePacket(request);</span><br><span class="line">result = Add(1, 2);</span><br><span class="line">response = MakePacket(result);</span><br><span class="line">SendResponse(response);</span><br></pre></td></tr></table></figure><p>步骤如下：</p><ol><li>服务端收到字节流；</li><li>将收到的字节流转为函数名与参数；</li><li>本地调用函数得到结果；</li><li>将结果转变为字节流；</li><li>将字节流发送给调用方；</li></ol><p>过程描述图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/7.png" alt="图片"></p><p><strong>调用过程很清晰，那最大的问题是什么呢？</strong></p><p>调用方太麻烦，每次调用都要关注很多底层的细节：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">入参到字节流的转化，即序列化应用层协议细节</span><br><span class="line">字节流socket发送，即网络传输协议细节</span><br><span class="line">字节流socket接收</span><br><span class="line">字节流到出参的转化，即反序列化应用层协议细节</span><br></pre></td></tr></table></figure><p><strong>可不可以调用层不关注这个细节？</strong></p><p>答案是可以的。RPC框架就是解决这个问题的。它能够让调用方像调用本地函数一样调用远端的函数（服务），而不需要过多关注底层细节。</p><p>讲到这里对RPC，序列化和反序列化是不是有点感觉了？那接着往下看。</p><h2 id="RPC框架"><a href="#RPC框架" class="headerlink" title="RPC框架"></a><strong>RPC框架</strong></h2><p>RPC框架的职责是：向调用方和服务提供方都屏蔽各种底层的复杂细节：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">服务调用方client就像调用本地函数一样来调用服务</span><br><span class="line">服务提供方server就像实现一个本地函数一样来实现服务对外提供服务</span><br></pre></td></tr></table></figure><p>所以整个RPC框架又分为<code>client端（服务调用方）</code>和<code>server端（服务器提供方）</code>，如下图： </p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/8.png" alt="图片"><br>业务方的调用过程是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">调用方A：传入参数--&gt;执行调用--&gt;拿到结果</span><br><span class="line">服务方B：收到参数--&gt;执行逻辑--&gt;返回结果</span><br></pre></td></tr></table></figure><p><strong>RPC框架的作用</strong>就是上图中间虚线框的那部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client端（调用方）：序列化、反序列化、连接池管理、负载均衡、故障转移、队列管理，超时管理、异步管理等等</span><br><span class="line">server端（服务方）：服务端组件、服务端收发包队列、io线程、工作线程、序列化反序列化等</span><br></pre></td></tr></table></figure><h2 id="详解客户端"><a href="#详解客户端" class="headerlink" title="详解客户端"></a>详解客户端</h2><p>server端的技术大家知道比较多，这里不再赘述，接下来重点聊聊client端的技术细节。</p><h2 id="序列化和反序列化"><a href="#序列化和反序列化" class="headerlink" title="序列化和反序列化"></a>序列化和反序列化</h2><p><strong>为什么要进行序列化呢？</strong></p><p>程序员常用对象来操作数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#定义</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span></span>&#123;</span><br><span class="line">std::String user_name;</span><br><span class="line">uint64_t user_id;</span><br><span class="line">uint32_t user_age;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">#实例</span><br><span class="line">User u = <span class="keyword">new</span> User(“shenjian”);</span><br><span class="line">u.setUid(<span class="number">123</span>);</span><br><span class="line">u.setAge(<span class="number">35</span>);</span><br></pre></td></tr></table></figure><p> 但对数据<strong>存储</strong>或<strong>传输</strong>时，对象就不好用了，往往把数据转化成连续空间的二进制字节流。</p><p><strong>典型的场景</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据库索引的磁盘存储：</span><br><span class="line">数据库的索引在内存的b+树中存储，但这种格式是不能够直接存储在磁盘上的，所以需要把b+树转化为连续空间的二进制字节流，然后才能存储到磁盘上</span><br><span class="line"></span><br><span class="line">缓存的KV存储：</span><br><span class="line">redis/memcache是KV类型的缓存，缓存存储的value也必须是连续空间的二进制字节流</span><br><span class="line"></span><br><span class="line">数据的网络传输：</span><br><span class="line">socket发送的数据同样必须是连续空间的二进制字节流</span><br></pre></td></tr></table></figure><p> <strong>序列化</strong>（Serialization）：</p><p>就是将对象形态的数据转化为<code>连续空间二进制字节流</code>形态数据的过程。</p><p><strong>反序列化</strong>：</p><p>就是序列化的逆过程，有连续空间二进制字节流转化为对象形态数据。</p><p><strong>如何序列化呢？</strong></p><p>这个过程容易想到的是转化为xml或json这类具有自描述特性的标记性语言：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;class name=&quot;User&quot;&gt;</span><br><span class="line">&lt;element name=&quot;user_name&quot; type=&quot;std::String&quot; value=&quot;shenjian&quot; /&gt;</span><br><span class="line">&lt;element name=&quot;user_id&quot; type=&quot;uint64_t&quot; value=&quot;123&quot; /&gt;</span><br><span class="line">&lt;element name=&quot;user_age&quot; type=&quot;uint32_t&quot; value=&quot;35&quot; /&gt;</span><br><span class="line">&lt;/class&gt;</span><br></pre></td></tr></table></figure><p><strong>方法一</strong>：</p><p>约定好<code>转换规则</code>：</p><p>发送方把User类的对象序列化为xml或json；服务方收到xml或json的二进制流后，再将其反序列化为User对象。</p><p><strong>方法二</strong>:</p><p>自定义二进制协议来进行序列化，还以User对象为例，设计一个通用协议如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/9.png" alt="图片"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">最前面4个字节表示序号</span><br><span class="line">序号后面4个字节表示key的长度m</span><br><span class="line">后面m个字节表示key的值</span><br><span class="line">在后面4个字节表示value的长度n</span><br><span class="line">接着n个字节表示value的值</span><br><span class="line">像xml一样递归，直至描述完整个对象</span><br></pre></td></tr></table></figure><p> 那根据上面协议描述出来的User对象，如下：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/10.png" alt="图片"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">第一行：序号4个字节（设0表示类名），类名长度4个字节（长度为4），接下来4个字节是类名（User），共12字节</span><br><span class="line">第二行：序号4个字节（1表示第一个属性），属性长度4个字节（长度为9），接下来9个字节是属性名（user_name），属性值长度4个字节（长度为8），属性值8个字节（值为”shenjian”），共29字节</span><br><span class="line">第三行：序号4个字节（2表示第二个属性），属性长度4个字节（长度为7），接下来7个字节是属性名（user_id），属性值长度4个字节（长度为8），属性值8个字节（值为123），共27字节</span><br><span class="line">第四行：序号4个字节（3表示第三个属性），属性长度4个字节（长度为8），接下来8个字节是属性名（user_name），属性值长度4个字节（长度为4），属性值4个字节（值为35），共24字节</span><br><span class="line"></span><br><span class="line">整个二进制字节流共12+29+27+24=92字节。</span><br></pre></td></tr></table></figure><p> 实际序列化协议要考虑的细节远比这个多。例如：强类型的语言不仅要还原属性名，属性值，还要还原属性类型；复杂的对象不仅要考虑普通类型，还要考虑对象嵌套类型等。无论如何，序列化的思路都是类似的。</p><h2 id="序列化协议考虑因素"><a href="#序列化协议考虑因素" class="headerlink" title="序列化协议考虑因素"></a><strong>序列化协议考虑因素</strong></h2><p>不管使用成熟协议xml/json，还是自定义二进制协议来序列化对象。序列化的协议都需要考虑以下因素：</p><ul><li><strong>解析效率</strong>：这个应该是序列化协议应首要考虑的因素，像xml/json解析起来比较耗时，需要解析doom树，二进制自定义协议解析起来效率就很高</li><li><strong>压缩率和传输有效性</strong>：同一对象xml/json传输起来包含有大量的xml标签等，信息的有效性较低，二进制自定义协议占用的空间相对来说就小很多</li><li><strong>扩展和兼容性</strong>：是否能方便的增减字段，增减字段后旧版客户端是否要强制升级，xml/json和上面的二进制协议都能够方便的扩展</li><li><strong>可读和可调试性</strong>：很好理解，xml/json的可读性就比二进制协议好很多</li><li><strong>跨语言</strong>：上面的两个协议都是跨语言，有些序列化协议与开发语言紧密相关。例如dubbo序列化协议只能支持Java的RPC调用</li><li><strong>通用性</strong>：xml/json非常通用，都有很好的第三方解析库，各个语言解析起来也十分方便。自定义二进制协议虽能够跨语言，但每个语言都要写一个简易的协议客户端</li></ul><h2 id="常见序列化格式"><a href="#常见序列化格式" class="headerlink" title="常见序列化格式"></a><strong>常见序列化格式</strong></h2><p><strong>xml/json</strong>：解析效率和压缩率较差，但扩展性、可读性和通用性较好</p><p><strong>protobuf</strong>：Google出品，各方面都不错，强烈推荐，属于二进制协议。可读性差点，但有类似的to-string协议帮助调试</p><p><strong>mc_pack</strong>：传说各方面要超越protobuf，但是只能说是传说</p><p><strong>thrift</strong>：由Facebook开发，有兴趣可以了解下</p><p>当然还有<strong>Avro</strong>、<strong>CORBA</strong>等等</p><h2 id="调用方式"><a href="#调用方式" class="headerlink" title="调用方式"></a>调用方式</h2><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/11.png" alt="图片"></p><p>RPC-client 不仅包含序列化和反序列化（上图中的1和4）还包含发送和接收字节流（上图中的2和3）</p><p>字节流传输分为<code>同步调用</code>和<code>异步调用</code>两种</p><p>同步调用代码片段如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Result = Add(Obj1, Obj2);      //得到Result之前处于阻塞状态</span><br></pre></td></tr></table></figure><p>异步调用代码片段如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Add(Obj1, Obj2, callback);    //调用后直接返回，不等结果</span><br></pre></td></tr></table></figure><p>处理结果通过回调为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">callback(Result)&#123;</span><br><span class="line">...</span><br><span class="line">&#125;                   // 得到处理结果后会调用这个回调函数</span><br></pre></td></tr></table></figure><p>两类调用在RPC-client中实现方式完全不一样的</p><h3 id="同步调用架构"><a href="#同步调用架构" class="headerlink" title="同步调用架构"></a><strong>同步调用架构</strong></h3><p>在调用得到结果前，一直处于阻塞状态，也会一直占用工作线程，直到的到结果才能再处理调用。如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/12.png" alt="图片"></p><p><strong>组件描述</strong>：</p><p>左边大框：代表调用方的整个工作线程</p><p>右边橙框：代表RPC-server服务提供方</p><p>粉色中框：代表RPC-client组件</p><p>蓝色小框：代表同步RPC-client两个核心组件（序列化和连接池组件）</p><p>白色流程框，以及箭头序号：代表整个工作线程的串行执行步骤</p><p><strong>执行步骤</strong>：</p><ol><li>业务代码发起RPC对象调用</li><li>序列化组件将对象调用序列化成二进制字节流</li><li>通过连接池组件拿到一个可用的连接进程</li><li>通过连接进程将包发送给RPC-server</li><li>发送的包在网络传输到RPC-server接收</li><li>RPC-server响应包在网络传输返回给RPC-client</li><li>通过连接进程收取响应包</li><li>通过连接池组件，将连接进程释放放回连接池中</li><li>序列化组件，将响应包反序列化为对象返回给调用方</li><li>业务代码获取到结果</li></ol><p><strong>连接池组件</strong></p><p>连接池组件作用：RPC框架锁支持的负载均衡、故障转移、发送超时等特性的实现。</p><p>​                                           <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/13.png" alt="图片"><br>典型连接池组件对外接口代码为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int ConnectionPool::init(…);</span><br><span class="line">Connection ConnectionPool::getConnection();</span><br><span class="line">int ConnectionPool::putConnection**(Connection t);</span><br></pre></td></tr></table></figure><p><strong>init</strong>和下游RPC-server，建立N个TCP长连接，即所谓的连接池。</p><p><strong>getconnection</strong>从连接池中得到一个连接，加锁（置个标志位），返回给调用方。</p><p><strong>putConnection</strong>将使用的连接放回连接池中，解锁（也是置个标志位）。</p><p><strong>其他功能实现</strong></p><p>连接池建立了一个与RPC-server集群的连接，连接池在返回连接的时候，需具备随机性，来实现<strong>负载均衡</strong>。</p><p>连接池建立了一个与RPC-server集群的连接，当连接池发现某个节点的连接异常后，需要将这个节点的连接剔除掉，返回正常的连接；在节点恢复后，再将连接加回来，来实现<strong>故障转移</strong>。</p><p>同步阻塞调用，得到一个连接后，使用带超时的send/recv即可，实现带<strong>超时</strong>的发送和接收。</p><p>总的来说：同步RPC-client调用实现是相对较容易的。序列化和连接池组件配合多工作线程数就能够实现了。</p><h3 id="异步回调架构"><a href="#异步回调架构" class="headerlink" title="异步回调架构"></a><strong>异步回调架构</strong></h3><p>在调用得到结果前，不会处于阻塞状态，理论上任何时候都没有任何线程处于阻塞状态。所以理论上只需要很少工作线程与服务连接就能够达到很高的吞吐量。如下图</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/14.png" alt="图片"></p><p><strong>组件描述</strong>：</p><p>左边框框：是少量工作线程，执行调用和回调动作</p><p>中间粉框：代表RPC-client组件</p><p>右边橙框：代表RPC-server组件</p><p>蓝色小框：代表异步RPC-client六个核心组件：<strong>上下文管理器、超时管理器、序列化组件</strong>、<strong>下游收发队列、下游收发线程和连接池组件</strong></p><p>白色流程框，以及箭头序号：代表整个工作线程的串行执行步骤</p><p><strong>执行步骤</strong>：</p><ol><li><p>业务代码发起对象RPC异步调用</p></li><li><p>上下文管理器将请求回调和上下文存储起来</p></li><li><p>序列化组件调用序列化将对象转成二进制字节流</p></li><li><p>下游收发队列将报文放入待发送队列中此时调用返回，不会阻塞工作线程；</p></li><li><p>下游收发线程从待发送队列中将报文取出，再通过连接池组件拿到一个可用的连接进程</p></li><li><p>通过连接进程将请求包发送给RPC-server组件</p></li><li><p>请求包在网络中传输到RPC-server</p></li><li><p>RPC-server响应包在网络中传输，返回给RPC-client</p></li><li><p>通过连接进程收取从RPC-server的响应包</p></li><li><p>下游收发线程将响应报文放入已接受队列中，通过连接池组件将连接进程放回到连接池中</p></li><li><p>下游收发队列里将响应报文取出</p></li><li><p>序列化组件将响应包反序列化为对象结果</p></li><li><p>上下文管理器将结果回调和上下文取出</p></li><li><p>通过回调业务代码返回结果，整个流程结束</p><p><strong>请求长时间不返回</strong>处理流程是：</p></li><li><p>上下文管理器的请求将长时间得不到返回结果</p></li><li><p>超时管理器拿到超时的上下文</p></li><li><p>通过超时管理器回调业务代码，代码报错</p></li></ol><p><strong>其他功能实现</strong></p><p>由于请求包的发送和响应包的回调都是异步发生的，甚至不在同一个工作线程中完成，所以需要一个组件来记录请求的上下文，来把请求–&gt;响应–&gt;回调等信息匹配起来，这就是<strong>上下文管理器</strong>。</p><p>将请求–&gt;响应–&gt;回调信息匹配，假设通过连接向下游服务发送a，b，c三个请求包，那么异步收到了x，y，z三个响应包，如下图：</p><p>​                                                             <img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/15.png" alt="图片"></p><p>可以通过<strong>请求id</strong>来实现请求–&gt;响应–&gt;回调信息匹配，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/16.png" alt="图片"><br><strong>流程描述</strong>如下：</p><ol><li>请求时生成请求id</li><li>生成请求上下文内容，上下文内容中包含请求时间和回调函数等信息</li><li>上下文管理器记录请求id与上下文内容的映射关系</li><li>将请求id放在请求包里发给RPC-server</li><li>RPC-server也把请求id打在响应包里进行返回</li><li>有响应包中的请求id，通过上下文管理器找到原来的上下文内容</li><li>从上下文内容中拿到回调函数</li><li>回调函数将响应结果返回</li></ol><p>与同步连接池类似，不同的是同步连接池使用阻塞方式来收发，需要服务与ip建立多条连接；而异步调用的收发是服务和ip只需要建立少量的连接即可，来实现<strong>负载均衡和故障转移</strong></p><p><strong>超时收发</strong>，与同步阻塞收发差异的是：同步阻塞超时可以直接使用带超时的send/recv来实现，而异步非阻塞的ni网络报文收发，由于请求连接不会一直等待响应回包，所以超时是由超时管理器这个组件来专门管理。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/1ad48174/17.png" alt="图片"></p><p>每个请求发送给下游RPC-server时，会在上下文管理器中保存请求id与上下文的信息，而上下文中也保存了请求的很多相关信息。如请求id、回包回调、超时回调和发送时间等等。超时管理器启动定时器对上下文管理器中的内容进行扫描，判断上下文中的请求发送时间是否过长，如果过长，就不再等待回包，直接超时回调，并将上下文删除掉。如果超时回调执行后，正常响应回包又到达，通过请求id在上下文管理器里找不到上下文，就直接将请求丢弃，超时已处理是无法恢复上下文的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>异步回调比同步回调，除共有序列化组件和连接池组件外，还会多出上下文管理器、超时管理器、下游收发队列和下游收发线程等组件，并对调用方的调用习惯也有影响，由同步直接得到结果变为了回调才可以。但是异步回调能极大的提高提高系统的吞吐量和处理请求，要结合业务场景合理的选取方式来实现RPC-client。</p><h2 id="知识梳理"><a href="#知识梳理" class="headerlink" title="知识梳理"></a><strong>知识梳理</strong></h2><p><strong>RPC调用</strong>：就是像调用本地函数一样去调用远端服务。</p><p><strong>RPC框架优点</strong>：屏蔽调用过程中的序列化，网络传输等技术细节。让调用方只专注于调用，服务方只专注于实现调用。</p><p><strong>序列化</strong>：将对象转化为连续存储空间二进制流的过程，就叫做序列化。而磁盘存储、缓存存储和网络传输等操作只能是二进制流。</p><p><strong>同步RPC客户端核心组件</strong>：序列化组件和连接池组件。通过连接池实现负载均衡和故障转移，通过阻塞收发实现超时处理。</p><p><strong>异步RPC客户端的核心组件</strong>：序列化组件、连接池组件、收发队列、收发线程、上下文管理器和超时管理器。通过请求id将请求包–&gt;响应包–&gt;回调函数进行映射关联，上下文管理器管理上下文，超时管理器定时器触发超时回调。</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;读了上篇文章充分了解了传统架构的痛点和微服务架构的好处。本篇文章进入主题详解RPC的原理和架构。&lt;/p&gt;
    
    </summary>
    
      <category term="心得体会" scheme="https://wandouduoduo.github.io/categories/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="Java" scheme="https://wandouduoduo.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>详解为什么微服务架构绕不开RPC&lt;一&gt;</title>
    <link href="https://wandouduoduo.github.io/articles/b5e51a16.html"/>
    <id>https://wandouduoduo.github.io/articles/b5e51a16.html</id>
    <published>2020-12-10T07:26:17.000Z</published>
    <updated>2020-12-10T12:00:29.060Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>科技改变生活，搞互联网的绕不开BAT、TMD等大厂，那么搞微服务架构就绕不开RPC。知其然知其所以然才行，是什么原因绕不开呢？想要知道这个问题，先要搞清楚为什么要搞微服务呢？本文就来详细和你聊聊。</p><a id="more"></a><h2 id="为什么要搞微服务框架？"><a href="#为什么要搞微服务框架？" class="headerlink" title="为什么要搞微服务框架？"></a>为什么要搞微服务框架？</h2><p>有的同学说，技术总监或着是运维经理说要搞微服务，那公司就搞了。或者是看到很多大厂都采用微服务框架，我们不能落后，所以我们也要搞跟上步伐嘛。那么这些同学是否会有疑问为什么总监或着经理要搞呢？为什么各大厂都用微服务呢？这种框架带来哪些好处解决哪些痛点，让这么多公司和技术大牛青睐有加呢？如果你有过这些疑问，并想要搞清楚，那么你才能是一个合格的技术人。<strong>做技术的千万不能有别人都在做，所以我们也要搞的这种想法</strong>。下面通过对比，让你切实感受服务化的好处。</p><h2 id="传统高可用架构"><a href="#传统高可用架构" class="headerlink" title="传统高可用架构"></a>传统高可用架构</h2><p>典型的互联网高可用架构如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/1.png" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）客户端：APP，H5，小程序，PC浏览器；</span><br><span class="line">（2）后端入口：高可用的反向代理web集群(nginx或apache等)；</span><br><span class="line">（3）站点应用：高可用的web-server集群(tomcat或docker等)；</span><br><span class="line">（4）后端存储：高可用db集群（mysql等）；</span><br></pre></td></tr></table></figure><p>更典型的架构如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/2.png" alt></p><p>web-server集群通过DAO/ORM等技术来访问数据库。</p><h3 id="传统架构痛点"><a href="#传统架构痛点" class="headerlink" title="传统架构痛点"></a>传统架构痛点</h3><p>传统的架构可以看到没有服务层的，那么传统架构存在什么典型痛点呢？</p><h4 id="痛点一：代码随处拷贝"><a href="#痛点一：代码随处拷贝" class="headerlink" title="痛点一：代码随处拷贝"></a><strong>痛点一：代码随处拷贝</strong></h4><p>最常见的业务例子：用户的数据访问。互联网公司都用一个数据库来存储用户数据，而且各个业务都有访问用户数据的需求。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/3.png" alt></p><p><strong>场景</strong>：各个业务线（A,B和C）都需要获取用户数据，通常做法是通过DAO/ORM用SQL语句去操作数据库来获取，这就导致了代码的拷贝重复。</p><h4 id="痛点二：架构复杂性扩散"><a href="#痛点二：架构复杂性扩散" class="headerlink" title="痛点二：架构复杂性扩散"></a><strong>痛点二：架构复杂性扩散</strong></h4><p><strong>缓存机制引入</strong></p><p>随着并发量越来越高，访问用户数据的数据库成为了瓶颈，通常做法加入缓存机制（redis或memcache等）来降低数据库的读压力。那么各个业务线都需要针对底层架构的调整做响应更改，导致了的架构的复杂性扩散。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/4.png" alt></p><p>增加了缓存机制，所有业务代码的读写操作需要升级调整：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写操作：</span></span><br><span class="line">（1）先剔除缓存；</span><br><span class="line">（2）再在数据库中写入数据；</span><br><span class="line"></span><br><span class="line"><span class="comment">#读请求：</span></span><br><span class="line">（1）先读cache命中则返回；</span><br><span class="line">（2）没命中则读数据库；</span><br><span class="line">（3）再把数据放入cache中；</span><br></pre></td></tr></table></figure><p>但是开发就郁闷了，我业务代码没做跑的好好的，没做调整。你底层数据库做调整，我们所有业务的代码都需要被迫升级。</p><p><strong>分库分表引入</strong></p><p>随着数据量的不断增多，数据库需要做水平扩容拆分，于是又引入了数据库的分库分表。各个业务组又都需要去关注数据库分库分表引入导致的复杂性。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/5.png" alt></p><p>这里举了两个例子：各个业务组的开发奔溃了，这完全和业务本身是无关，而所有业务都需要被迫升级来适应架构调整导致的影响。</p><h4 id="痛点三：库的复用和耦合"><a href="#痛点三：库的复用和耦合" class="headerlink" title="痛点三：库的复用和耦合"></a><strong>痛点三：库的复用和耦合</strong></h4><p>解决上述两个痛点，我们最容易想到的解决方法是：抽象出统一的库来解决。</p><p>抽象出一个user.so库文件，该库文件负责对整个用户数据的存取操作。从而避免了代码的拷贝。架构调整也只需要关注user.so这个库文件即可。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/6.png" alt></p><p>有同学就会说是的这种方法也可以实现，那大家不妨想这样一个场景：</p><p><strong>场景</strong>：业务线A因需求想将user.so这个库由版本1.0升级至版本2.0，那么就需要这个库兼容其他所有业务线，如果不兼容业务线B，就会导致B业务出现问题，获取不到数据。</p><p>那么业务线A如果通知了业务线B去升级，而业务线B的开发表示很无辜，业务线B自身的业务没改动，也需要被动升级，郁闷吧。而且是其他业务线也都需要改。造成问题：<strong>库的版本维护会导致业务线之间的耦合关系</strong></p><h4 id="痛点四：SQL质量和业务相互影响"><a href="#痛点四：SQL质量和业务相互影响" class="headerlink" title="痛点四：SQL质量和业务相互影响"></a><strong>痛点四：SQL质量和业务相互影响</strong></h4><p>各个业务线都通过DAO/ORM操作数据库，实质上就是根据各个业务线需求拼装的SQL语句，资深的工程师写出的SQL语句质量比较高，但是经验没有这么丰富的工程师可能会写出一些低效的SQL语句。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/7.png" alt></p><p><strong>场景</strong>：业务线A新招了个初级程序员，写了一个全表扫描的SQL操作，从而导致整个数据库的CPU100%跑满，其他业务线读写都不能操作，都会受影响。你是该程序员很慌不慌。</p><h4 id="痛点五：疯狂的DB耦合"><a href="#痛点五：疯狂的DB耦合" class="headerlink" title="痛点五：疯狂的DB耦合"></a><strong>痛点五：疯狂的DB耦合</strong></h4><p>通常业务线不只访问user表的用户数据，还会结合自己的业务访问自己的数据。典型的就是通过join数据表来实现各自业务线的业务逻辑。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/8.png" alt></p><p><strong>场景</strong>：各个业务表和user表耦合在一起，例如业务线A的table-user和table-A耦合在一起，结果就是user表和所有业务表都耦合在了一起。那么随着数据量的增大，各个业务线的数据库是无法垂直拆分开的，必须使用一个大库来存储。想想一下一个大库几百个业务表，崩不崩溃。</p><h4 id="痛点六：其他等等"><a href="#痛点六：其他等等" class="headerlink" title="痛点六：其他等等"></a><strong>痛点六：其他等等</strong></h4><h2 id="微服务架构"><a href="#微服务架构" class="headerlink" title="微服务架构"></a>微服务架构</h2><p>服务化后互联网高可用分层架构如下,引入了高可用user-service层，该层统一操作数据库存取用户</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/9.png" alt></p><h3 id="服务化架构好处"><a href="#服务化架构好处" class="headerlink" title="服务化架构好处"></a>服务化架构好处</h3><h4 id="好处一：调用方便清爽"><a href="#好处一：调用方便清爽" class="headerlink" title="好处一：调用方便清爽"></a><strong>好处一：调用方便清爽</strong></h4><p>原来业务访问用户数据，是需要通过DAO/ORM拼装SQL语句操作数据库来存取，现在业务方通过RPC调用就可访得到用户数据，就像调用本地函数一样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">User = UserService::GetUserById(uid);</span><br></pre></td></tr></table></figure><p>传入uid参数，得到User实体，就像调用本地函数一样，不需要关心序列化，网络传输，后端执行，网络传输，反序列化等等就可以得到用户数据。</p><h4 id="好处二：代码复用性"><a href="#好处二：代码复用性" class="headerlink" title="好处二：代码复用性"></a><strong>好处二：代码复用性</strong></h4><p>所有user用户数据的存取，都通过user-service来进行的，只此一份。</p><h4 id="好处三：屏蔽底层提高专注"><a href="#好处三：屏蔽底层提高专注" class="headerlink" title="好处三：屏蔽底层提高专注"></a><strong>好处三：屏蔽底层提高专注</strong></h4><p>原来所有业务线都需要关注缓存、分库分表等底层细节。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/10.png" alt></p><p>现在只有服务层需要专注关注底层的复杂性，上游业务调用屏蔽了细节。</p><p><img src="https://cdn.jsdelivr.net/gh/wandouduoduo/wandouduoduo.github.io@master/articles/b5e51a16/11.png" alt></p><h4 id="好处四：SQL质量得到保障"><a href="#好处四：SQL质量得到保障" class="headerlink" title="好处四：SQL质量得到保障"></a><strong>好处四：SQL质量得到保障</strong></h4><p>原来是业务直接拼接SQL语句访问数据库。现在所有业务请求的SQL语句都是服务层提供，业务线不能再为所欲为。</p><h4 id="好处五：数据库解耦"><a href="#好处五：数据库解耦" class="headerlink" title="好处五：数据库解耦"></a><strong>好处五：数据库解耦</strong></h4><p>原来各业务的数据库都混在一个大库里，难以拆分。现在数据库被隔离开，可以很方便的拆分出来，进行横向扩容。</p><h4 id="好处六：有限接口无限性能"><a href="#好处六：有限接口无限性能" class="headerlink" title="好处六：有限接口无限性能"></a><strong>好处六：有限接口无限性能</strong></h4><p>原先各业务线服务想怎么操纵数据库都可以，遇到性能瓶颈，各业务线容易扯皮，相互推诿。现在服务只提供有限的通用接口，理论上可认为集群可以提供无限的性能，性能出现瓶颈，服务层集中优化即可。</p><h4 id="好处七：其他等等"><a href="#好处七：其他等等" class="headerlink" title="好处七：其他等等"></a><strong>好处七：其他等等</strong></h4><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>结合公司现状和遇到的问题，以实际问题出发，进行架构设计，才能解决真正问题和痛点。一切脱离业务的架构设计，都是空中楼阁，可能高大上，但真的好用和实用吗？</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "19128-1606361858239-837",        "name": "运维随笔",        "qrcode": "https://wandouduoduo.github.io/about/index/gongzhonghao.jpg",        "keyword": "yunwei"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;科技改变生活，搞互联网的绕不开BAT、TMD等大厂，那么搞微服务架构就绕不开RPC。知其然知其所以然才行，是什么原因绕不开呢？想要知道这个问题，先要搞清楚为什么要搞微服务呢？本文就来详细和你聊聊。&lt;/p&gt;
    
    </summary>
    
      <category term="心得体会" scheme="https://wandouduoduo.github.io/categories/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="Java" scheme="https://wandouduoduo.github.io/tags/Java/"/>
    
  </entry>
  
</feed>
